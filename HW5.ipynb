{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "1b2e3915-4ea5-4393-9844-41209d5e5444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 'Tesla' exists in Google search: False\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def verify_entity_google_search(entity_name, api_key, cx):\n",
    "    \"\"\"\n",
    "    Verifies if a named entity exists using Google's Custom Search API.\n",
    "    \n",
    "    Args:\n",
    "    - entity_name (str): The name of the entity to check.\n",
    "    - api_key (str): Your Google API Key.\n",
    "    - cx (str): Your Custom Search Engine ID (CX).\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the entity exists, False otherwise.\n",
    "    \"\"\"\n",
    "    # URL for Google Custom Search API\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?q={entity_name}&key={api_key}&cx={cx}\"\n",
    "    \n",
    "    # Send request to the API\n",
    "    response = requests.get(url).json()\n",
    "    \n",
    "    # Check if the 'items' key exists in the response (meaning search results were found)\n",
    "    if 'items' in response:\n",
    "        # If we get search results, the entity likely exists\n",
    "        return True\n",
    "    else:\n",
    "        # No results, the entity might not exist or isn't prominent enough\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "api_key = \"YOUR_GOOGLE_API_KEY\"  # Replace with your actual API key\n",
    "cx = \"YOUR_CUSTOM_SEARCH_ENGINE_ID\"  # Replace with your actual CSE ID\n",
    "\n",
    "entity_name = \"Tesla\"\n",
    "exists = verify_entity_google_search(entity_name, api_key, cx)\n",
    "print(f\"Entity '{entity_name}' exists in Google search: {exists}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8039e912-ada2-4f32-afc2-e833d5d1877d",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Getting dataset from <a href = https://www.kaggle.com/competitions/nlp-getting-started/data style=\"text-decoration: underline; color : Default\">kaggle</a>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "70f50199-ca35-492b-be1d-d7a42e514498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.12/site-packages/kaggle/__init__.py\n",
      "nlp-getting-started.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Extraction completed\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules for file handling and Kaggle operations\n",
    "import os       \n",
    "import shutil    \n",
    "import zipfile  # Handle ZIP archive files\n",
    "import kaggle   # Kaggle API client for downloading datasets\n",
    "\n",
    "# Print the location of the installed kaggle package\n",
    "print(kaggle.__file__)\n",
    "# Download dataset using the Kaggle API\n",
    "!kaggle competitions download -c nlp-getting-started\n",
    "\n",
    "file_name = os.path.join(os.getcwd(), 'nlp-getting-started.zip')  # Get full path to the ZIP file\n",
    "if os.path.exists(file_name):  # Check if the file exists\n",
    "    with zipfile.ZipFile(file_name, 'r') as zip_f:  # Open ZIP file in read mode\n",
    "        zip_f.extractall(os.getcwd())  # Extract all files to current directory\n",
    "    print('Extraction completed')  # Confirm extraction\n",
    "else:\n",
    "    print(f'\"{file_name}\" does not exist. Download dataset from https://www.kaggle.com/competitions/nlp-getting-started/data')  # File not found message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "92bb5130-e5b1-4d7d-a342-dbedb55014fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install slugify\n",
    "!#pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ee83d",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Data Preprocessing\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "72f098be-94a4-4ae2-8cb1-ad6b219a86d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyxDamerauLevenshtein in /usr/local/anaconda3/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: fuzzywuzzy in /usr/local/anaconda3/lib/python3.12/site-packages (0.18.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the pyxDamerauLevenshtein package for calculating string distance using the Damerau-Levenshtein algorithm\n",
    "!pip install pyxDamerauLevenshtein\n",
    "# Install the fuzzywuzzy package for fuzzy string matching\n",
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "58449ea6-629f-43b3-91ba-e1c1b9d5456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob  # For sentiment analysis\n",
    "from collections import Counter  # For counting frequencies\n",
    "import re  # For text cleaning using regular expressions \n",
    "import spacy  # For natural language processing\n",
    "from unidecode import unidecode  # For Unicode normalization  \n",
    "import pandas as pd \n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "ed8ed60d-2462-463c-a269-e585d7ab2b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm \n",
    "#!python -m spacy download en_core_web_md \n",
    "#!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "6bb66c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy's language model for advanced NLP features\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "# Define custom stop words that are specific to the context of tweets\n",
    "custom_stop_words = { \n",
    "   'tweets', 'retweet', 'retweets', 'follow', 'following',\n",
    "    'follower', 'followers', 'dm', 'directmessage', 'hashtag', 'amp', 'via', 'twitter'\n",
    "}   \n",
    "\n",
    "# Combine NLTK stopwords, custom stopwords, and spaCy's stopwords into a comprehensive set\n",
    "combined_stop_words = custom_stop_words.union(nlp.Defaults.stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "23995e56-cd7b-467a-aaf0-133dd3f04213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy vocabulary length is: 84780\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spacy vocabulary length is: {len(nlp.vocab.strings ) }\")#check spacy vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "0d71f911-91c3-4052-a635-5eaf3d7121f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataHandler:\n",
    "    def __init__(self, train_files, test_file=None, text_column='text'):\n",
    "        \"\"\"\n",
    "        Initialize the DataHandler class.\n",
    "\n",
    "        :param train_files: List of file paths for training data.\n",
    "        :param test_file: File path for test data (optional).\n",
    "        :param text_column: Name of the text column in the dataset.\n",
    "        \"\"\"\n",
    "        self.train_files = train_files  # List of file paths for training data.\n",
    "        self.test_file = test_file  # File path for test data.\n",
    "        self.text_column = text_column  # Text column name.\n",
    "        self.train_data = None  # Placeholder for the combined training data.\n",
    "        self.test_data = None  # Placeholder for the test data.\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load and concatenate training data from multiple files.\n",
    "        Load the test data separately if provided.\n",
    "        \"\"\"\n",
    "        # Load and concatenate training data\n",
    "        train_dataframes = []\n",
    "        for file in self.train_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                train_dataframes.append(df)\n",
    "                print(f\"Loaded training file: {file}, Shape: {df.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading training file {file}: {e}\")\n",
    "        \n",
    "        try:\n",
    "            self.train_data = pd.concat(train_dataframes, ignore_index=True)\n",
    "            print(f\"All training data concatenated successfully. Shape: {self.train_data.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating training data: {e}\")\n",
    "        \n",
    "        # Load test data if provided\n",
    "        if self.test_file:\n",
    "            try:\n",
    "                self.test_data = pd.read_csv(self.test_file)\n",
    "                print(f\"Test data loaded successfully. Shape: {self.test_data.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading test data: {e}\")\n",
    "\n",
    "    def validate_train_data(self):\n",
    "        \"\"\"\n",
    "        Validate the loaded training data and output metadata summaries.\n",
    "        \"\"\"\n",
    "        if self.train_data is None:\n",
    "            print(\"No training data loaded. Please load the training data first.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Training Data Shape: {self.train_data.shape}\")\n",
    "        print(f\"Columns: {self.train_data.columns.tolist()}\")\n",
    "        print(\"Data Types:\")\n",
    "        print(self.train_data.dtypes)\n",
    "        print(\"Missing Values:\")\n",
    "        print(self.train_data.isnull().sum())\n",
    "        print(\"Sample Training Data:\")\n",
    "        print(self.train_data.head())\n",
    "\n",
    "    def validate_test_data(self):\n",
    "        \"\"\"\n",
    "        Validate the loaded test data and output metadata summaries.\n",
    "        \"\"\"\n",
    "        if self.test_data is None:\n",
    "            print(\"No test data loaded. Please load the test data first.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Test Data Shape: {self.test_data.shape}\")\n",
    "        print(f\"Columns: {self.test_data.columns.tolist()}\")\n",
    "        print(\"Data Type\")\n",
    "        print(self.test_data.dtypes)\n",
    "        print(\"Missing Values:\")\n",
    "        print(self.test_data.isnull().sum())\n",
    "        print(\"Sample Test Data:\")\n",
    "        print(self.test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3d9f25-7c8f-43f2-b491-afa5a654c13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "8336082f-9d97-43cf-a16b-146220e82026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc  # Add this import to avoid NameError\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "from spacy.language import Language\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, stages=[]):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.stop_words = self.nlp.Defaults.stop_words\n",
    "        self.valid_words = set(self.nlp.vocab.strings)\n",
    "        self.word_list = set(words.words())  # NLTK word list\n",
    "        self.spell_checker = SpellChecker()  # Initialize spell checker\n",
    "        self.stages = stages\n",
    "\n",
    "        \n",
    "        self.patterns = {\n",
    "            'HASHTAG': re.compile(r'#\\w+'),\n",
    "            'USERNAME': re.compile(r'@\\w+'),\n",
    "            'TWITTER_URL': re.compile(r'https?://t.co/\\S+'),\n",
    "            'TELEGRAM_URL': re.compile(r'https?://t.me/\\S+'),\n",
    "            'WEB_URL': re.compile(r'https?://www\\.\\S+')\n",
    "        }\n",
    "        self.date_patterns = [\n",
    "            r'\\b(?:\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4})\\b',\n",
    "            r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2},? \\d{4}\\b',\n",
    "            r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) \\d{1,2},? \\d{4}\\b',\n",
    "            r'\\b\\d{4}-(?:0[1-9]|1[0-2])-(?:0[1-9]|[12][0-9]|3[01])\\b',\n",
    "            r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December) \\d{4}\\b',\n",
    "            r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) \\d{4}\\b',\n",
    "            r'\\b(?:0?[1-9]|[12][0-9]|3[01])-(?:January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)-\\d{4}\\b',\n",
    "            r'\\b\\d{4} (?:January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}\\b',\n",
    "            r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\b',\n",
    "            r'\\b(?:19[0-9]{2}|20[0-9]{2})\\b',\n",
    "            r'\\b(?:\\d{1,2}(?:st|nd|rd|th)? (?:January|February|March|April|May|June|July|August|September|October|November|December),? \\d{4})\\b',\n",
    "            r'\\b(?:\\d{1,2}(?:st|nd|rd|th)? of (?:January|February|March|April|May|June|July|August|September|October|November|December),? \\d{4})\\b'\n",
    "        ]\n",
    "\n",
    "        \n",
    "        self.regex_filter_patterns = [ \n",
    "            r'https?://\\S+|www\\.\\S+',            # Drop all links (URLs).\n",
    "            r'#(?!\\S)',                          # Drop standalone hashtags (e.g., #).\n",
    "            r'\\b\\d+\\b',                          # Drop standalone numbers.\n",
    "            r'\\b(\\w)\\1{1,}\\b',                   # Drop standalone two-letter words with repeating letters (e.g., \"aa\").\n",
    "            r'[^\\w\\s#@]',                        # Replace non-alphanumeric characters with space (already done).\n",
    "            r'(?<!\\S)\\b\\w{1,2}\\b(?!\\S)',         # Drop single-letter or two-letter words.\n",
    "            r'[^\\x00-\\x7F]',                     # Drop non-ASCII characters.\n",
    "            r'(?<=\\w)[^\\w#@]+(?=\\s|$)',          # Remove unwanted characters following words, hashtags, or mentions.\n",
    "        ]\n",
    "\n",
    "\n",
    "        \n",
    "        self._add_custom_components()\n",
    "\n",
    "    def _ensure_text(self, input_data):\n",
    "        \"\"\"Ensure input is in string format for uniform processing.\"\"\"\n",
    "        if isinstance(input_data, list):\n",
    "            return ' '.join(input_data)  # Join list into a single string\n",
    "        return input_data\n",
    "\n",
    "        self.word_list = set(words.words())  # NLTK word list\n",
    "        self.spell_checker = SpellChecker()  # Initialize spell checker\n",
    "\n",
    "    def process_words(self, text):\n",
    "        \"\"\"\n",
    "        Processes the input text to classify words into valid and invalid, \n",
    "        and appends corrected versions of invalid words to the valid words list.\n",
    "        \"\"\"\n",
    "        words_list = text.split()  # Split text into words\n",
    "        valid_words = []  # List to store valid words\n",
    "        invalid_words_list = []  # List to store invalid words\n",
    "        valid_found = []  # List to store words that were corrected or found valid\n",
    "\n",
    "        # Split words into valid and invalid categories based on spaCy's vocabulary and NLTK word list\n",
    "        for word in words_list:\n",
    "            lower_word = word.lower()  # Convert to lowercase before checking\n",
    "            if lower_word in self.valid_words or lower_word in self.word_list:\n",
    "                valid_words.append(word)  # Word is valid\n",
    "            else:\n",
    "                invalid_words_list.append(word)  # Word is invalid\n",
    "\n",
    "        # Check invalid words against NLTK word list and spell checker\n",
    "        for word in invalid_words_list[:]:  # Iterate over a copy of the invalid list\n",
    "            lower_word = word.lower()  # Convert to lowercase before checking\n",
    "            if lower_word in self.word_list:\n",
    "                valid_found.append(word)  # Word found in NLTK word list\n",
    "                invalid_words_list.remove(word)  # Remove word from invalid list\n",
    "            else:\n",
    "                # Use the spell checker to find the closest valid word\n",
    "                corrected_word = self.spell_checker.correction(lower_word)\n",
    "                if corrected_word in self.word_list and corrected_word != lower_word:\n",
    "                    valid_found.append(corrected_word)  # Append the corrected word\n",
    "                    invalid_words_list.remove(word)  # Remove the original invalid word\n",
    "\n",
    "        # Append newly found valid words to the existing valid words list\n",
    "        valid_combined = valid_words + list(set(valid_found))  # Combine and remove duplicates\n",
    "        updated_invalid_words = \" \".join(invalid_words_list)  # Remaining invalid words\n",
    "\n",
    "        return \" \".join(valid_combined), updated_invalid_words\n",
    " \n",
    "\n",
    "    def apply_regex_filter(self, text):\n",
    "        \"\"\"Apply all regex patterns to input text recursively until no further changes.\"\"\"\n",
    "        text = self._ensure_text(text)\n",
    "        cleaned_text = text\n",
    "        previous_text = \"\"\n",
    "        \n",
    "        while cleaned_text != previous_text:  # Keep cleaning until text stops changing\n",
    "            previous_text = cleaned_text\n",
    "            for pattern in self.regex_filter_patterns:\n",
    "                cleaned_text = re.sub(pattern, ' ', cleaned_text)  # Replace matches with space\n",
    "        \n",
    "        return cleaned_text.split()  # Return a list of words\n",
    "\n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove stopwords from the list of tokens and return only unique tokens.\"\"\"\n",
    "        if isinstance(tokens, str):\n",
    "            tokens = tokens.split()  # If input is string, split it into tokens\n",
    "        return [word.lower() for word in tokens if word not in self.stop_words]\n",
    "\n",
    "    def drop_mentions(self, text):\n",
    "        \"\"\"Remove username mentions from the input text.\"\"\"\n",
    "        text = self._ensure_text(text)\n",
    "        pattern = r'@\\w+'\n",
    "        cleaned_text = re.sub(pattern, '', text)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "        return cleaned_text.lower().split()  # Return as list of tokens\n",
    "\n",
    "    def drop_hashtags(self, text):\n",
    "        \"\"\"Remove hashtags from the input text.\"\"\"\n",
    "        text = self._ensure_text(text)\n",
    "        pattern = r'#\\S+'\n",
    "        cleaned_text = re.sub(pattern, '', text)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "        return cleaned_text.lower().split()  # Return as list of tokens\n",
    "\n",
    "    def filter_valid_tokens(self, tokens):\n",
    "        \"\"\"Filter out invalid tokens based on word rules, keeping valid usernames starting with @.\"\"\"\n",
    "        if isinstance(tokens, str):\n",
    "            tokens = tokens.split()  # If input is string, split it into tokens\n",
    "    \n",
    "        # Regex pattern for a valid username: starts with '@', followed by letters, numbers, or underscores\n",
    "        username_pattern = r\"^@[a-zA-Z0-9_]+$\"\n",
    "        \n",
    "        return [\n",
    "            token.lower() for token in tokens\n",
    "            if (\n",
    "                # Keep tokens that match the valid username pattern (starting with '@')\n",
    "                re.match(username_pattern, token)\n",
    "                or\n",
    "                # Keep valid words with letters and numbers (e.g., 'word123')\n",
    "                (re.match(r\"^[a-zA-Z]+[0-9]*$\", token) and re.findall(r'[a-zA-Z]+', token)[0] in self.valid_words)\n",
    "                or\n",
    "                # Keep tokens starting with '#' followed by letters and numbers (e.g., '#hashtag')\n",
    "                (token.startswith(\"#\") and re.match(r\"^[#][a-zA-Z]+[0-9]*$\", token))\n",
    "            )\n",
    "        ]\n",
    " \n",
    "\n",
    "    def apply_tweet_tokenizer(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes the input text using the Twitter tokenizer.\n",
    "        This function is applied as a stage in the pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        text (str): The text to tokenize.\n",
    "        \n",
    "        Returns:\n",
    "        list: The list of tokenized words.\n",
    "        \"\"\"\n",
    "        tweet_tokenizer = TweetTokenizer()\n",
    "        return tweet_tokenizer.tokenize(str(text))\n",
    "\n",
    "    def _add_custom_components(self):\n",
    "        # Register the 'process_words' function as the first component in the pipeline\n",
    "        @Language.component(\"process_words\")\n",
    "        def process_words_component(doc):\n",
    "            processed_text, _ = self.process_words(doc.text)\n",
    "            doc._.processed_text = processed_text  # Store the processed text in the doc object\n",
    "            return doc\n",
    "\n",
    "        # Add the custom component to the pipeline as the first process\n",
    "        if \"process_words\" not in self.nlp.pipe_names:\n",
    "            self.nlp.add_pipe(\"process_words\", first=True)\n",
    "\n",
    "        @Language.component(\"regex_matcher\")\n",
    "        def regex_matcher(doc):\n",
    "            matches = []\n",
    "            for label, pattern in self.patterns.items():\n",
    "                for match in pattern.finditer(doc.text):\n",
    "                    span = doc.char_span(match.start(), match.end(), label=label)\n",
    "                    if span:\n",
    "                        matches.append((label, span.text))\n",
    "            doc._.custom_matches = matches\n",
    "            return doc\n",
    "        \n",
    "        @Language.component(\"date_extractor\")\n",
    "        def date_extractor(doc):\n",
    "            raw_dates = []\n",
    "            combined_pattern = '|'.join(self.date_patterns)\n",
    "            for match in re.findall(combined_pattern, doc.text):\n",
    "                raw_dates.append(match)\n",
    "            doc._.raw_dates = raw_dates\n",
    "            doc._.raw_date = random.choice(raw_dates) if raw_dates else None\n",
    "\n",
    "            doc._.date_info = self.convert_to_datetime(doc._.raw_date)\n",
    "            return doc\n",
    "\n",
    "        \n",
    "        @Language.component(\"token_cleaner\")\n",
    "        def token_cleaner(doc):\n",
    "            tokens = [token.text for token in doc]\n",
    "\n",
    "                        \n",
    "            # Run remove_stopwords only if it's in active_components\n",
    "            if \"apply_regex_filter\" in self.stages:\n",
    "                tokens = self.apply_regex_filter(tokens)\n",
    "                        \n",
    "            # Run remove_stopwords only if it's in active_components\n",
    "            if \"remove_stopwords\" in self.stages:\n",
    "                tokens = self.remove_stopwords(tokens)\n",
    "            \n",
    "            # Run filter_valid_tokens only if it's in active_components\n",
    "            if \"filter_valid_tokens\" in self.stages:\n",
    "                tokens = self.filter_valid_tokens(tokens)\n",
    "                \n",
    "            # Run drop_mentions only if it's in active_components\n",
    "            if \"drop_mentions\" in self.stages:\n",
    "                tokens = self.drop_mentions(tokens)\n",
    "            \n",
    "            # Run drop_hashtags only if it's in active_components\n",
    "            if \"drop_hashtags\" in self.stages:\n",
    "                tokens = self.drop_hashtags(tokens)\n",
    "            \n",
    "            doc._.cleaned_tokens = tokens\n",
    "            return doc\n",
    "\n",
    "        # Add custom extensions to store cleaned tokens\n",
    "        \n",
    "        if not spacy.tokens.Doc.has_extension('processed_text'):\n",
    "            spacy.tokens.Doc.set_extension('processed_text', default=None)\n",
    "        if not spacy.tokens.Doc.has_extension(\"cleaned_tokens\"):\n",
    "            spacy.tokens.Doc.set_extension(\"cleaned_tokens\", default=[])\n",
    "        if not spacy.tokens.Doc.has_extension(\"custom_matches\"):\n",
    "            spacy.tokens.Doc.set_extension(\"custom_matches\", default=[])\n",
    "        if not spacy.tokens.Doc.has_extension(\"raw_dates\"):\n",
    "            spacy.tokens.Doc.set_extension(\"raw_dates\", default=[])\n",
    "        if not spacy.tokens.Doc.has_extension(\"raw_date\"):\n",
    "            spacy.tokens.Doc.set_extension(\"raw_date\", default=None)\n",
    "        if not spacy.tokens.Doc.has_extension(\"date_info\"):\n",
    "            spacy.tokens.Doc.set_extension(\"date_info\", default=(None, None)) \n",
    "        if not spacy.tokens.Doc.has_extension(\"cleaned_tokens\"):\n",
    "            spacy.tokens.Doc.set_extension(\"cleaned_tokens\", default=[])\n",
    "\n",
    "        # Add components to pipeline \n",
    "        self.nlp.add_pipe(\"token_cleaner\", first=True)\n",
    "        self.nlp.add_pipe(\"regex_matcher\", last=True)\n",
    "        self.nlp.add_pipe(\"date_extractor\", last=True)\n",
    "\n",
    "\n",
    "    def convert_to_datetime(self, raw_date):\n",
    "        if not raw_date:\n",
    "            return float('nan'), float('nan')\n",
    "        \n",
    "        try:\n",
    "            date = pd.to_datetime(raw_date, format='%d/%m/%Y')\n",
    "            return date, 'full'\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            date = pd.to_datetime(raw_date, format='%d-%m-%Y')\n",
    "            return date, 'full'\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            date = pd.to_datetime(raw_date, format='%B, %d %Y')\n",
    "            return date, 'full'\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            date = pd.to_datetime(raw_date, format='%B %d %Y')\n",
    "            return date, 'full'\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            date = pd.to_datetime(raw_date, format='%B %Y')\n",
    "            date = date.replace(day=1)  # impute day\n",
    "            return date, 'month and year'\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            date = pd.to_datetime(raw_date, format='%B')\n",
    "            return date, 'month'\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            date = pd.to_datetime(raw_date, format='%Y')\n",
    "            date = date.replace(day=1, month=1)\n",
    "            return date, 'year'\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        return float('nan'), float('nan')\n",
    "\n",
    "    def process_text(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        cleaned_tokens = doc._.cleaned_tokens\n",
    "        regex_matches = [(label, match) for label, match in doc._.custom_matches]\n",
    "        named_entities = [(ent.label_, ent.text) for ent in doc.ents]\n",
    "        raw_dates = doc._.raw_dates\n",
    "        date_info = doc._.date_info\n",
    "    \n",
    "        # Combine regex matches and named entities\n",
    "        all_entities = regex_matches + named_entities\n",
    "    \n",
    "        # Group entities by their labels into a dictionary\n",
    "        entity_dict = {}\n",
    "        for label, value in all_entities:\n",
    "            if label not in entity_dict:\n",
    "                entity_dict[label] = []\n",
    "            entity_dict[label].append(value)\n",
    "    \n",
    "        # Return the processed results\n",
    "        return cleaned_tokens, entity_dict, raw_dates, date_info\n",
    "\n",
    "\n",
    "\n",
    "    def count_entity_label(self, row, entity_label):\n",
    "        # Retrieve the dictionary of entities for the row\n",
    "        entities_dict = row['entities']\n",
    "    \n",
    "        # Count the occurrences of the specified entity label\n",
    "        return len(entities_dict.get(entity_label, []))\n",
    "\n",
    "    def add_entity_counts(self, df):\n",
    "        # First, create a new column containing all entities for each row\n",
    "        df['entities'] = df['text'].apply(lambda x: self.process_text(x)[1])  # Assuming process_text returns entities as 2nd element\n",
    "    \n",
    "        # Extract unique entity labels across the entire DataFrame\n",
    "        entity_labels = set()\n",
    "        for entities_dict in df['entities']:\n",
    "            entity_labels.update(entities_dict.keys())\n",
    "    \n",
    "        # Sort entity labels to ensure consistent column ordering\n",
    "        entity_labels = sorted(entity_labels)\n",
    "    \n",
    "        # Now create a column for each entity label to count occurrences\n",
    "        for entity_label in entity_labels:\n",
    "            column_name = f\"{entity_label.lower()}_count\"\n",
    "            df[column_name] = df['entities'].apply(\n",
    "                lambda entities_dict: len(entities_dict.get(entity_label, []))\n",
    "            )\n",
    "    \n",
    "        return df\n",
    "\n",
    "        \n",
    "    def add_date_counts(self, df):\n",
    "        df['year_or_month_count'] = df['raw_dates'].apply(lambda x: sum([1 for date in x if len(date.split()) <= 2]))\n",
    "        df['full_date_count'] = df['raw_dates'].apply(lambda x: sum([1 for date in x if len(date.split()) == 3]))\n",
    "        return df\n",
    " \n",
    "    def process_dataframe(self, df):\n",
    "        df['cleaned_tokens'], df['entities'], df['raw_dates'], df['date_info'] = zip(\n",
    "            *df['text'].map(self.process_text)\n",
    "        )\n",
    "        df[\"date_month\"], df[\"date_year\"] = zip(*df[\"date_info\"])\n",
    "        df = self.add_entity_counts(df)\n",
    "        df = self.add_date_counts(df)        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def apply_stages_incrementally(self, df, text_column):\n",
    "        \"\"\"\n",
    "        Apply the stages incrementally and add a column for each stage with the name f'stage{i}'.\n",
    "        Generate a bar plot for the count of unique tokens after each filtering stage,\n",
    "        with the value of each bar displayed on top.\n",
    "    \n",
    "        :param df: Input DataFrame containing the text data.\n",
    "        :param text_column: The name of the column containing the input text.\n",
    "        :return: DataFrame with additional columns for each stage.\n",
    "        \"\"\"\n",
    "        # Lists to hold data for plotting\n",
    "        filter_steps = []\n",
    "        unique_token_counts = []\n",
    "        \n",
    "        for i, stage in enumerate(self.stages, start=1):\n",
    "            # Check if the stage is a valid method in the class\n",
    "            if hasattr(self, stage):\n",
    "                method = getattr(self, stage)\n",
    "                # Apply the method for the current stage and create a new column with name f'stage{i}'\n",
    "                new_column_name = f'stage{i}'\n",
    "                df[new_column_name] = df[text_column].apply(method)\n",
    "                \n",
    "                # Collect data for plotting\n",
    "                filter_steps.append(new_column_name)\n",
    "                tokens = df[new_column_name].explode().dropna().tolist()  # Flatten tokens for counting\n",
    "                unique_token_counts.append(len(set(tokens)))  # Count unique tokens\n",
    "                \n",
    "                # Update the text_column to reflect the latest processing stage\n",
    "                text_column = new_column_name\n",
    "            else:\n",
    "                # If the stage is not a valid method, raise an error\n",
    "                raise ValueError(f\"Stage '{stage}' is not a valid method.\")\n",
    "        \n",
    "        # Create a figure for the bar chart\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        # Adjust bar width to reduce space between bars\n",
    "        bar_width = 0.4  # Reduce bar width to make bars thinner\n",
    "        x_positions = range(len(filter_steps))  # Set the x positions for the bars directly adjacent\n",
    "        \n",
    "        # Make sure bars are placed directly on top of each other by setting width\n",
    "        bars = ax.bar(x_positions, unique_token_counts, color='skyblue', width=bar_width)\n",
    "        \n",
    "        # Annotate each bar with its value on top\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()  # Get the height (value) of the bar\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2,  # x position (center of the bar)\n",
    "                yval,  # y position (top of the bar)\n",
    "                f'{yval}',  # Text to display (value of the bar)\n",
    "                ha='center',  # Horizontal alignment\n",
    "                va='bottom',  # Vertical alignment\n",
    "                fontsize=10  # Font size for the annotation\n",
    "            )\n",
    "        \n",
    "        # Customize the plot\n",
    "        ax.set_xlabel('Filtering Steps')\n",
    "        ax.set_ylabel('Count of Unique Tokens')\n",
    "        ax.set_title('Count of Unique Tokens After Each Filtering Step')\n",
    "        \n",
    "        # Set x-tick labels to the names of the filtering steps\n",
    "        ax.set_xticks(x_positions)\n",
    "        ax.set_xticklabels(filter_steps)\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "        # Adjust the layout to avoid overlap and make the plot more compact\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return df\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9635a-5f55-4e86-965e-9ce4fbdfafa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training file: train.csv, Shape: (7613, 5)\n",
      "All training data concatenated successfully. Shape: (7613, 5)\n",
      "Test data loaded successfully. Shape: (3263, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, train_files, test_file=None, text_column='text', target_column='target'):\n",
    "        self.train_files = train_files\n",
    "        self.test_file = test_file\n",
    "        self.text_column = text_column\n",
    "        self.target_column = target_column\n",
    "\n",
    "        # DataProcessingPipeline setup\n",
    "        self.pipeline = TokenPipeline(\n",
    "            train_files=train_files,\n",
    "            test_file=test_file,\n",
    "            text_column=text_column,\n",
    "            target_column=target_column\n",
    "        )\n",
    "        \n",
    "        # FeatureCorrelation setup (initialized later)\n",
    "        self.feature_corr = None\n",
    "        \n",
    "        # Class-level variables for storing data\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        self.feature_df = None\n",
    "        self.combined_df = None\n",
    "        self.dropped_columns = {\n",
    "            'train_tokens': [],\n",
    "            'feature': []\n",
    "        }\n",
    "\n",
    "        # Initialize results dictionary\n",
    "        self.results = {}\n",
    "\n",
    "    def execute_pipeline(self, lemmatization_columns=None, method='nltk'):\n",
    "        # Step 2: Access the processed train and test data\n",
    "        train_tokens, test_tokens = self.pipeline.process_and_move_columns()\n",
    "        self.train_data = train_tokens[0] \n",
    "        self.test_data  = test_tokens[0]  \n",
    "\n",
    "        # Store the processed tokens in the results dictionary\n",
    "        self.results[\"train_tokens_df\"] = self.train_data\n",
    "        self.results[\"test_tokens_df\"] = self.test_data\n",
    "\n",
    "        # Step 3: Extract features from both train and test data (Ensure correct feature extraction)\n",
    "        self.feature_df = self.pipeline.extract_features()  # Assuming this method extracts features from processed data\n",
    "\n",
    "        # Store the extracted features in the results dictionary\n",
    "        self.results[\"train_features_df\"] = self.feature_df\n",
    "        self.results[\"test_features_df\"] = None\n",
    "        if self.test_data is not None:\n",
    "            self.results[\"test_features_df\"] = self.pipeline.extract_features()\n",
    "\n",
    "        # Step 4: Initialize FeatureCorrelation with processed data\n",
    "        self.feature_corr = FeatureCorrelation(\n",
    "            train_tokens_df=self.train_data,  # Use train tokens for correlation\n",
    "            feature_df=self.feature_df,            # Use feature DataFrame from processed data\n",
    "            target_column=self.target_column\n",
    "        )\n",
    "\n",
    "        # Step 5: Perform feature correlation and remove multicollinearity\n",
    "        self.combined_df = self.feature_corr.process_and_plot()\n",
    "\n",
    "        # Store the combined DataFrame in the results dictionary\n",
    "        self.results[\"combined_df\"] = self.combined_df\n",
    "\n",
    "        # Step 6: Store dropped columns due to multicollinearity\n",
    "        self.dropped_columns['train_tokens'] = self.feature_corr.remove_multicollinearity(\n",
    "            self.train_data, \n",
    "            self.feature_corr.calculate_correlation(self.feature_corr.encode_non_numeric(self.train_data))\n",
    "        )\n",
    "        self.dropped_columns['feature'] = self.feature_corr.remove_multicollinearity(\n",
    "            self.feature_df, \n",
    "            self.feature_corr.calculate_correlation(self.feature_corr.encode_non_numeric(self.feature_df))\n",
    "        )\n",
    "\n",
    "        # Store dropped columns information in the results dictionary\n",
    "        self.results[\"dropped_columns\"] = self.dropped_columns\n",
    "\n",
    "        # Step 7: Apply lemmatization if columns are provided\n",
    "        if lemmatization_columns:\n",
    "            # Apply lemmatization to both train and test data if columns are provided\n",
    "            self.train_data = TokenPipeline.lemmatize_df(self.train_data, column_names=lemmatization_columns, method=method)\n",
    "            if self.test_data is not None:\n",
    "                self.test_data = TokenPipeline.lemmatize_df(self.test_data, column_names=lemmatization_columns, method=method)\n",
    "\n",
    "            # Store the lemmatized data in the results dictionary\n",
    "            self.results[\"lemmatized_train_df\"] = self.train_data\n",
    "            self.results[\"lemmatized_test_df\"] = self.test_data if self.test_data is not None else None\n",
    "\n",
    "# Example usage\n",
    "train_files = ['train.csv']  # Replace with actual training data file paths\n",
    "test_file = 'test.csv'  # Replace with the actual test file path\n",
    "\n",
    "# Instantiate the DataProcessor class\n",
    "data_processor = DataProcessor(\n",
    "    train_files=train_files,\n",
    "    test_file=test_file,\n",
    "    text_column='text',      # Replace with the actual column name for text\n",
    "    target_column='target'   # Replace with the actual column name for target\n",
    ")\n",
    "\n",
    "# Execute the pipeline and apply lemmatization on specific columns\n",
    "lemmatization_columns = ['stage6', 'stage5']  # Specify the columns to lemmatize\n",
    "data_processor.execute_pipeline(lemmatization_columns=lemmatization_columns, method='nltk')\n",
    "\n",
    "# Loop through the results dictionary and display each dataframe\n",
    "for key, df in data_processor.results.items():\n",
    "    if isinstance(df, pd.DataFrame):  # Check if the result is a DataFrame\n",
    "        display(Markdown(f\"### {key}\"))\n",
    "        display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1e1e0-b050-47ae-bc1f-cfbae4207648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emoji\n",
    "#!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "id": "55c788dd-ec88-40a1-bafb-4c2661f0d1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/AlbertLeo1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from emoji import is_emoji\n",
    "\n",
    "# Ensure that necessary NLTK data is downloaded\n",
    "nltk.download('punkt')\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "9958f307-ac5a-454d-89bb-e29b84f03c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TokenPipeline:\n",
    "    def __init__(self, train_files, test_file=None, stages=None, text_column=\"text\", target_column=None):\n",
    "        self.loader = DataHandler(train_files=train_files, test_file=test_file)\n",
    "        self.loader.load_data()  # Load the data using DataHandler\n",
    "        \n",
    "        self.train_data = self.loader.train_data\n",
    "        self.test_data = self.loader.test_data\n",
    "        self.target_column = target_column  # Add target_column here\n",
    "        \n",
    "        self.stages = stages if stages else [\n",
    "            \"apply_tweet_tokenizer\",  \n",
    "            \"apply_regex_filter\",\n",
    "            \"remove_stopwords\",\n",
    "            \"filter_valid_tokens\",\n",
    "            \"drop_mentions\",\n",
    "            \"drop_hashtags\",\n",
    "        ]\n",
    "        self.text_column = text_column\n",
    "        self.feature_columns = []\n",
    "        \n",
    "        self.processor = TextProcessor(stages=self.stages)\n",
    "\n",
    "\n",
    "    def prepare_and_process_data(self):\n",
    "        processed_train_data = self.processor.apply_stages_incrementally(self.train_data, self.text_column)\n",
    "        self.processor.process_dataframe(processed_train_data)\n",
    "\n",
    "        processed_test_data = None\n",
    "        if self.test_data is not None:\n",
    "            processed_test_data = self.processor.apply_stages_incrementally(self.test_data, self.text_column)\n",
    "            self.processor.process_dataframe(processed_test_data)\n",
    "            \n",
    "        return processed_train_data, processed_test_data\n",
    "    \n",
    "    def extract_features(self):\n",
    "        # Initialize feature columns\n",
    "        self.feature_columns = [\n",
    "            'text_length', 'word_count', 'exclamation_count', 'capitalized_words_pct', \n",
    "            'sentiment_polarity', 'sentiment_subjectivity', 'url_count', \n",
    "            'emoji_count', 'unique_word_count', 'noun_count', 'verb_count', \n",
    "            'adj_count', 'adverb_count', 'time_of_day', 'readability_score'\n",
    "        ]\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.train_data[self.text_column].apply(self._extract_features_from_text)\n",
    "        \n",
    "        # Create DataFrame from the features list\n",
    "        feature_df = pd.DataFrame(features.tolist(), columns=self.feature_columns)\n",
    "        \n",
    "        return feature_df \n",
    "    \n",
    "    def _extract_features_from_text(self, text):\n",
    "        text = str(text)  # Ensure it's a string\n",
    "        doc = nlp(text)  # Process text using SpaCy\n",
    "\n",
    "        text_length = len(text)\n",
    "        word_count = len([token for token in doc if token.is_alpha])\n",
    "        exclamation_count = text.count('!')\n",
    "\n",
    "        capitalized_words = [token.text for token in doc if token.is_upper]\n",
    "        capitalized_words_pct = len(capitalized_words) / word_count if word_count else 0\n",
    "\n",
    "        sentiment_polarity = None  # Placeholder (SpaCy doesn't have built-in sentiment)\n",
    "        sentiment_subjectivity = None\n",
    "\n",
    "        url_count = len([token for token in doc if token.like_url])\n",
    "        emoji_count = len([token.text for token in doc if is_emoji(token.text)])\n",
    "        \n",
    "        unique_word_count = len(set([token.text.lower() for token in doc if token.is_alpha]))\n",
    "        \n",
    "        noun_count = sum(1 for token in doc if token.pos_ == \"NOUN\")\n",
    "        verb_count = sum(1 for token in doc if token.pos_ == \"VERB\")\n",
    "        adj_count = sum(1 for token in doc if token.pos_ == \"ADJ\")\n",
    "        adverb_count = sum(1 for token in doc if token.pos_ == \"ADV\")\n",
    "\n",
    "        time_of_day = None  # Placeholder (needs additional timestamp logic)\n",
    "\n",
    "        syllables = sum(self._syllables_in_word(token.text) for token in doc if token.is_alpha)\n",
    "        sentences = len(list(doc.sents))\n",
    "        readability_score = 0\n",
    "        if sentences > 0 and word_count > 0:\n",
    "            readability_score = 0.39 * (word_count / sentences) + 11.8 * (syllables / word_count) - 15.59\n",
    "\n",
    "        return [\n",
    "            text_length, word_count, exclamation_count, capitalized_words_pct, \n",
    "            sentiment_polarity, sentiment_subjectivity, url_count, \n",
    "            emoji_count, unique_word_count, noun_count, verb_count, \n",
    "            adj_count, adverb_count, time_of_day, readability_score\n",
    "        ]\n",
    "\n",
    "    def _syllables_in_word(self, word):\n",
    "        word = word.lower()\n",
    "        vowels = \"aeiouy\"\n",
    "        syllables = 0\n",
    "        prev_char = \"\"\n",
    "\n",
    "        for char in word:\n",
    "            if char in vowels and prev_char not in vowels:\n",
    "                syllables += 1\n",
    "            prev_char = char\n",
    "\n",
    "        if word.endswith('e'):\n",
    "            syllables -= 1\n",
    "\n",
    "        return max(syllables, 1)\n",
    "\n",
    "    def move_count_columns(self, df):\n",
    "        count_columns = [col for col in df.columns if col.endswith('count') or col in self.feature_columns]\n",
    "        feature_df = df[count_columns].copy()\n",
    "        df.drop(columns=count_columns, inplace=True)\n",
    "        return df, feature_df\n",
    "\n",
    "    def process_and_move_columns(self):\n",
    "        processed_train_data, processed_test_data = self.prepare_and_process_data()\n",
    "        train_tokens_df, train_entity_count_df = self.move_count_columns(processed_train_data)\n",
    "\n",
    "        test_tokens_df, test_entity_count_df = None, None\n",
    "        if processed_test_data is not None:\n",
    "            test_tokens_df, test_entity_count_df = self.move_count_columns(processed_test_data)\n",
    "\n",
    "        return (train_tokens_df, train_entity_count_df), (test_tokens_df, test_entity_count_df)\n",
    "\n",
    " \n",
    "    @staticmethod\n",
    "    def lemmatize_df(df, column_names, method='nltk'):\n",
    "        \"\"\"\n",
    "        Applies lemmatization on the given DataFrame columns based on the chosen method.\n",
    "        \"\"\"\n",
    "        # Ensure column_names is a list (if a single string is passed, convert it to a list)\n",
    "        if isinstance(column_names, str):\n",
    "            column_names = [column_names]\n",
    "\n",
    "        # Apply lemmatization to each specified column\n",
    "        for column_name in column_names:\n",
    "            # Ensure the column contains strings, and handle missing values\n",
    "            df[column_name] = df[column_name].fillna('').astype(str)\n",
    "\n",
    "            # Apply lemmatization to each entry in the column\n",
    "            df[column_name] = df[column_name].apply(lambda x: TokenPipeline.lemmatize_text(x, method))\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def lemmatize_text(text, method='nltk'):\n",
    "        \"\"\"\n",
    "        Perform lemmatization on a single text entry using the specified method (e.g., 'nltk').\n",
    "        \"\"\"\n",
    "        # Replace with actual lemmatization logic\n",
    "        # For simplicity, using the method argument to choose a method\n",
    "        if method == 'nltk':\n",
    "            # Implement NLTK-based lemmatization (this is just a placeholder)\n",
    "            return text  # Replace with actual lemmatization logic\n",
    "        else:\n",
    "            return text  # Placeholder, implement other methods if needed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a82625-8b3f-42b8-91c7-4339dfef8beb",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "e9d93d70-80a7-4f6c-af20-6cd2a7796aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FeatureCorrelation:\n",
    "    def __init__(self, train_tokens_df, feature_df, target_column, drop_stage_columns=False, exclude_columns='entities'):\n",
    "        self.train_tokens_df = train_tokens_df\n",
    "        self.feature_df = feature_df  # Directly pass feature_df\n",
    "        self.target_column = target_column\n",
    "        self.drop_stage_columns = drop_stage_columns\n",
    "        self.exclude_columns = exclude_columns if exclude_columns is not None else []\n",
    "\n",
    "    def encode_non_numeric(self, df):\n",
    "        \"\"\"\n",
    "        Encode non-numeric columns in the DataFrame into numeric values.\n",
    "        Converts lists and dictionaries to strings and factorizes categorical values.\n",
    "        \"\"\"\n",
    "        encoded_df = df.copy()\n",
    "        for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "            # Convert lists and dicts to strings for consistent processing\n",
    "            encoded_df[col] = encoded_df[col].apply(\n",
    "                lambda x: str(x) if isinstance(x, (list, dict)) else x\n",
    "            )\n",
    "            # Factorize the column to convert to numeric\n",
    "            encoded_df[col] = pd.factorize(encoded_df[col])[0]\n",
    "        return encoded_df\n",
    "\n",
    "    def calculate_correlation(self, df):\n",
    "        \"\"\"\n",
    "        Calculate the correlation matrix for a DataFrame.\n",
    "        \"\"\"\n",
    "        return df.corr()\n",
    "\n",
    "    def plot_heatmap(self, corr_matrix, title):\n",
    "        \"\"\"\n",
    "        Plot a heatmap for the given correlation matrix.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")  # annot=True to show values\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def remove_multicollinearity(self, df, corr_matrix, threshold=0.9):\n",
    "        \"\"\"\n",
    "        Remove columns with high correlation (above the threshold).\n",
    "        Returns a list of columns to drop.\n",
    "        Excludes the columns specified in self.exclude_columns from the check.\n",
    "        \"\"\"\n",
    "        correlated_features = set()\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i):\n",
    "                # Skip the columns specified in exclude_columns\n",
    "                colname_i = corr_matrix.columns[i]\n",
    "                colname_j = corr_matrix.columns[j]\n",
    "                if colname_i in self.exclude_columns or colname_j in self.exclude_columns:\n",
    "                    continue\n",
    "                if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                    correlated_features.add(colname_i)\n",
    "                    correlated_features.add(colname_j)\n",
    "        return list(correlated_features)\n",
    "\n",
    "    def process_and_plot(self):\n",
    "        \"\"\"\n",
    "        Process the input DataFrame (`train_tokens_df`) and `feature_df`.\n",
    "        This includes encoding non-numeric data, calculating correlation matrices,\n",
    "        removing multicollinearity, and optionally dropping stage-related columns.\n",
    "        Returns the final combined DataFrame.\n",
    "        \"\"\"\n",
    "        # Check if DataFrames are empty\n",
    "        if self.train_tokens_df.empty:\n",
    "            print(\"Warning: train_tokens_df is empty.\")\n",
    "            return pd.DataFrame()\n",
    "        if self.feature_df.empty:\n",
    "            print(\"Warning: feature_df is empty.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Encode non-numeric columns\n",
    "        train_tokens_df_encoded = self.encode_non_numeric(self.train_tokens_df)\n",
    "        feature_df_encoded = self.encode_non_numeric(self.feature_df)\n",
    "\n",
    "        # Calculate correlation matrices\n",
    "        train_tokens_corr = self.calculate_correlation(train_tokens_df_encoded)\n",
    "        feature_corr = self.calculate_correlation(feature_df_encoded)\n",
    "\n",
    "        # Plot heatmaps\n",
    "        self.plot_heatmap(train_tokens_corr, \"Feature Correlation Matrix for train_tokens_df\")\n",
    "        self.plot_heatmap(feature_corr, \"Feature Correlation Matrix for feature_df\")\n",
    "\n",
    "        # Remove multicollinearity\n",
    "        dropped_train_tokens = self.remove_multicollinearity(train_tokens_df_encoded, train_tokens_corr)\n",
    "        dropped_feature = self.remove_multicollinearity(feature_df_encoded, feature_corr)\n",
    "\n",
    "        # Filter out dropped columns\n",
    "        final_train_tokens_df = self.train_tokens_df.drop(columns=dropped_train_tokens, errors='ignore')\n",
    "        final_feature_df = self.feature_df.drop(columns=dropped_feature, errors='ignore')\n",
    "\n",
    "        # Optionally drop stage columns\n",
    "        if self.drop_stage_columns:\n",
    "            stage_columns = [col for col in final_feature_df.columns if 'stage' in col]\n",
    "            final_feature_df = final_feature_df.drop(columns=stage_columns, errors='ignore')\n",
    "\n",
    "        # Combine cleaned DataFrames\n",
    "        combined_df = pd.concat([final_train_tokens_df, final_feature_df], axis=1)\n",
    "\n",
    "        # Drop columns with all NaN values\n",
    "        combined_df = combined_df.dropna(axis=1, how='all')\n",
    "\n",
    "        # Display results\n",
    "        print(f\"Dropped columns from train_tokens_df due to multicollinearity: {dropped_train_tokens}\")\n",
    "        print(f\"Dropped columns from feature_df due to multicollinearity: {dropped_feature}\")\n",
    "        print(f\"Remaining columns in train_tokens_df: {final_train_tokens_df.columns.tolist()}\")\n",
    "        print(f\"Remaining columns in feature_df: {final_feature_df.columns.tolist()}\")\n",
    "        print(\"Combined DataFrame after removing multicollinearity and NaN-only columns:\")\n",
    "\n",
    "        return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa60b604-1177-4e99-b71a-9ab90371f5d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "id": "b33350ba-e9a6-4f44-9e48-fceba5516916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training file: train.csv, Shape: (7613, 5)\n",
      "All training data concatenated successfully. Shape: (7613, 5)\n",
      "Test data loaded successfully. Shape: (3263, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E007] 'process_words' already exists in pipeline. Existing names: ['token_cleaner', 'process_words', 'tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[811], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m test_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with the actual test file path\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Instantiate the DataProcessor class\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m data_processor \u001b[38;5;241m=\u001b[39m DataProcessor(\n\u001b[1;32m     96\u001b[0m     train_files\u001b[38;5;241m=\u001b[39mtrain_files,\n\u001b[1;32m     97\u001b[0m     test_file\u001b[38;5;241m=\u001b[39mtest_file,\n\u001b[1;32m     98\u001b[0m     text_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m,      \u001b[38;5;66;03m# Replace with the actual column name for text\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     target_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m   \u001b[38;5;66;03m# Replace with the actual column name for target\u001b[39;00m\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Execute the pipeline and apply lemmatization on specific columns\u001b[39;00m\n\u001b[1;32m    103\u001b[0m lemmatization_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstage6\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstage5\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Specify the columns to lemmatize\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[811], line 11\u001b[0m, in \u001b[0;36mDataProcessor.__init__\u001b[0;34m(self, train_files, test_file, text_column, target_column)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_column \u001b[38;5;241m=\u001b[39m target_column\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# DataProcessingPipeline setup\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline \u001b[38;5;241m=\u001b[39m TokenPipeline(\n\u001b[1;32m     12\u001b[0m     train_files\u001b[38;5;241m=\u001b[39mtrain_files,\n\u001b[1;32m     13\u001b[0m     test_file\u001b[38;5;241m=\u001b[39mtest_file,\n\u001b[1;32m     14\u001b[0m     text_column\u001b[38;5;241m=\u001b[39mtext_column,\n\u001b[1;32m     15\u001b[0m     target_column\u001b[38;5;241m=\u001b[39mtarget_column\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# FeatureCorrelation setup (initialized later)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_corr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[807], line 21\u001b[0m, in \u001b[0;36mTokenPipeline.__init__\u001b[0;34m(self, train_files, test_file, stages, text_column, target_column)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_column \u001b[38;5;241m=\u001b[39m text_column\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_columns \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor \u001b[38;5;241m=\u001b[39m TextProcessor(stages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages)\n",
      "Cell \u001b[0;32mIn[802], line 56\u001b[0m, in \u001b[0;36mTextProcessor.__init__\u001b[0;34m(self, stages)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate_patterns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb(?:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,2}[./-]\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,2}[./-]\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m2,4})\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb(?:January|February|March|April|May|June|July|August|September|October|November|December) \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,2},? \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{4}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb(?:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m1,2}(?:st|nd|rd|th)? of (?:January|February|March|April|May|June|July|August|September|October|November|December),? \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;132;01m{4}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     40\u001b[0m ]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregex_filter_patterns \u001b[38;5;241m=\u001b[39m [ \n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps?://\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+|www\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m'\u001b[39m,            \u001b[38;5;66;03m# Drop all links (URLs).\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#(?!\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS)\u001b[39m\u001b[38;5;124m'\u001b[39m,                          \u001b[38;5;66;03m# Drop standalone hashtags (e.g., #).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(?<=\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw)[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw#@]+(?=\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms|$)\u001b[39m\u001b[38;5;124m'\u001b[39m,          \u001b[38;5;66;03m# Remove unwanted characters following words, hashtags, or mentions.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m ]\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_custom_components()\n",
      "Cell \u001b[0;32mIn[802], line 291\u001b[0m, in \u001b[0;36mTextProcessor._add_custom_components\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# Add components to pipeline \u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_cleaner\u001b[39m\u001b[38;5;124m\"\u001b[39m, first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess_words\u001b[39m\u001b[38;5;124m\"\u001b[39m, first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregex_matcher\u001b[39m\u001b[38;5;124m\"\u001b[39m, last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp\u001b[38;5;241m.\u001b[39madd_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_extractor\u001b[39m\u001b[38;5;124m\"\u001b[39m, last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.12/site-packages/spacy/language.py:810\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    808\u001b[0m name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m factory_name\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names:\n\u001b[0;32m--> 810\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE007\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, opts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_names))\n\u001b[1;32m    811\u001b[0m \u001b[38;5;66;03m# Overriding pipe name in the config is not supported and will be ignored.\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n",
      "\u001b[0;31mValueError\u001b[0m: [E007] 'process_words' already exists in pipeline. Existing names: ['token_cleaner', 'process_words', 'tok2vec', 'tagger', 'parser', 'senter', 'attribute_ruler', 'lemmatizer', 'ner']"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, train_files, test_file=None, text_column='text', target_column='target'):\n",
    "        self.train_files = train_files\n",
    "        self.test_file = test_file\n",
    "        self.text_column = text_column\n",
    "        self.target_column = target_column\n",
    "\n",
    "        # DataProcessingPipeline setup\n",
    "        self.pipeline = TokenPipeline(\n",
    "            train_files=train_files,\n",
    "            test_file=test_file,\n",
    "            text_column=text_column,\n",
    "            target_column=target_column\n",
    "        )\n",
    "        \n",
    "        # FeatureCorrelation setup (initialized later)\n",
    "        self.feature_corr = None\n",
    "        \n",
    "        # Class-level variables for storing data\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "        self.feature_df = None\n",
    "        self.combined_df = None\n",
    "        self.dropped_columns = {\n",
    "            'train_tokens': [],\n",
    "            'feature': []\n",
    "        }\n",
    "\n",
    "        # Initialize results dictionary\n",
    "        self.results = {}\n",
    "\n",
    "    def execute_pipeline(self, lemmatization_columns=None, method='nltk'):\n",
    "        # Step 2: Access the processed train and test data\n",
    "        train_tokens, test_tokens = self.pipeline.process_and_move_columns()\n",
    "        self.train_data = train_tokens[0] \n",
    "        self.test_data  = test_tokens[0]  \n",
    "\n",
    "        # Store the processed tokens in the results dictionary\n",
    "        self.results[\"train_tokens_df\"] = self.train_data\n",
    "        self.results[\"test_tokens_df\"] = self.test_data\n",
    "\n",
    "        # Step 3: Extract features from both train and test data (Ensure correct feature extraction)\n",
    "        self.feature_df = self.pipeline.extract_features()  # Assuming this method extracts features from processed data\n",
    "\n",
    "        # Store the extracted features in the results dictionary\n",
    "        self.results[\"train_features_df\"] = self.feature_df\n",
    "        self.results[\"test_features_df\"] = None\n",
    "        if self.test_data is not None:\n",
    "            self.results[\"test_features_df\"] = self.pipeline.extract_features()\n",
    "\n",
    "        # Step 4: Initialize FeatureCorrelation with processed data\n",
    "        self.feature_corr = FeatureCorrelation(\n",
    "            train_tokens_df=self.train_data,  # Use train tokens for correlation\n",
    "            feature_df=self.feature_df,            # Use feature DataFrame from processed data\n",
    "            target_column=self.target_column\n",
    "        )\n",
    "\n",
    "        # Step 5: Perform feature correlation and remove multicollinearity\n",
    "        self.combined_df = self.feature_corr.process_and_plot()\n",
    "\n",
    "        # Store the combined DataFrame in the results dictionary\n",
    "        self.results[\"combined_df\"] = self.combined_df\n",
    "\n",
    "        # Step 6: Store dropped columns due to multicollinearity\n",
    "        self.dropped_columns['train_tokens'] = self.feature_corr.remove_multicollinearity(\n",
    "            self.train_data, \n",
    "            self.feature_corr.calculate_correlation(self.feature_corr.encode_non_numeric(self.train_data))\n",
    "        )\n",
    "        self.dropped_columns['feature'] = self.feature_corr.remove_multicollinearity(\n",
    "            self.feature_df, \n",
    "            self.feature_corr.calculate_correlation(self.feature_corr.encode_non_numeric(self.feature_df))\n",
    "        )\n",
    "\n",
    "        # Store dropped columns information in the results dictionary\n",
    "        self.results[\"dropped_columns\"] = self.dropped_columns\n",
    "\n",
    "        # Step 7: Apply lemmatization if columns are provided\n",
    "        if lemmatization_columns:\n",
    "            # Apply lemmatization to both train and test data if columns are provided\n",
    "            self.train_data = TokenPipeline.lemmatize_df(self.train_data, column_names=lemmatization_columns, method=method)\n",
    "            if self.test_data is not None:\n",
    "                self.test_data = TokenPipeline.lemmatize_df(self.test_data, column_names=lemmatization_columns, method=method)\n",
    "\n",
    "            # Store the lemmatized data in the results dictionary\n",
    "            self.results[\"lemmatized_train_df\"] = self.train_data\n",
    "            self.results[\"lemmatized_test_df\"] = self.test_data if self.test_data is not None else None\n",
    "\n",
    "# Example usage\n",
    "train_files = ['train.csv']  # Replace with actual training data file paths\n",
    "test_file = 'test.csv'  # Replace with the actual test file path\n",
    "\n",
    "# Instantiate the DataProcessor class\n",
    "data_processor = DataProcessor(\n",
    "    train_files=train_files,\n",
    "    test_file=test_file,\n",
    "    text_column='text',      # Replace with the actual column name for text\n",
    "    target_column='target'   # Replace with the actual column name for target\n",
    ")\n",
    "\n",
    "# Execute the pipeline and apply lemmatization on specific columns\n",
    "lemmatization_columns = ['stage6', 'stage5']  # Specify the columns to lemmatize\n",
    "data_processor.execute_pipeline(lemmatization_columns=lemmatization_columns, method='nltk')\n",
    "\n",
    "# Loop through the results dictionary and display each dataframe\n",
    "for key, df in data_processor.results.items():\n",
    "    if isinstance(df, pd.DataFrame):  # Check if the result is a DataFrame\n",
    "        display(Markdown(f\"### {key}\"))\n",
    "        display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2794d27-b5a8-49e6-bb8a-e889b850a25d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -m \"Tokenization and data splitting stage completed\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ce244-f34f-421e-aab2-0f62ad002c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    " \n",
    "class OutputData:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def convert_stage_columns_to_strings(self):\n",
    "        \"\"\"\n",
    "        Convert list-type columns (like 'stage4') into a string of tokens\n",
    "        instead of keeping them as lists.\n",
    "        \"\"\"\n",
    "        stage_columns = [col for col in self.df.columns if col.startswith('stage')]\n",
    "        for col in stage_columns:\n",
    "            if isinstance(self.df[col].iloc[0], list):  # Check if the column contains lists\n",
    "                self.df[col] = self.df[col].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "\n",
    "    def display_data(self):\n",
    "        \"\"\"\n",
    "        Display the dataframe for visual inspection\n",
    "        \"\"\"\n",
    "        print(\"Tokenized Data:\")\n",
    "        print(self.df.head())  # Modify to display more rows if needed\n",
    "\n",
    "class DataSplitter:\n",
    "    def __init__(self, features, target, test_size=0.2, random_state=229):\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def perform_split_and_plot(self):\n",
    "        \"\"\"\n",
    "        Perform stratified and non-stratified splits, plot class distributions, \n",
    "        and return the split that is recommended based on the class balance.\n",
    "        \"\"\"\n",
    "        def _plot_class_distribution(y_data, title, ax):\n",
    "            \"\"\"\n",
    "            Helper function to plot the class distribution and display the value on top of each bar.\n",
    "\n",
    "            Parameters:\n",
    "            - y_data: The target data (labels).\n",
    "            - title: Title of the plot.\n",
    "            - ax: Matplotlib axis to plot on (for side-by-side plots).\n",
    "            \"\"\"\n",
    "            sns.countplot(\n",
    "                x=y_data, \n",
    "                hue=y_data,  # Use `hue` instead of `palette` to avoid the deprecation warning\n",
    "                palette=\"Set2\", \n",
    "                order=y_data.value_counts().index, \n",
    "                ax=ax,\n",
    "                legend=False  # Disable legend\n",
    "            )\n",
    "            ax.set_title(title)\n",
    "            ax.set_xlabel('Class')\n",
    "            ax.set_ylabel('Count')\n",
    "\n",
    "            # Add the count on top of each bar\n",
    "            for p in ax.patches:\n",
    "                height = p.get_height()\n",
    "                ax.annotate(\n",
    "                    f'{int(height)}',  # Display the integer count on top of each bar\n",
    "                    (p.get_x() + p.get_width() / 2., height),  # Position the annotation at the top of the bar\n",
    "                    ha='center',\n",
    "                    va='center',\n",
    "                    fontsize=10,\n",
    "                    color='black',\n",
    "                    xytext=(0, 5),  # Offset the text slightly above the bar\n",
    "                    textcoords='offset points'\n",
    "                )\n",
    "\n",
    "        # Convert any columns starting with 'stage' to Series format\n",
    "        stage_columns = [col for col in self.features.columns if col.startswith('stage')]\n",
    "        for col in stage_columns:\n",
    "            if isinstance(self.features[col].iloc[0], list):  # Check if the column contains lists\n",
    "                # Flatten list into a string representation, or leave as list if that's intended\n",
    "                self.features[col] = self.features[col].apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "            else:\n",
    "                self.features[col] = pd.Series(self.features[col])\n",
    "\n",
    "        # Entire dataset distribution\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "        _plot_class_distribution(self.target, 'Class Distribution in the Entire Dataset', ax)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Non-stratified split\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            self.features, self.target, \n",
    "            test_size=self.test_size, \n",
    "            random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        # No need to convert features to Series (they should stay as DataFrame or 2D numpy array)\n",
    "        y_train = pd.Series(y_train)\n",
    "        y_val = pd.Series(y_val)\n",
    "\n",
    "        # Side-by-side plots for non-stratified split\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))  # Two columns, one row\n",
    "        _plot_class_distribution(y_train, 'Training Set (No Stratification)', axes[0])\n",
    "        _plot_class_distribution(y_val, 'Validation Set (No Stratification)', axes[1])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Stratified split\n",
    "        X_train_strat, X_val_strat, y_train_strat, y_val_strat = train_test_split(\n",
    "            self.features, self.target, \n",
    "            test_size=self.test_size, \n",
    "            random_state=self.random_state, \n",
    "            stratify=self.target\n",
    "        )\n",
    "\n",
    "        # No need to convert features to Series (they should stay as DataFrame or 2D numpy array)\n",
    "        y_train_strat = pd.Series(y_train_strat)\n",
    "        y_val_strat = pd.Series(y_val_strat)\n",
    "\n",
    "        # Side-by-side plots for stratified split\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 4))  # Two columns, one row\n",
    "        _plot_class_distribution(y_train_strat, 'Training Set (Stratified)', axes[0])\n",
    "        _plot_class_distribution(y_val_strat, 'Validation Set (Stratified)', axes[1])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Criteria for decision:\n",
    "        imbalance_check = self.target.value_counts(normalize=True)\n",
    "        if imbalance_check.min() < 0.1:\n",
    "            print(\"\\nRecommendation: Used stratified splitting as the dataset is imbalanced.\")\n",
    "            return (X_train_strat, X_val_strat, y_train_strat, y_val_strat)\n",
    "        else:\n",
    "            print(\"\\nRecommendation: Non-stratified splitting may be sufficient as the dataset is relatively balanced.\")\n",
    "            return (X_train, X_val, y_train, y_val)\n",
    "   \n",
    "features = data_processor.combined_df\n",
    "target = data_processor.combined_df['target'] \n",
    "\n",
    "# Instantiate the DataSplitter class\n",
    "splitter = DataSplitter(features, target)\n",
    "\n",
    "# Perform splits and plot, returning the recommended split\n",
    "X_train_data, X_val_data, y_train_data, y_val_data = splitter.perform_split_and_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2a36d-558a-4612-92b3-127b7ff274d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0070d9-f4cf-4a48-907e-4f380f5bd0d9",
   "metadata": {},
   "source": [
    "The key fingerprint is:\n",
    "SHA256:WQRydCAw9QwmLLbROrbKLifaSSbufRFRTwzzvbbn39o albertohyna@gmail.com\n",
    "\n",
    "Agent pid 6970\n",
    "\n",
    "Identity added: /Users/AlbertLeo1/.ssh/id_ed25519 (albertohyna@gmail.com)\n",
    "\n",
    "AlbertLeo1@Alberts-MacBook-Pro HSEPY % cat ~/.ssh/id_ed25519.pub\n",
    "\n",
    "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJtInuOgnOVLeGdbkGO7ZiLMNK25OOaHhjKsE25CMRBx albertohyna@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b49394-d7eb-474d-96fe-5cb9713df49e",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Topic Modelling</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f43e6d-5360-46cb-81ae-5827af31f5ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "class TopicModeling:\n",
    "    def __init__(self, data, text_column='text', stages=None):\n",
    "        \"\"\"\n",
    "        Initialize the TopicModeling class with a DataFrame and optional stages.\n",
    "\n",
    "        Parameters:\n",
    "        - data: The DataFrame containing raw text data for various stages.\n",
    "        - text_column: The name of the column containing text data (default is 'text').\n",
    "        - stages: A list of stages to process; defaults to an empty list if None.\n",
    "        \"\"\"\n",
    "        self.data = data  # Input DataFrame\n",
    "        self.text_column = text_column  # Name of the text column (e.g., 'text')\n",
    "        self.stages = [f'stage{i}' for i in stages] if stages is not None else []  # Updated format\n",
    "        self.stage_data = {}  # Dictionary to hold tokenized data for each stage\n",
    "        self.coherence_scores = {}  # Dictionary to hold coherence scores for each stage\n",
    "        self.best_topics = {}  # Dictionary to hold best topics for each stage\n",
    "        self.best_stage = None\n",
    "        self.log_df = pd.DataFrame(columns=[\"Coherence Score\", \"Num Topics\", \"Topics\"])  # DataFrame to log results\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize a single text entry into words (tokens).\n",
    "        Assumes input is a string of text.\n",
    "        \"\"\"\n",
    "        return text.split()\n",
    "\n",
    "    def prepare_stage_data(self):\n",
    "        \"\"\"\n",
    "        Prepare tokenized data for each specified stage using the stage column.\n",
    "        \"\"\"\n",
    "        for stage in self.stages:\n",
    "            if stage not in self.data.columns:\n",
    "                print(f\"Column {stage} not found in data. Skipping stage {stage}.\")\n",
    "                continue\n",
    "            \n",
    "            # Extract text data from the specified text column\n",
    "            raw_text_data = self.data[self.text_column].dropna().tolist()\n",
    "\n",
    "            # Tokenize each document (text entry) in the series\n",
    "            tokenized_data = [self.tokenize_text(text) for text in raw_text_data]\n",
    "            \n",
    "            # Ensure all entries are lists (tokenized format)\n",
    "            if all(isinstance(entry, list) for entry in tokenized_data):\n",
    "                self.stage_data[stage] = tokenized_data\n",
    "            else:\n",
    "                raise ValueError(f\"Data in column {self.text_column} is not properly tokenized.\")\n",
    "\n",
    "    def prepare_corpus(self, tokenized_data):\n",
    "        \"\"\"\n",
    "        Prepare the corpus by vectorizing the tokenized data.\n",
    "\n",
    "        Parameters:\n",
    "        - tokenized_data: List of tokenized documents.\n",
    "\n",
    "        Returns:\n",
    "        - corpus: The vectorized document-term matrix.\n",
    "        - vectorizer: The CountVectorizer used for vectorization.\n",
    "        - gensim_dictionary: A Gensim dictionary mapping words to their IDs.\n",
    "        - gensim_corpus: A Gensim corpus of documents in bag-of-words format.\n",
    "        \"\"\"\n",
    "        vectorizer = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, lowercase=False, token_pattern=None)\n",
    "        corpus = vectorizer.fit_transform(tokenized_data)  # Vectorize using CountVectorizer\n",
    "        gensim_dictionary = corpora.Dictionary(tokenized_data)  # Create Gensim dictionary\n",
    "        gensim_corpus = [gensim_dictionary.doc2bow(text) for text in tokenized_data]  # Create bag-of-words corpus\n",
    "        return corpus, vectorizer, gensim_dictionary, gensim_corpus\n",
    "\n",
    "    def evaluate_coherence(self, gensim_corpus, gensim_dictionary, tokenized_data, stage, start=2, end=40):\n",
    "        \"\"\"\n",
    "        Evaluate coherence for a range of topic numbers using Gensim's LDA model.\n",
    "\n",
    "        Parameters:\n",
    "        - gensim_corpus: The Gensim corpus for the LDA model.\n",
    "        - gensim_dictionary: The Gensim dictionary for the corpus.\n",
    "        - tokenized_data: The original tokenized data.\n",
    "        - stage: The current stage being evaluated.\n",
    "        - start: The starting number of topics to evaluate (default is 2).\n",
    "        - end: The ending number of topics to evaluate (default is 20).\n",
    "\n",
    "        Returns:\n",
    "        - coherence_scores: A dictionary of coherence scores for each topic number.\n",
    "        - topics_by_num: A dictionary of extracted topics organized by number of topics.\n",
    "        \"\"\"\n",
    "        coherence_scores = {}\n",
    "        max_coherence_score = None\n",
    "        topics_by_num = {}\n",
    "    \n",
    "        for num_topics in range(start, end + 1):\n",
    "            # Create and train the LDA model\n",
    "            lda_model = LdaModel(corpus=gensim_corpus, id2word=gensim_dictionary, num_topics=num_topics, random_state=229)\n",
    "            coherence_model = CoherenceModel(model=lda_model, texts=tokenized_data, dictionary=gensim_dictionary, coherence='c_v')\n",
    "            coherence_score = coherence_model.get_coherence()  # Calculate coherence score\n",
    "            coherence_scores[num_topics] = coherence_score\n",
    "    \n",
    "            # Extract topics and clean the output using regex\n",
    "            topics = lda_model.print_topics(num_topics=num_topics, num_words=10)\n",
    "            cleaned_topics = [\n",
    "                re.sub(r'\\d+\\.\\d+\\*\"?(\\w+)\"?', r'\\1', topic[1]).replace(' + ', ' ')  # Remove probabilities and replace '+' with space\n",
    "                for topic in topics\n",
    "            ]\n",
    "            \n",
    "            # Add cleaned topics to the DataFrame\n",
    "            new_row = pd.DataFrame({\n",
    "                \"Coherence Score\": [coherence_score], \n",
    "                \"Num Topics\": [num_topics],\n",
    "                \"Topics\": [\" | \".join(cleaned_topics)]  # Join topics into a single string for readability\n",
    "            }, index=[f'{stage}_{num_topics}'])\n",
    "            \n",
    "            # Drop columns with all NaN values from new_row\n",
    "            new_row_cleaned = new_row.dropna(axis=1, how='any')  # Drop any column with NaN values\n",
    "            self.log_df = pd.concat([self.log_df, new_row_cleaned], axis=0)\n",
    "\n",
    "\n",
    "            topics_by_num[num_topics] = cleaned_topics  # Store topics by number of topics\n",
    "    \n",
    "        return coherence_scores, topics_by_num\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run the process for each specified stage: prepare data, evaluate coherence,\n",
    "        and store the best topics.\n",
    "        \"\"\"\n",
    "        self.prepare_stage_data()  # Prepare tokenized data for all stages\n",
    "        \n",
    "        best_coherence_score = -float('inf')  # Initialize with a very low value\n",
    "        best_stage = None  # Variable to store the best stage\n",
    "        \n",
    "        for stage, tokenized_data in self.stage_data.items():\n",
    "            # Check if the stage exists in the DataFrame columns before processing\n",
    "            if stage not in self.data.columns:\n",
    "                print(f\"Column {stage} not found in data. Skipping stage {stage}.\")\n",
    "                continue\n",
    "            \n",
    "            # Prepare Gensim corpus and dictionary from tokenized data\n",
    "            _, _, gensim_dictionary, gensim_corpus = self.prepare_corpus(tokenized_data)\n",
    "            \n",
    "            # Evaluate coherence scores and extract topics\n",
    "            stage_coherence_scores, topics_by_num = self.evaluate_coherence(gensim_corpus, gensim_dictionary, tokenized_data, stage)\n",
    "            \n",
    "            # Store coherence scores for the stage\n",
    "            self.coherence_scores[stage] = stage_coherence_scores\n",
    "            \n",
    "            # Find the best number of topics based on the coherence scores\n",
    "            best_num_topics = max(stage_coherence_scores, key=stage_coherence_scores.get)\n",
    "            \n",
    "            # Store only the best topics associated with the best number of topics\n",
    "            self.best_topics[stage] = topics_by_num[best_num_topics]\n",
    "            \n",
    "            # Check if the current stage has the best coherence score\n",
    "            if stage_coherence_scores[best_num_topics] > best_coherence_score:\n",
    "                best_coherence_score = stage_coherence_scores[best_num_topics]\n",
    "                best_stage = stage  # Update best stage\n",
    "        \n",
    "            print(f\"Best coherence score for {stage}: {stage_coherence_scores[best_num_topics]} with {best_num_topics} topics.\")\n",
    "        \n",
    "        # Store the stage with the best coherence score\n",
    "        self.best_stage = best_stage\n",
    "        self.best_coherence_score = best_coherence_score\n",
    "    \n",
    "        print(f\"Best stage overall: {self.best_stage} with a coherence score of {self.best_coherence_score}\")\n",
    "\n",
    "\n",
    "\n",
    "    def plot_coherence(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for stage, scores in self.coherence_scores.items():\n",
    "            # Plot the line\n",
    "            plt.plot(scores.keys(), scores.values(), label=f\"Stage: {stage}\")\n",
    "            # Mark each point with \"o\"\n",
    "            plt.scatter(scores.keys(), scores.values(), marker='o')\n",
    "        \n",
    "        plt.xlabel(\"Number of Topics\")\n",
    "        plt.ylabel(\"Coherence Score\")\n",
    "        plt.title(\"Coherence Scores by Number of Topics\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_wordcloud(self, stage, num_topics=None, num_words=10):\n",
    "        \"\"\"\n",
    "        Generate a word cloud for the best topics of the given stage.\n",
    "\n",
    "        Parameters:\n",
    "        - stage: The stage to generate the word cloud for.\n",
    "        - num_topics: The number of topics to consider for word cloud (default None, meaning all topics).\n",
    "        - num_words: The number of words to display per topic (default 10).\n",
    "        \"\"\"\n",
    "        topics = self.best_topics.get(stage)\n",
    "        if topics:\n",
    "            if num_topics is None:\n",
    "                words = ' '.join([word for topic in topics for word in topic.split()[:num_words]])  # Join words across all topics\n",
    "            else:\n",
    "                words = ' '.join(topics[num_topics][:num_words])  # Use the specified topic\n",
    "            \n",
    "            wordcloud = WordCloud(width=800, height=400, background_color='white').generate(words)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"No topics found for stage {stage}.\")\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# Example usage:\n",
    "topic_modeling = TopicModeling(data_processor.combined_df, stages=[2, 3, 4, 5])\n",
    "\n",
    "# Run the topic modeling process\n",
    "topic_modeling.run()\n",
    "\n",
    "# Display the log DataFrame with coherence scores and topics\n",
    "display(topic_modeling.log_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621b737-00ca-4d2a-9e6c-e8fbed9c3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stage_best_topics = topic_modeling.best_topics\n",
    "best_stage = topic_modeling.best_stage \n",
    "best_topic = all_stage_best_topics[best_stage]\n",
    "all_stage_best_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c361a41-0d60-4033-a33d-e58df76e5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b22939-a683-4a48-ac4c-1cef9db98b64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_modeling.plot_coherence()\n",
    "topic_modeling.plot_word_cloud(stage = 5, num_topics=18, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a8737-acce-42d6-9009-9bbf06217370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d2a38-1dba-48f4-9449-3bf7a77b2d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf HSEPY/\n",
    "#!git clone https://github.com/AlbertLeo1/HSEPY.git \n",
    "#data_processor.combined_df\n",
    "#!pip install numpy --upgrade  # Or specify a version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b68b6f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1143ecc-ebe3-4017-b6c0-00b68599e57a",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Topic Modelling\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00a2a3-fbd2-4eca-ab2b-0afc972d4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeling.plot_cloud('stage5', list(range(2, 21)))#specify the range of topics to explore from 2 to 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713be540-fe43-4590-8bc3-51e61ac3944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeling.plot_cloud('stage5', [2, 6, 8, 16, 20])#splot specific topics combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bae32c",
   "metadata": {},
   "source": [
    "#### <em>2. Using torch create and train neural network to predict target class (use train set to calculate loss and update weights. validation set to calculate f1 average score after every epoch and test set for final testing) Experiment with size of layers, number of layers, activation functions If you encounter OOM errors reduxe the number of layers/neurons.</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9516073",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891b78d-e8e7-4a51-889a-100e7bdc27bf",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Train NN\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab3262-8880-43f7-b9cc-5d08a0177f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "class Vectorizer:\n",
    "    def __init__(self, method='tfidf', ngram_range=(1, 2), max_features=4000, \n",
    "                 data_type='tokenized', embedding_file_paths=None):\n",
    "        \"\"\"\n",
    "        Initializes the Vectorizer with specified parameters.\n",
    "\n",
    "        Args:\n",
    "            method (str): The vectorization method to use ('tfidf' or 'count').\n",
    "            ngram_range (tuple): The range of n-grams to consider.\n",
    "            max_features (int): The maximum number of features to extract.\n",
    "            data_type (str): The type of data being processed ('tokenized' or 'embedding').\n",
    "            embedding_file_paths (list or None): List of file paths for embedding files.\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.ngram_range = ngram_range\n",
    "        self.max_features = max_features\n",
    "        self.vectorization = None\n",
    "        self.data_type = data_type\n",
    "        self.embedding_file_paths = embedding_file_paths if embedding_file_paths is not None else []\n",
    "\n",
    "    def fit_transform(self, X_train, X_val, y_train=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Fit the vectorizer on the training data and transform both training and validation data.\n",
    "\n",
    "        Args:\n",
    "            X_train (list): Training data.\n",
    "            X_val (list): Validation data.\n",
    "            y_train (list or None): Target labels for training data (optional).\n",
    "            y_val (list or None): Target labels for validation data (optional).\n",
    "\n",
    "        Returns:\n",
    "            tuple: Transformed training and validation data along with their labels if provided.\n",
    "        \"\"\"\n",
    "        # Choose vectorization method\n",
    "        if self.method == 'tfidf':\n",
    "            self.vectorization = TfidfVectorizer(ngram_range=self.ngram_range, max_features=self.max_features)\n",
    "        elif self.method == 'count':\n",
    "            self.vectorization = CountVectorizer(ngram_range=self.ngram_range, max_features=self.max_features)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported method. Use 'tfidf' or 'count'.\")\n",
    "\n",
    "        # Fit on the training data\n",
    "        X_train_vec = self.vectorization.fit_transform(X_train).toarray()\n",
    "        X_val_vec = self.vectorization.transform(X_val).toarray()\n",
    "\n",
    "        # Convert target labels to NumPy arrays if provided\n",
    "        y_train_vec = np.array(y_train) if y_train is not None else None\n",
    "        y_val_vec = np.array(y_val) if y_val is not None else None\n",
    "        \n",
    "        # Return transformed data based on data type\n",
    "        if self.data_type == 'tokenized':\n",
    "            return X_train_vec, X_val_vec, y_train_vec, y_val_vec\n",
    "        else:\n",
    "            print(\"Calling a wrong method for your data?\")\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the test data using the fitted vectorizer.\n",
    "\n",
    "        Args:\n",
    "            X (list): Test data to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            array: Transformed test data.\n",
    "        \"\"\"\n",
    "        if self.data_type == 'tokenized':\n",
    "            return self.vectorization.transform(X).toarray()\n",
    "        else:\n",
    "            print(\"Calling a wrong method for your data?\")\n",
    "\n",
    "    def load_embeddings(self):\n",
    "        \"\"\"\n",
    "        Loads embeddings from one or more .bin files.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Combined embedding matrix and word indices if data type is 'embedding'.\n",
    "        \"\"\"\n",
    "        embedding_matrices = []\n",
    "        word_indices = {}\n",
    "\n",
    "        # Check if embedding_file_paths is a single string or a list\n",
    "        if isinstance(self.embedding_file_paths, str):\n",
    "            self.embedding_file_paths = [self.embedding_file_paths]\n",
    "\n",
    "        for file_path in self.embedding_file_paths:\n",
    "            # Load word embeddings from the specified file\n",
    "            model = KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "            \n",
    "            # Get the vocabulary size and embedding dimension\n",
    "            vocab_size = len(model.key_to_index)\n",
    "            embedding_dimension = model.vector_size\n",
    "            embedding_matrix = np.zeros((vocab_size, embedding_dimension))\n",
    "\n",
    "            # Create a word index mapping\n",
    "            word_index = {word: i for i, word in enumerate(model.key_to_index)}\n",
    "            for word, i in word_index.items():\n",
    "                embedding_matrix[i] = model[word]\n",
    "\n",
    "            # Append the current embedding matrix to the list\n",
    "            embedding_matrices.append(embedding_matrix)\n",
    "\n",
    "            # Update the global word index if necessary\n",
    "            word_indices.update(word_index)\n",
    "\n",
    "        # Concatenate all embedding matrices into one\n",
    "        combined_embedding_matrix = np.vstack(embedding_matrices)\n",
    "\n",
    "        # Return combined embeddings and word indices based on data type\n",
    "        if self.data_type == 'embedding':\n",
    "            return combined_embedding_matrix, word_indices\n",
    "        else:\n",
    "            print(\"Calling a wrong method for your data?\")\n",
    "\n",
    "    def texts_to_indices(self, texts, word_index):\n",
    "        \"\"\"\n",
    "        Converts texts into lists of indices based on the provided word index.\n",
    "\n",
    "        Args:\n",
    "            texts (list): List of texts to convert.\n",
    "            word_index (dict): Dictionary mapping words to indices.\n",
    "\n",
    "        Returns:\n",
    "            list: List of lists containing indices for each text.\n",
    "        \"\"\"\n",
    "        indices_list = []\n",
    "        for text in texts:\n",
    "            # Convert each word in the text to its corresponding index\n",
    "            indices = [word_index[word] for word in text.split() if word in word_index]\n",
    "            indices_list.append(indices)\n",
    "\n",
    "        # Return indices based on data type\n",
    "        if self.data_type == 'embedding':\n",
    "            return indices_list\n",
    "        else:\n",
    "            print(\"Calling a wrong method for your data?\")\n",
    "\n",
    "    def get_embedding_data(self, embedding_matrix, texts, word_index):\n",
    "        \"\"\"\n",
    "        Returns embeddings for the given texts based on the provided embedding matrix.\n",
    "\n",
    "        Args:\n",
    "            embedding_matrix (array): Matrix containing word embeddings.\n",
    "            texts (list): List of texts to get embeddings for.\n",
    "            word_index (dict): Dictionary mapping words to indices.\n",
    "\n",
    "        Returns:\n",
    "            array: Array of embeddings for the input texts.\n",
    "        \"\"\"\n",
    "        # Convert texts to indices\n",
    "        indices_list = self.texts_to_indices(texts, word_index)\n",
    "        \n",
    "        # Initialize embeddings array\n",
    "        embeddings = np.zeros((len(texts), embedding_matrix.shape[1]))\n",
    "\n",
    "        for i, index_list in enumerate(indices_list):\n",
    "            if index_list:\n",
    "                # Calculate the mean embedding for the indices in the list\n",
    "                embeddings[i] = np.mean(embedding_matrix[index_list], axis=0)\n",
    "\n",
    "        # Return embeddings based on data type\n",
    "        if self.data_type == 'embedding':\n",
    "            return embeddings\n",
    "        else:\n",
    "            print(\"Calling a wrong method for your data?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d2a8e-9360-40a3-ae4d-91dbb70ed6c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    \"\"\"Custom dataset for handling text data.\"\"\"\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long) if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx], self.y[idx]) if self.y is not None else self.X[idx]\n",
    "\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    \"\"\"Multilayer Perceptron model for text classification.\n",
    "\n",
    "    Args:\n",
    "        stages (list): List of stages for processing.\n",
    "        vectorizer: Vectorizer instance for text data.\n",
    "        num_classes (int, optional): Number of output classes. Defaults to None.\n",
    "        hidden_size (int or list, optional): Size of hidden layers. Defaults to 128.\n",
    "        num_layers (int, optional): Number of hidden layers. Defaults to 2.\n",
    "        activation_fn (str, optional): Activation function name. Defaults to 'relu'.\n",
    "        learning_rate (float, optional): Learning rate for optimizer. Defaults to 0.00005.\n",
    "        batch_size (int, optional): Batch size for training. Defaults to 32.\n",
    "        patience (int, optional): Early stopping patience. Defaults to 10.\n",
    "        num_epochs (int, optional): Number of epochs for training. Defaults to 50.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, stages, vectorizer, num_classes=None, hidden_size=[128, 64], num_layers=2, \n",
    "                 activation_fn='relu', learning_rate=0.00007, batch_size=64, \n",
    "                 patience=10, num_epochs=50):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.stages = stages  # Store the stages for processing\n",
    "        self.vectorizer = vectorizer  # Assign the vectorizer for data transformation\n",
    "        self.embedding_file_paths = vectorizer.embedding_file_paths  # Retrieve embedding file paths from vectorizer\n",
    "        self.hidden_size = hidden_size if isinstance(hidden_size, list) else [hidden_size]  # Set hidden size as a list\n",
    "        self.layer = []  # Initialize list to store layers\n",
    "        self.best_f1_score = 0  # Initialize the best F1 score\n",
    "        \n",
    "        # Initialize a DataFrame to collect all epoch data across stages\n",
    "        self.all_epoch_data = pd.DataFrame(columns=[\"Stage\", \"Epoch\", \"Loss\", \"F1_macro\", \"Prediction\"])\n",
    "    \n",
    "        for stage in self.stages: \n",
    "            print(f\"Processing Stage: {stage}\")  # Print the current stage being processed\n",
    "            tokenized_data = pipeline.get_tokenized_data(f'stage_{stage}')  # Retrieve tokenized data for the current stage\n",
    "            self.X_train, self.X_val, self.y_train, self.y_val, self.test_data = tokenized_data  # Unpack tokenized data\n",
    "            self.num_classes = num_classes  # Set number of classes\n",
    "            \n",
    "            # If num_classes is not provided, infer from training data\n",
    "            if num_classes is None:\n",
    "                self.num_classes = len(set(self.y_train))  # Calculate the number of unique classes from training labels\n",
    "                \n",
    "            self.batch_size = batch_size  # Set batch size for training\n",
    "            \n",
    "            # Calculate input size based on data type\n",
    "            if self.vectorizer.data_type == 'embedding':\n",
    "                self.input_size = self.prepare_embedding_data()  # Prepare input size for embedding data\n",
    "                    \n",
    "            elif self.vectorizer.data_type == 'tokenized':\n",
    "                self.input_size = self.prepare_tokenized_data()  # Prepare input size for tokenized data\n",
    "                \n",
    "            else:\n",
    "                print(\"Error: Unsupported data type. Please use 'embedding' or 'tokenized'.\")  # Error message for unsupported data types\n",
    "                raise ValueError(\"Unsupported data type: {}\".format(self.vectorizer.data_type))\n",
    "    \n",
    "            self.class_weights = self.calculate_class_weights(self.y_train)  # Calculate class weights for handling class imbalance\n",
    "            \n",
    "            # Define the activation function based on user input\n",
    "            self.activation_function = self.get_activation_function(activation_fn)\n",
    "            \n",
    "            # Define the MLP structure with dynamic input size and output layer based on num_classes\n",
    "            self.network = self.build_network()\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set device for computation\n",
    "            self.network.to(self.device)  # Move the model to the specified device\n",
    "            \n",
    "            # Set the appropriate loss function based on number of classes\n",
    "            if self.num_classes == 1:\n",
    "                self.criterion = nn.BCEWithLogitsLoss()  # Use binary cross-entropy loss for binary classification\n",
    "            else:\n",
    "                self.criterion = nn.CrossEntropyLoss(weight=self.class_weights)  # Use cross-entropy loss for multi-class classification\n",
    "            \n",
    "            # Layers and training components\n",
    "            self.layers = list(self.network.children())  # Extract the layers from the network\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)  # Set up the Adam optimizer\n",
    "            self.scheduler = lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=2)  # Learning rate scheduler\n",
    "            \n",
    "            # Set up training parameters\n",
    "            self.patience = patience  # Set the patience for early stopping\n",
    "            self.num_epochs = num_epochs  # Set total epochs for training\n",
    "            self.epoch_data = pd.DataFrame(columns=[\"Epoch\", \"Loss\", \"F1_macro\", \"Prediction\"])  # DataFrame to collect epoch data\n",
    "            self.all_epoch_data = pd.concat([self.all_epoch_data, self.epoch_data], ignore_index=True)  # Concatenate epoch data across stages\n",
    "\n",
    "    \n",
    "    def prepare_tokenized_data(self):\n",
    "        \"\"\"Prepare data for both binary and multiclass classification using the vectorizer.\n",
    "    \n",
    "        This method transforms the training and validation datasets into tokenized formats \n",
    "        suitable for classification tasks. It also prepares data loaders for training, \n",
    "        validation, and optionally testing.\n",
    "    \n",
    "        Returns:\n",
    "            int: The input size for building the network based on the vectorized dimension.\n",
    "                  Returns None if an error occurs during preparation.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If vectorization results in None or if the data does not conform\n",
    "                        to expected dimensions.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize y_train_vec and y_val_vec to None\n",
    "            y_train_vec, y_val_vec = None, None\n",
    "    \n",
    "            # Assuming the vectorizer is handling tokenized data\n",
    "            # Transform the data using the vectorizer\n",
    "            X_train_vec, X_val_vec, y_train_vec, y_val_vec = self.vectorizer.fit_transform(self.X_train, self.X_val, self.y_train, self.y_val)\n",
    "    \n",
    "            # Check that vectorization results are valid\n",
    "            if X_train_vec is None or X_val_vec is None:\n",
    "                raise ValueError(\"Vectorization resulted in None for training or validation data.\")\n",
    "    \n",
    "            # Ensure the resulting arrays are 2D\n",
    "            if len(X_train_vec.shape) != 2 or len(X_val_vec.shape) != 2:\n",
    "                raise ValueError(\"Vectorized data must be 2D arrays.\")\n",
    "    \n",
    "            # Handle labels\n",
    "            y_train_vec = np.ravel(y_train_vec) if y_train_vec is not None else np.ravel(self.y_train)\n",
    "            y_val_vec = np.ravel(y_val_vec) if y_val_vec is not None else np.ravel(self.y_val)\n",
    "    \n",
    "            # Check unique classes for binary vs multiclass classification\n",
    "            unique_classes = np.unique(self.y_train)\n",
    "            if len(unique_classes) > 2:\n",
    "                # Multiclass: one-hot encode labels\n",
    "                lb = LabelBinarizer()\n",
    "                y_train_vec = lb.fit_transform(y_train_vec)\n",
    "                y_val_vec = lb.transform(y_val_vec)\n",
    "    \n",
    "            # Automatically set input size\n",
    "            input_size = X_train_vec.shape[1]\n",
    "            assert X_train_vec.shape[0] == len(y_train_vec), \"Mismatch in X_train and y_train lengths\"\n",
    "            assert X_val_vec.shape[0] == len(y_val_vec), \"Mismatch in X_val and y_val lengths\"\n",
    "    \n",
    "            # Prepare data loaders for training and validation\n",
    "            self.train_loader = DataLoader(CustomTextDataset(X_train_vec, y_train_vec), batch_size=self.batch_size, shuffle=True)\n",
    "            self.val_loader = DataLoader(CustomTextDataset(X_val_vec, y_val_vec), batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "            # Prepare test data if applicable\n",
    "            if hasattr(self, 'test_data') and self.test_data is not None:\n",
    "                self.test_data_vec = self.vectorizer.transform(self.test_data)\n",
    "                self.test_loader = DataLoader(CustomTextDataset(self.test_data_vec), batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "            return input_size  # Return the input size for building the network\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred in prepare_tokenized_data: {str(e)}\")\n",
    "            return None  # Ensure a return value even on failure\n",
    "\n",
    "\n",
    "    def prepare_embedding_data(self):\n",
    "        \"\"\"Prepare data for classification using only embeddings.\n",
    "    \n",
    "        This method loads embeddings from a specified file, processes training and validation\n",
    "        datasets to obtain embedding vectors, and prepares data loaders for training,\n",
    "        validation, and optionally testing.\n",
    "    \n",
    "        Returns:\n",
    "            int: The input size for building the network based on the embedding dimension.\n",
    "                  Returns None if an error occurs during preparation.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If the embedding file path is not specified or if the data does not\n",
    "                        conform to expected dimensions.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load embeddings from the specified file\n",
    "            if self.embedding_file_paths is not None:\n",
    "                embedding_matrix, word_index = self.vectorizer.load_embeddings()\n",
    "            else:\n",
    "                raise ValueError(\"Embedding file path must be specified for embedding data type.\")\n",
    "    \n",
    "            # Use the vectorizer to get the embeddings for X_train and X_val\n",
    "            X_train_vec = self.vectorizer.get_embedding_data(embedding_matrix, self.X_train, word_index)\n",
    "            X_val_vec = self.vectorizer.get_embedding_data(embedding_matrix, self.X_val, word_index)\n",
    "    \n",
    "            # Ensure embeddings are in 2D array format\n",
    "            if len(X_train_vec.shape) != 2 or len(X_val_vec.shape) != 2:\n",
    "                raise ValueError(\"Embedding data must be 2D arrays.\")\n",
    "    \n",
    "            # Handle labels (assumed to be present as y_train, y_val)\n",
    "            y_train_vec = np.ravel(self.y_train) if self.y_train is not None else None\n",
    "            y_val_vec = np.ravel(self.y_val) if self.y_val is not None else None\n",
    "    \n",
    "            # Check unique classes for binary vs multiclass classification\n",
    "            unique_classes = np.unique(y_train_vec)\n",
    "            if len(unique_classes) > 2:\n",
    "                # Multiclass: one-hot encode labels\n",
    "                lb = LabelBinarizer()\n",
    "                y_train_vec = lb.fit_transform(y_train_vec)\n",
    "                y_val_vec = lb.transform(y_val_vec)\n",
    "    \n",
    "            # Automatically set input size\n",
    "            input_size = X_train_vec.shape[1]  \n",
    "            assert X_train_vec.shape[0] == len(y_train_vec), \"Mismatch in X_train and y_train lengths\"\n",
    "            assert X_val_vec.shape[0] == len(y_val_vec), \"Mismatch in X_val and y_val lengths\"\n",
    "    \n",
    "            # Prepare data loaders for training and validation\n",
    "            self.train_loader = DataLoader(CustomTextDataset(X_train_vec, y_train_vec), batch_size=self.batch_size, shuffle=True)\n",
    "            self.val_loader = DataLoader(CustomTextDataset(X_val_vec, y_val_vec), batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "            # Prepare test data if applicable\n",
    "            if hasattr(self, 'test_data') and self.test_data is not None:\n",
    "                self.test_data_vec = self.vectorizer.get_embedding_data(embedding_matrix, self.test_data, word_index)\n",
    "                self.test_loader = DataLoader(CustomTextDataset(self.test_data_vec), batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "            return input_size  # Return the input size for building the network\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred in prepare_data: {str(e)}\")\n",
    "            return None  # Ensure a return value even on failure\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Define the forward pass.\"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "        \n",
    "    def update_layer(self, index, new_layer, new_learning_rate=None, new_batch_size=None, new_num_epochs=None):\n",
    "        \"\"\"Update a layer in the network at the specified index and modify learning parameters.\n",
    "    \n",
    "        Args:\n",
    "            index (int): The index of the layer to update.\n",
    "            new_layer (nn.Module): The new layer to replace the existing layer at the specified index.\n",
    "            new_learning_rate (float, optional): New learning rate for the optimizer. If not provided, the current learning rate is retained.\n",
    "            new_batch_size (int, optional): New batch size for training. If not provided, the current batch size is retained.\n",
    "            new_num_epochs (int, optional): New number of epochs for training. If not provided, the current number of epochs is retained.\n",
    "    \n",
    "        Raises:\n",
    "            IndexError: If the specified index is out of range for the current layers.\n",
    "        \"\"\"\n",
    "        layers = list(self.network.children())  # Get the current layers\n",
    "        # Check if the index is valid\n",
    "        if index < -len(layers) or index >= len(layers):\n",
    "            raise IndexError(\"Layer index out of range.\")  \n",
    "        \n",
    "        layers[index] = new_layer  # Replace the specified layer\n",
    "        self.network = nn.Sequential(*layers)  # Rebuild the network with the modified layers\n",
    "        \n",
    "        # Update learning parameters if provided\n",
    "        if new_learning_rate is not None:\n",
    "            self.learning_rate = new_learning_rate  # Update learning rate\n",
    "        if new_batch_size is not None:\n",
    "            self.batch_size = new_batch_size  # Update batch size\n",
    "        if new_num_epochs is not None:\n",
    "            self.num_epochs = new_num_epochs  # Update number of epochs\n",
    "    \n",
    "        # Update the optimizer with the new learning rate\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "        # Print confirmation of the updates\n",
    "        print(f\"Layer at index {index} updated. Learning rate: {self.learning_rate}, Batch size: {self.batch_size}, Epochs: {self.num_epochs}\")\n",
    "\n",
    "    \n",
    "    def calculate_class_weights(self, y_train):\n",
    "        \"\"\"Calculate class weights based on the training targets.\"\"\"\n",
    "        classes = np.unique(y_train)\n",
    "        class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "        return torch.FloatTensor(class_weights)  # Ensure class weights are on the correct device\n",
    "\n",
    "    def get_activation_function(self, activation_fn):\n",
    "        \"\"\"Return the activation function based on the name provided.\"\"\"\n",
    "        if activation_fn == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif activation_fn == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        elif activation_fn == 'tanh':\n",
    "            return nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "            \n",
    "    def build_network(self):\n",
    "        \"\"\"Build the neural network architecture based on the specified hidden layers and output configuration.\n",
    "    \n",
    "        This method constructs the network by sequentially adding layers. \n",
    "        It creates hidden layers as defined in self.hidden_size and adds an output layer \n",
    "        based on the number of classes specified for the task (binary or multi-class classification).\n",
    "    \n",
    "        Returns:\n",
    "            nn.Sequential: A PyTorch sequential model containing the configured layers.\n",
    "        \"\"\"\n",
    "        layers = []  # List to hold all the layers of the network\n",
    "        input_size = self.input_size  # Start with the input size defined for the model\n",
    "    \n",
    "        # Build hidden layers based on specified hidden sizes\n",
    "        for h in self.hidden_size:\n",
    "            layers.append(nn.Linear(input_size, h))  # Add a linear layer\n",
    "            layers.append(self.activation_function)  # Add the activation function\n",
    "            input_size = h  # Update input size for the next layer\n",
    "    \n",
    "        # Build the output layer\n",
    "        if self.num_classes == 1:  # For binary classification\n",
    "            layers.append(nn.Linear(input_size, 1))  # Single output neuron\n",
    "            layers.append(nn.Sigmoid())  # Sigmoid activation function for binary output\n",
    "        else:  # For multi-class classification\n",
    "            layers.append(nn.Linear(input_size, self.num_classes))  # Linear layer for multi-class outputs\n",
    "            # Softmax is typically used for multi-class output, but it can also be incorporated in the loss function.\n",
    "    \n",
    "        return nn.Sequential(*layers)  # Return the sequential model containing all layers\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the model with Out Of Memory (OOM) error handling.\n",
    "    \n",
    "        This method iteratively trains the neural network for a specified number of epochs, \n",
    "        monitors training and validation losses, and evaluates the F1 score. It also implements \n",
    "        early stopping based on validation F1 score and handles potential OOM errors by reducing \n",
    "        model complexity.\n",
    "    \n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the epoch data including loss, F1 score, and predictions.\n",
    "        \"\"\"\n",
    "        best_f1 = 0.0  # Initialize best F1 score\n",
    "        epochs_without_improvement = 0  # Counter for early stopping\n",
    "        train_losses = []  # List to store training losses\n",
    "        val_losses = []  # List to store validation losses\n",
    "        \n",
    "        # Loop over the specified number of epochs\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{self.num_epochs}\")\n",
    "            self.network.train()  # Set the model to training mode\n",
    "            all_train_preds = []  # List to store predictions from training\n",
    "            running_loss = 0.0  # Variable to accumulate training loss\n",
    "    \n",
    "            try:\n",
    "                # Iterate over batches from the training loader\n",
    "                for texts, labels in self.train_loader:\n",
    "                    self.optimizer.zero_grad()  # Clear previous gradients\n",
    "                    outputs = self.network(texts)  # Forward pass through the network\n",
    "    \n",
    "                    # Handle loss calculation based on the number of classes\n",
    "                    if self.num_classes == 1:\n",
    "                        labels = labels.view(-1, 1)  # Reshape labels for binary classification\n",
    "                        loss = self.criterion(outputs, labels)  # Compute binary loss\n",
    "                    else:\n",
    "                        loss = self.criterion(outputs, labels)  # Compute multi-class loss\n",
    "                    \n",
    "                    loss.backward()  # Backward pass to compute gradients\n",
    "                    self.optimizer.step()  # Update model parameters\n",
    "    \n",
    "                    running_loss += loss.item()  # Accumulate loss for the current batch\n",
    "    \n",
    "                    # Generate predictions based on model output\n",
    "                    if self.num_classes == 1:\n",
    "                        predicted = (torch.sigmoid(outputs) > 0.5).float()  # Convert outputs to binary predictions\n",
    "                    else:\n",
    "                        _, predicted = torch.max(outputs.data, 1)  # Get predicted classes for multi-class\n",
    "    \n",
    "                    all_train_preds.extend(predicted.cpu().numpy())  # Store predictions\n",
    "    \n",
    "                # Calculate average training loss for the epoch\n",
    "                train_loss = running_loss / len(self.train_loader)\n",
    "                train_losses.append(train_loss)  # Append to training losses\n",
    "    \n",
    "                # Validate the model and retrieve validation outputs and loss\n",
    "                val_outputs, val_labels, average_loss = self.validate()\n",
    "                val_losses.append(average_loss)  # Append validation loss\n",
    "    \n",
    "                # Evaluate F1 score on the validation set\n",
    "                f1 = self.evaluate(val_labels, val_outputs)\n",
    "    \n",
    "                # Create a DataFrame for the current epoch's data\n",
    "                new_epoch_data = pd.DataFrame({\n",
    "                    \"Epoch\": [epoch + 1], \n",
    "                    \"Loss\": train_loss,\n",
    "                    \"F1_macro\": [f1],\n",
    "                    \"Prediction\": [val_outputs],\n",
    "                })\n",
    "    \n",
    "                self.epoch_data = pd.concat([self.epoch_data, new_epoch_data], ignore_index=True)\n",
    "    \n",
    "                # Print the results for the current epoch\n",
    "                print(f'Epoch [{epoch + 1}], F1 Score: {f1:.4f}, Validation Loss: {average_loss:.4f}')\n",
    "    \n",
    "                # Plot the confusion matrix after each epoch\n",
    "                self.plot_confusion_matrix(val_labels, val_outputs)\n",
    "    \n",
    "                # Call the learning rate scheduler to adjust learning rate\n",
    "                self.scheduler.step(average_loss)\n",
    "                print(f'Current learning rate: {self.scheduler.optimizer.param_groups[0][\"lr\"]}')\n",
    "    \n",
    "                # Early stopping logic based on F1 score improvement\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1  # Update best F1 score\n",
    "                    epochs_without_improvement = 0  # Reset counter\n",
    "                else:\n",
    "                    epochs_without_improvement += 1  # Increment counter for no improvement\n",
    "                    if epochs_without_improvement >= self.patience: \n",
    "                        display(Markdown(f\"<pre style='color: Red; font-size: 20'>Early stopping triggered after {epoch + 1} epochs.</pre>\"))\n",
    "                        break  # Stop training if patience is exceeded\n",
    "    \n",
    "            except RuntimeError as e:\n",
    "                # Handle out of memory (OOM) error by reducing model complexity\n",
    "                if 'out of memory' in str(e):\n",
    "                    print(\"Encountered OOM error. Reducing the number of layers or neurons.\")\n",
    "                    self.hidden_size = [max(32, layer // 2) for layer in self.hidden_size]  # Reduce the size of hidden layers\n",
    "                    self.network = self.build_network()  # Rebuild the model with new size\n",
    "                    self.network.to(self.device)  # Ensure the model is on the correct device\n",
    "                    self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.learning_rate)  # Reinitialize optimizer\n",
    "                    print(\"Retrying training with a smaller model.\")\n",
    "                    break  # Exit the loop and retry training\n",
    "    \n",
    "        # After training, plot the losses\n",
    "        self.plot_loss(train_losses, val_losses)\n",
    "    \n",
    "        return self.epoch_data  # Return accumulated data for all epochs\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self, true_labels, predicted_labels):\n",
    "        \"\"\"Evaluate the model using F1 score.\n",
    "    \n",
    "        This method calculates the F1 score based on the true labels and predicted labels. \n",
    "        It also updates the best F1 score if the current score exceeds the previously recorded best.\n",
    "    \n",
    "        Args:\n",
    "            true_labels (list or numpy array): The true labels for the data being evaluated.\n",
    "            predicted_labels (list or numpy array): The predicted labels from the model.\n",
    "    \n",
    "        Returns:\n",
    "            float: The calculated F1 score.\n",
    "        \"\"\"\n",
    "        # If using single output for binary classification, convert predicted_labels to binary\n",
    "        if self.num_classes == 1:\n",
    "            predicted_labels = (predicted_labels > 0.5).astype(int)  # Convert probabilities to binary labels\n",
    "    \n",
    "        # Calculate the F1 score with macro averaging\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "        \n",
    "        # Update best F1 score if the current F1 score is higher\n",
    "        if f1 > self.best_f1_score:\n",
    "            self.best_f1_score = f1  # Store the best F1 score\n",
    "    \n",
    "        return f1  # Return the calculated F1 score\n",
    "\n",
    "\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"Validate the model and return predictions and true labels.\n",
    "    \n",
    "        This method evaluates the model's performance on the validation set and computes \n",
    "        the average loss. It returns the predictions made by the model and the true labels \n",
    "        for comparison.\n",
    "    \n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - list: Predicted labels for the validation set.\n",
    "                - list: True labels for the validation set.\n",
    "                - float: Average loss over the validation set.\n",
    "        \"\"\"\n",
    "        self.network.eval()  # Set the model to evaluation mode\n",
    "        val_outputs = []  # Initialize a list to store validation predictions\n",
    "        val_labels = []   # Initialize a list to store true labels\n",
    "        total_loss = 0.0  # Initialize total loss accumulator\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient calculations for efficiency\n",
    "            for val_texts, val_labels_batch in self.val_loader:\n",
    "                outputs = self.network(val_texts)  # Perform forward pass on validation data\n",
    "                loss = self.criterion(outputs, val_labels_batch)  # Calculate the loss\n",
    "                total_loss += loss.item()  # Accumulate total loss\n",
    "                \n",
    "                # Process outputs based on the number of classes\n",
    "                if self.num_classes == 1:\n",
    "                    # Convert outputs to binary predictions for binary classification\n",
    "                    predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                else:\n",
    "                    # For multi-class classification, get the index of the max output\n",
    "                    predicted = torch.argmax(outputs, axis=1)\n",
    "        \n",
    "                # Store predictions and true labels\n",
    "                val_outputs.extend(predicted.cpu().numpy())\n",
    "                val_labels.extend(val_labels_batch.cpu().numpy())\n",
    "        \n",
    "        # Calculate the average loss over all validation batches\n",
    "        average_loss = total_loss / len(self.val_loader)\n",
    "        \n",
    "        return val_outputs, val_labels, average_loss  # Return predictions, true labels, and average loss\n",
    "\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Test the model on the test set and display the final F1 score if labels are available.\n",
    "    \n",
    "        This method evaluates the model's performance on unseen data and computes the F1 score.\n",
    "        If true labels for the test set are not available, it will notify the user.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of predicted labels for the test set.\n",
    "        \"\"\"\n",
    "        self.network.eval()  # Set the model to evaluation mode\n",
    "        test_preds = []  # Initialize a list to store test predictions\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient calculations for efficiency\n",
    "            for texts in self.test_loader:\n",
    "                outputs = self.network(texts)  # Perform forward pass on test data\n",
    "                \n",
    "                # Process outputs based on the number of classes\n",
    "                if self.num_classes == 1:\n",
    "                    # Convert outputs to binary predictions for binary classification\n",
    "                    predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                    test_preds.extend(predicted.cpu().numpy())  # Store predictions\n",
    "                else:\n",
    "                    # For multi-class classification, get the index of the max output\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    test_preds.extend(predicted.cpu().numpy())  # Store predictions\n",
    "        \n",
    "        # Check if test labels are available\n",
    "        if hasattr(self, 'y_test'):\n",
    "            test_labels = self.y_test.tolist()  # Use actual test labels if available\n",
    "            \n",
    "            # Calculate the F1 score using the true labels and predictions\n",
    "            final_f1 = f1_score(test_labels, test_preds, average='macro')\n",
    "            # Display the final F1 score in Markdown format\n",
    "            display(Markdown(f\"<pre style='color: Chartreuse; font-size: 20'>Final F1 Score: {final_f1:.4f}</pre>\"))\n",
    "        else:\n",
    "            # If no true labels are available, notify the user and skip F1 score calculation\n",
    "            display(Markdown(f\"<pre style='color: Chartreuse; font-size: 20'>Final F1 Score: No true label</pre>\"))\n",
    "        \n",
    "        return test_preds  # Return the list of predicted labels\n",
    "\n",
    "\n",
    "\n",
    "    def plot_loss(self, train_losses, val_losses):\n",
    "        \"\"\"Plot the training and validation loss.\"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.title('Loss Over Epochs')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_confusion_matrix(self, true_labels, predicted_labels):\n",
    "        \"\"\"Plot the confusion matrix.\"\"\"\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "        plt.figure(figsize=(1.5, 0.6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "\n",
    "    def wrong_prediction_at_best_epoch(self, X_val):\n",
    "        \"\"\"Get all wrong predictions from the best F1 score epoch and drop duplicates based on val_text.\"\"\"\n",
    "        # Find the index of the epoch with the best F1 score\n",
    "        best_epoch_index = self.epoch_data['F1_macro'].idxmax()\n",
    "        best_epoch_predictions = self.epoch_data.loc[best_epoch_index, 'Prediction']\n",
    "        \n",
    "        # Create a DataFrame to hold wrong predictions\n",
    "        best_epoch_wrong_df = pd.DataFrame({\n",
    "            \"prediction\": best_epoch_predictions,\n",
    "            \"actual\": self.y_val.tolist(),  # Use validation true labels\n",
    "            \"val_text\": X_val.tolist()       # Use validation texts\n",
    "        })\n",
    "        \n",
    "        # Check for length mismatch\n",
    "        if len(best_epoch_predictions) != len(self.y_val) or len(best_epoch_predictions) != len(X_val):\n",
    "            print(f\"Warning: Length mismatch at best epoch {best_epoch_index + 1}. \"\n",
    "                  f\"Predictions: {len(best_epoch_predictions)}, Actual: {len(self.y_val)}, Text: {len(X_val)}.\")\n",
    "        \n",
    "        # Filter for wrong predictions\n",
    "        wrong_predictions_df = best_epoch_wrong_df[best_epoch_wrong_df['prediction'] != best_epoch_wrong_df['actual']]\n",
    "        \n",
    "        # Drop duplicates based on the 'val_text' column\n",
    "        wrong_predictions_df = wrong_predictions_df.drop_duplicates(subset='val_text')\n",
    "        \n",
    "        return wrong_predictions_df\n",
    "\n",
    "\n",
    "vectorizer=Vectorizer(ngram_range=(1, 2), max_features=4000)\n",
    "classifier = MLPClassifier(stages=[3], vectorizer=vectorizer, hidden_size=[128, 64], learning_rate=0.00008)\n",
    "classifier.train()\n",
    "display(Markdown(f\"<pre style='color: Chartreuse; font-size: 20'>Best F1 Score: {classifier.best_f1_score:.4f}</pre>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c065ee-0890-40c3-8b9b-08ebe4aede9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_predictions = pd.DataFrame({'prediction': classifier.test(), 'true_label': None})\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec72e9-7138-4361-8f74-4969dffbcc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(test_predictions['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294058b-e445-44be-b7ab-4aa2591264e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_val_raw, _, _, _ = pipeline.get_tokenized_data('stage_0')\n",
    "# Get predictions from the classifier\n",
    "wrong_predictions = classifier.wrong_prediction_at_best_epoch(X_val=X_val_raw)\n",
    "wrong_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d58abb-5467-4324-9d8d-6e30f506bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_preds_class = wrong_predictions.groupby('prediction').size()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 3))\n",
    "wrong_preds_class.plot(kind='bar')\n",
    "plt.title('Count of Wrong Predictions per Class')\n",
    "plt.xlabel('Prediction Class (0 or 1)')\n",
    "plt.ylabel('Count of Wrong Predictions')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2028c",
   "metadata": {},
   "source": [
    "#### 3. Create embeddings using this dataset: At the end of every epoch, print dot product similarity between embeddings for different pairs of words to see if they change in direction we would like (for opposite words it should gradually become negative, for similar - positive, for not related words - close to 0):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c2801-920d-4485-a2e2-84933a2a949b",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Word Embedding\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5a0cb-835f-4d30-a4f4-3bc24dfbeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "\n",
    "class WordRelationships:\n",
    "    def __init__(self, vocab, nlp):\n",
    "        \"\"\"Initialize WordRelationships with a vocabulary and a SpaCy NLP model.\n",
    "        \n",
    "        Args:\n",
    "            vocab (list): List of words in the vocabulary.\n",
    "            nlp: SpaCy NLP model instance for processing text.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.nlp = nlp  # Store the nlp instance\n",
    "\n",
    "        # Precompute SpaCy tokens for efficiency\n",
    "        self.vocab_tokens = {word: self.nlp(word) for word in self.vocab}\n",
    "        \n",
    "        # Build relationships if vocabulary is not empty\n",
    "        self.antonyms, self.synonyms, self.unrelated = (self.build_word_relationships() \n",
    "                                                        if self.vocab else ({}, {}, {}))\n",
    "\n",
    "    def build_word_relationships(self):\n",
    "        \"\"\"Build dictionaries of antonyms, synonyms, and unrelated words.\"\"\"\n",
    "        antonym_dict = {}\n",
    "        synonym_dict = {}\n",
    "        unrelated_dict = {}\n",
    "\n",
    "        for word in self.vocab:\n",
    "            antonyms, synonyms = self.get_wordnet_relationships(word)\n",
    "            if antonyms:\n",
    "                antonym_dict[word] = antonyms\n",
    "            if synonyms:\n",
    "                synonym_dict[word] = synonyms\n",
    "            \n",
    "            unrelated = self.get_unrelated_words(word)\n",
    "            if unrelated:\n",
    "                unrelated_dict[word] = unrelated\n",
    "\n",
    "        return antonym_dict, synonym_dict, unrelated_dict\n",
    "\n",
    "    def get_wordnet_relationships(self, word):\n",
    "        \"\"\"Get synonyms and antonyms from WordNet for a given word.\n",
    "\n",
    "        Args:\n",
    "            word (str): The word to find relationships for.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing lists of antonyms and synonyms.\n",
    "        \"\"\"\n",
    "        antonyms, synonyms = set(), set()\n",
    "        for syn in wn.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.antonyms():\n",
    "                    antonyms.add(lemma.antonyms()[0].name())\n",
    "                synonyms.add(lemma.name())\n",
    "        return list(antonyms), list(synonyms)  # Return as lists\n",
    "\n",
    "    def get_unrelated_words(self, word, topn=10, threshold=0.1):\n",
    "        \"\"\"Get words that are unrelated based on low similarity.\n",
    "\n",
    "        Args:\n",
    "            word (str): The word to find unrelated words for.\n",
    "            topn (int): Maximum number of unrelated words to return.\n",
    "            threshold (float): Similarity threshold to determine unrelatedness.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of unrelated words.\n",
    "        \"\"\"\n",
    "        word_token = self.vocab_tokens[word]\n",
    "        unrelated_words = []\n",
    "\n",
    "        # Check similarity for all words and filter based on threshold\n",
    "        for vocab_word, vocab_token in self.vocab_tokens.items():\n",
    "            similarity = word_token.similarity(vocab_token)\n",
    "            if 0 < similarity < threshold:\n",
    "                unrelated_words.append(vocab_word)\n",
    "                if len(unrelated_words) == topn:\n",
    "                    break\n",
    "        return unrelated_words\n",
    "\n",
    "    def get_opposite_words(self, word):\n",
    "        \"\"\"Return a list of antonym words for the given word.\"\"\"\n",
    "        return self.antonyms.get(word, [])\n",
    "\n",
    "    def get_synonyms(self, word):\n",
    "        \"\"\"Return a list of synonym words for the given word.\"\"\"\n",
    "        return self.synonyms.get(word, [])\n",
    "\n",
    "    def get_relationship(self, word1, word2):\n",
    "        \"\"\"Identify the relationship between two words: synonym, opposite, unrelated, or related.\n",
    "\n",
    "        Args:\n",
    "            word1 (str): The first word.\n",
    "            word2 (str): The second word.\n",
    "\n",
    "        Returns:\n",
    "            str: The relationship type between the two words.\n",
    "        \"\"\"\n",
    "        synonyms_of_word1 = self.get_synonyms(word1)\n",
    "        antonyms_of_word1 = self.get_opposite_words(word1)\n",
    "        \n",
    "        if word2 in synonyms_of_word1:\n",
    "            return \"synonym\"\n",
    "        elif word2 in antonyms_of_word1:\n",
    "            return \"opposite\"\n",
    "        elif word2 in self.unrelated.get(word1, []):\n",
    "            return \"unrelated\"\n",
    "        else:\n",
    "            return \"related\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab67267-6206-4669-bcbc-55268a87e4f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, word_pairs):\n",
    "        self.word_pairs = word_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word1, word2, similarity = self.word_pairs[idx]\n",
    "        return (torch.tensor(word1, dtype=torch.long), \n",
    "                torch.tensor(word2, dtype=torch.long), \n",
    "                torch.tensor(similarity, dtype=torch.float))\n",
    "\n",
    "\n",
    "class WordEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(WordEmbeddingModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.embeddings(input)\n",
    "\n",
    "\n",
    "class EmbeddingTrainer:\n",
    "    \"\"\"\n",
    "    A class to train word embeddings based on tokenized data.\n",
    "\n",
    "    Attributes:\n",
    "        tokenized_data (pd.Series): The input tokenized data containing words.\n",
    "        vocab (dict): A dictionary mapping words to unique indices.\n",
    "        inv_vocab (dict): A dictionary mapping indices to words.\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "        epochs (int): The number of training epochs.\n",
    "        nlp (spacy.Language): The spaCy model for word relationships.\n",
    "        word_relationships (WordRelationships): An instance for managing word relationships.\n",
    "        word_pairs (list): A list of word pairs generated from the tokenized data.\n",
    "        dataset (EmbeddingDataset): An instance of the dataset containing word pairs.\n",
    "        data_loader (DataLoader): A PyTorch DataLoader for batching the dataset.\n",
    "        model (WordEmbeddingModel): The neural network model for word embeddings.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for updating model weights.\n",
    "        criterion (nn.Module): The loss function used for training.\n",
    "        word_vectors (KeyedVectors): A Gensim KeyedVectors instance for storing embeddings.\n",
    "        symmetric (str): Defines behavior for handling symmetric word pairs ('keep' or 'drop').\n",
    "        relationships_df (pd.DataFrame): A DataFrame to store similarity relationships for word pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenized_data: pd.Series, embedding_dim=100, batch_size=32, epochs=20, learning_rate=0.01, window_size=2, topn=10, symmetric='keep'):\n",
    "        self.tokenized_data = tokenized_data\n",
    "        self.vocab, self.inv_vocab = self.build_vocabulary(tokenized_data)  # Build vocabulary from tokenized data\n",
    "        self.vocab_size = len(self.vocab)  # Calculate vocabulary size here\n",
    "        self.epochs = epochs\n",
    "        self.nlp = spacy.load(\"en_core_web_md\")  # Load the spaCy model for word relationships\n",
    "\n",
    "        # Pass the nlp instance to WordRelationships\n",
    "        self.word_relationships = WordRelationships(self.vocab, self.nlp)  # Create an instance to handle word relationships\n",
    "        self.word_pairs = self.generate_word_pairs(window_size, topn)  # Generate word pairs based on the context\n",
    "        self.dataset = EmbeddingDataset(self.word_pairs)  # Create a dataset instance for word pairs\n",
    "        self.data_loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)  # Create a DataLoader for batching\n",
    "        self.model = WordEmbeddingModel(self.vocab_size, embedding_dim)  # Initialize the word embedding model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)  # Define the optimizer\n",
    "        self.criterion = nn.CosineEmbeddingLoss()  # Define the loss function\n",
    "        self.word_vectors = KeyedVectors(vector_size=embedding_dim)  # Initialize KeyedVectors for storing embeddings\n",
    "        self.symmetric = symmetric  # Store the symmetry behavior choice\n",
    "        \n",
    "        # Create a DataFrame to store relationships between word pairs\n",
    "        self.relationships_df = pd.DataFrame(index=[f\"{self.inv_vocab[word1_idx]}<-->{self.inv_vocab[word2_idx]}\" \n",
    "                                                    for word1_idx, word2_idx, _ in self.word_pairs])\n",
    "\n",
    "    def generate_word_pairs(self, window_size=2, topn=10):\n",
    "        \"\"\"\n",
    "        Generates word pairs from the tokenized data within a specified context window.\n",
    "    \n",
    "        Each word in the tokenized data is paired with its surrounding context words,\n",
    "        with the option to filter out pairs based on the similarity relationships.\n",
    "    \n",
    "        Parameters:\n",
    "            window_size (int): The size of the context window to look around each word.\n",
    "            topn (int): The number of top context words to consider for each target word (not used in this implementation).\n",
    "    \n",
    "        Returns:\n",
    "            list: A list of tuples, where each tuple contains the index of a target word,\n",
    "                  the index of a context word, and a similarity flag (1 for opposite words, 0 otherwise).\n",
    "        \"\"\"\n",
    "        word_pairs = []  # Initialize a list to store word pairs\n",
    "        for document in self.tokenized_data:  # Iterate over each document in the tokenized data\n",
    "            words = document.split()  # Split the document into individual words\n",
    "            for i, word in enumerate(words):  # Enumerate through the words to get their indices\n",
    "                # Define the start and end indices for the context window\n",
    "                start = max(0, i - window_size)  # Ensure start index is not negative\n",
    "                end = min(len(words), i + window_size + 1)  # Ensure end index does not exceed the word list\n",
    "                # Extract context words from the current window, excluding the target word itself\n",
    "                context_words = [words[j] for j in range(start, end) if j != i]\n",
    "                for context_word in context_words:  # Iterate through the context words\n",
    "                    # Check if both the target and context words are in the vocabulary\n",
    "                    if word in self.vocab and context_word in self.vocab:\n",
    "                        # Set similarity flag: 1 if the word has no opposite words, else 0\n",
    "                        similarity = 1 if self.word_relationships.get_opposite_words(word) == [] else 0\n",
    "                        # Append the target word index, context word index, and similarity to the word_pairs list\n",
    "                        word_pairs.append((self.vocab[word], self.vocab[context_word], similarity))\n",
    "        return word_pairs  # Return the list of generated word pairs\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the word embedding model for a specified number of epochs.\n",
    "    \n",
    "        This method performs the forward and backward passes through the model, \n",
    "        computes the loss using cosine embedding loss, and updates the model weights \n",
    "        using the Adam optimizer. It also tracks the average loss for each epoch \n",
    "        and updates the relationships DataFrame after training.\n",
    "    \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.model.train()  # Set the model to training mode\n",
    "        \n",
    "        for epoch in range(self.epochs):  # Iterate over each epoch\n",
    "            total_loss = 0  # Reset total loss at the start of each epoch\n",
    "            \n",
    "            for word1, word2, similarity in self.data_loader:  # Iterate through batches from the data loader\n",
    "                # Check if any index is out of range\n",
    "                if word1.max() >= self.vocab_size or word2.max() >= self.vocab_size:  # Ensure indices are within vocabulary size\n",
    "                    print(\"Index out of range!\")  # Print warning if index is out of range\n",
    "                    continue  # Skip this batch if indices are out of range\n",
    "    \n",
    "                self.optimizer.zero_grad()  # Clear previous gradients\n",
    "                \n",
    "                # Forward pass for word1 and word2 to get their embeddings\n",
    "                emb1 = self.model(word1)  # Get embeddings for the first word\n",
    "                emb2 = self.model(word2)  # Get embeddings for the second word\n",
    "    \n",
    "                # Compute loss using cosine embedding loss\n",
    "                loss = self.criterion(emb1, emb2, similarity)  # Calculate the loss\n",
    "                loss.backward()  # Perform the backward pass\n",
    "                \n",
    "                self.optimizer.step()  # Update model weights\n",
    "                \n",
    "                total_loss += loss.item()  # Accumulate the total loss\n",
    "    \n",
    "            # Calculate and print average loss for the epoch\n",
    "            average_loss = total_loss / len(self.data_loader)  # Compute average loss\n",
    "            display(Markdown(f\"<strong style='color: Chartreuse; font-size: 20'>\\nEpoch [{epoch + 1}/{self.epochs}], Loss: {average_loss:.4f}</strong>\")) \n",
    "    \n",
    "            # Update relationships DataFrame for each pair after training\n",
    "            self.update_relationships(epoch)  # Update relationships based on training results\n",
    "    \n",
    "            self.evaluate_embeddings()  # Call the evaluation function to assess embeddings\n",
    "\n",
    "\n",
    "    def update_relationships(self, epoch):\n",
    "        \"\"\"\n",
    "        Updates the relationships DataFrame with similarity values for word pairs \n",
    "        after training.\n",
    "    \n",
    "        This method computes the cosine similarity between embeddings of word pairs \n",
    "        generated during training. It adds the similarity values to a new column \n",
    "        corresponding to the current epoch in the relationships DataFrame.\n",
    "    \n",
    "        Args:\n",
    "            epoch (int): The current epoch number, used to create a unique column \n",
    "                         for the similarity values in the DataFrame.\n",
    "    \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        epoch_column = f\"Epoch {epoch + 1}\"  # Create a unique column name for the current epoch\n",
    "    \n",
    "        # Create a new column for the current epoch in the relationships DataFrame\n",
    "        self.relationships_df[epoch_column] = None\n",
    "    \n",
    "        # Iterate through the generated word pairs\n",
    "        for word1_idx, word2_idx, _ in self.word_pairs:\n",
    "            # Check if both word indices exist in the inverse vocabulary\n",
    "            if word1_idx in self.inv_vocab and word2_idx in self.inv_vocab:\n",
    "                word1 = self.inv_vocab[word1_idx]  # Get the first word from the inverse vocabulary\n",
    "                word2 = self.inv_vocab[word2_idx]  # Get the second word from the inverse vocabulary\n",
    "                pair_label = f\"{word1}<-->{word2}\"  # Create a label for the word pair\n",
    "                \n",
    "                # Get embeddings for the word pair\n",
    "                emb1 = self.model(torch.tensor(word1_idx, dtype=torch.long).unsqueeze(0))  # Get embedding for word1\n",
    "                emb2 = self.model(torch.tensor(word2_idx, dtype=torch.long).unsqueeze(0))  # Get embedding for word2\n",
    "    \n",
    "                # Check if embeddings are non-zero\n",
    "                if torch.count_nonzero(emb1) > 0 and torch.count_nonzero(emb2) > 0:\n",
    "                    # Calculate the cosine similarity between the two embeddings\n",
    "                    similarity = torch.dot(emb1.view(-1), emb2.view(-1)).item()\n",
    "                    \n",
    "                    # Store the similarity value in the corresponding epoch column\n",
    "                    self.relationships_df.loc[pair_label, epoch_column] = similarity\n",
    "                    self.relationships_df.index.name = \"WordPair\"  # Set the index name for the DataFrame\n",
    "\n",
    "\n",
    "    def evaluate_embeddings(self):\n",
    "        \"\"\"\n",
    "        Evaluates the learned embeddings by checking the similarity for word pairs.\n",
    "    \n",
    "        This method calculates the cosine similarity between the embeddings of word \n",
    "        pairs. It tracks evaluated pairs to avoid duplicate calculations, and it can \n",
    "        handle symmetry based on the class's `symmetric` attribute.\n",
    "    \n",
    "        If `symmetric` is set to 'drop', only unique pairs (word1, word2) are evaluated.\n",
    "        If set to 'keep', both (word1, word2) and (word2, word1) pairs are evaluated.\n",
    "    \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        seen_pairs = set()  # To track already evaluated pairs\n",
    "    \n",
    "        # Iterate through the generated word pairs\n",
    "        for word1_idx, word2_idx, _ in self.word_pairs:\n",
    "            if self.symmetric == 'drop':\n",
    "                # Only calculate similarity for unique pairs (word1, word2)\n",
    "                if (word2_idx, word1_idx) in seen_pairs:\n",
    "                    continue  # Skip already evaluated pairs\n",
    "                seen_pairs.add((word1_idx, word2_idx))  # Track this pair as seen\n",
    "    \n",
    "            # Check if both word indices exist in the inverse vocabulary\n",
    "            if word1_idx in self.inv_vocab and word2_idx in self.inv_vocab:\n",
    "                word1 = self.inv_vocab[word1_idx]  # Get the first word from the inverse vocabulary\n",
    "                word2 = self.inv_vocab[word2_idx]  # Get the second word from the inverse vocabulary\n",
    "                \n",
    "                # Get embeddings for the word pair\n",
    "                emb1 = self.model(torch.tensor(word1_idx, dtype=torch.long).unsqueeze(0))  # Embedding for word1\n",
    "                emb2 = self.model(torch.tensor(word2_idx, dtype=torch.long).unsqueeze(0))  # Embedding for word2\n",
    "    \n",
    "                # Check if embeddings are non-zero\n",
    "                if torch.count_nonzero(emb1) > 0 and torch.count_nonzero(emb2) > 0:\n",
    "                    # Calculate the cosine similarity between the two embeddings\n",
    "                    similarity = torch.dot(emb1.view(-1), emb2.view(-1)).item()\n",
    "    \n",
    "                    # Get the relationship category (synonym, opposite, unrelated, related)\n",
    "                    category = self.word_relationships.get_relationship(word1, word2)\n",
    "    \n",
    "                    # Print similarity for the pair\n",
    "                    print(f'{word1} and {word2}: {similarity:.4f}')  # Uncomment to show the category: ({category})\n",
    "    \n",
    "                if self.symmetric == 'keep':\n",
    "                    # If keeping symmetry, calculate the reverse as well\n",
    "                    emb1, emb2 = emb2, emb1  # Swap embeddings\n",
    "                    similarity = torch.dot(emb1.view(-1), emb2.view(-1)).item()  # Recalculate similarity\n",
    "                    print(f'{self.inv_vocab[word2_idx]}<-->{self.inv_vocab[word1_idx]}: {similarity:.4f}')  # Print reversed pair similarity\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def build_vocabulary(tokenized_data):\n",
    "        \"\"\"\n",
    "        Builds a vocabulary from the tokenized data.\n",
    "    \n",
    "        This method creates two dictionaries: \n",
    "        - `vocab`: mapping of words to unique indices.\n",
    "        - `inv_vocab`: mapping of indices back to words.\n",
    "    \n",
    "        Args:\n",
    "            tokenized_data (pd.Series): A series containing tokenized documents.\n",
    "    \n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - vocab (dict): A dictionary mapping words to indices.\n",
    "                - inv_vocab (dict): A dictionary mapping indices back to words.\n",
    "        \"\"\"\n",
    "        vocab = {}  # Dictionary to hold the vocabulary mapping\n",
    "        index = 0   # Initialize index for unique word assignment\n",
    "    \n",
    "        # Iterate through each document in the tokenized data\n",
    "        for document in tokenized_data:\n",
    "            # Split document into words\n",
    "            for word in document.split():\n",
    "                # If the word is not already in the vocabulary, add it\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = index  # Map word to the current index\n",
    "                    index += 1  # Increment the index for the next unique word\n",
    "                \n",
    "        # Create an inverse vocabulary mapping\n",
    "        inv_vocab = {index: word for word, index in vocab.items()}\n",
    "        return vocab, inv_vocab  # Return the vocabulary and its inverse\n",
    "    \n",
    "\n",
    "    def save_embeddings(self, filename='custom_embeddings.bin'):\n",
    "        \"\"\"\n",
    "        Save the learned embeddings to a .bin file using Gensim's KeyedVectors format.\n",
    "    \n",
    "        This method retrieves the learned embeddings from the model and saves them\n",
    "        in a binary format compatible with Gensim. It ensures that the vocabulary size\n",
    "        matches the number of embedding vectors before saving.\n",
    "    \n",
    "        Args:\n",
    "            filename (str): The name of the file to save embeddings to. Defaults to 'custom_embeddings.bin'.\n",
    "    \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        # Retrieve the learned embeddings as a NumPy array\n",
    "        embeddings = self.model.embeddings.weight.data.numpy()\n",
    "        \n",
    "        # Ensure the vocab size matches the number of embedding vectors\n",
    "        if len(self.inv_vocab) != embeddings.shape[0]:\n",
    "            print(\"Error: Vocab size and embedding size do not match.\")\n",
    "            return  # Exit if there is a mismatch\n",
    "        \n",
    "        # Initialize KeyedVectors with preallocated vocab size and embedding dimensions\n",
    "        vector_size = embeddings.shape[1]\n",
    "        keyed_vectors = KeyedVectors(vector_size=vector_size)\n",
    "        \n",
    "        # Collect all words and their vectors in a list\n",
    "        words = list(self.inv_vocab.values())\n",
    "        \n",
    "        # Use add_vectors to add all vectors in one batch\n",
    "        keyed_vectors.add_vectors(words, embeddings)\n",
    "        \n",
    "        # Save in a binary Word2Vec format\n",
    "        keyed_vectors.save_word2vec_format(filename, binary=True)\n",
    "        print(f\"Embeddings successfully saved to {filename}\")  # Confirmation message\n",
    "    \n",
    "\n",
    "# Retrieve tokenized data for training and validation from stage_5\n",
    "X_train, X_val, _, _, _ = pipeline.get_tokenized_data('stage_5')\n",
    "\n",
    "# Combine training and validation data, and select the first 10 entries for demonstration\n",
    "tokenized_data = pd.concat([X_train, X_val], ignore_index=True).head(10)\n",
    "\n",
    "# Initialize the EmbeddingTrainer with the tokenized data\n",
    "# 'symmetric' parameter is set to 'drop' to avoid calculating symmetry for word pairs\n",
    "trainer = EmbeddingTrainer(tokenized_data, symmetric='drop')\n",
    "\n",
    "# Train the embedding model using the initialized trainer\n",
    "trainer.train()\n",
    "\n",
    "# Save the learned embeddings to a binary file\n",
    "trainer.save_embeddings('custom_embeddings.bin')\n",
    "\n",
    "# Generate word pairs from the tokenized data\n",
    "word_pairs = trainer.generate_word_pairs()\n",
    "\n",
    "# Convert the word pairs (list of tuples) into a Pandas DataFrame\n",
    "word_pairs_df = pd.DataFrame(word_pairs, columns=['Word1', 'Word2', 'Similarity'])\n",
    "\n",
    "# Save the DataFrame containing word pairs to a CSV file\n",
    "word_pairs_df.to_csv('word_pairs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e949701a-1636-4f43-aba2-265854d22f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display the DataFrame of relationships\n",
    "display(trainer.relationships_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd599a",
   "metadata": {},
   "source": [
    "#### 4. Use embeddings learned during previous task for classification with any sklearn model. Try to use google pretrained embeddings as well: https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25554645-9e03-417d-832c-5686dff553cf",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Evaluation with Trained Embedding\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a5bee-896b-43f2-a099-f1f25086ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, pipeline, stage_indices, custom_embeddings_path, additional_embeddings_path=None):\n",
    "        \"\"\"\n",
    "        Initializes the EmbeddingModel.\n",
    "\n",
    "        Args:\n",
    "            pipeline: The data processing pipeline used to retrieve tokenized data.\n",
    "            stage_indices (list): List of integers representing the stages to process.\n",
    "            custom_embeddings_path (str): Path to the custom embeddings file.\n",
    "            additional_embeddings_path (str, optional): Path to additional embeddings file. Default is None.\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.stages = [f'stage_{i}' for i in stage_indices]  # Format stages from integers\n",
    "        self.custom_embeddings_path = custom_embeddings_path\n",
    "        self.additional_embeddings_path = additional_embeddings_path\n",
    "        self.custom_embeddings = None\n",
    "        self.additional_embeddings = None\n",
    "        self.combined_embeddings = None\n",
    "\n",
    "    def load_embeddings(self, file_path):\n",
    "        \"\"\"Load embeddings using Gensim from a binary file.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the embeddings file.\n",
    "\n",
    "        Returns:\n",
    "            KeyedVectors: Loaded embeddings if successful, None otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def load_all_embeddings(self):\n",
    "        \"\"\"Load custom and optional additional embeddings.\"\"\"\n",
    "        print(\"Loading custom embeddings...\")\n",
    "        self.custom_embeddings = self.load_embeddings(self.custom_embeddings_path)\n",
    "\n",
    "        if self.additional_embeddings_path is not None:  # Only load additional embeddings if a path is provided\n",
    "            print(\"Loading additional embeddings...\")\n",
    "            self.additional_embeddings = self.load_embeddings(self.additional_embeddings_path)\n",
    "\n",
    "        # Check if loading was successful\n",
    "        if self.custom_embeddings is not None:\n",
    "            print(f\"Custom embeddings loaded. Number of words: {len(self.custom_embeddings)}\")\n",
    "        else:\n",
    "            print(\"Failed to load custom embeddings.\")\n",
    "\n",
    "        if self.additional_embeddings is not None:\n",
    "            print(f\"Additional embeddings loaded. Number of words: {len(self.additional_embeddings)}\")\n",
    "        else:\n",
    "            print(\"Failed to load additional embeddings.\")\n",
    "\n",
    "    def create_combined_embeddings(self):\n",
    "        \"\"\"Create a combined embedding dictionary by concatenating vectors from both.\"\"\"\n",
    "        if self.custom_embeddings is None:\n",
    "            print(\"Custom embeddings must be loaded before creating combined embeddings.\")\n",
    "            return\n",
    "\n",
    "        combined_embeddings = {}\n",
    "        for word in self.custom_embeddings.index_to_key:  # Access words in custom embeddings\n",
    "            combined_vector = self.custom_embeddings[word]  # Start with custom embeddings\n",
    "            if self.additional_embeddings is not None and word in self.additional_embeddings:  # Check for additional embeddings\n",
    "                # Concatenate vectors from both embeddings\n",
    "                combined_vector = np.concatenate((combined_vector, self.additional_embeddings[word]))\n",
    "            combined_embeddings[word] = combined_vector\n",
    "            \n",
    "        self.combined_embeddings = combined_embeddings\n",
    "\n",
    "    def prepare_data(self, stage):\n",
    "        \"\"\"Load embeddings, create combined embeddings, and prepare data for a specific stage.\n",
    "\n",
    "        Args:\n",
    "            stage (str): The stage to prepare data for.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (X_train_embeddings, X_val_embeddings, y_train, y_val) or (None, None, None, None) on failure.\n",
    "        \"\"\"\n",
    "        self.load_all_embeddings()  # Load all embeddings\n",
    "        self.create_combined_embeddings()  # Create combined embeddings\n",
    "\n",
    "        # Retrieve data using the pipeline for the specified stage\n",
    "        X_train, X_val, y_train, y_val, test_data = self.pipeline.get_tokenized_data(stage)\n",
    "\n",
    "        if self.combined_embeddings is None:\n",
    "            print(\"Combined embeddings must be created before preparing data.\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        # Prepare embedding for the datasets\n",
    "        embedding_size = len(next(iter(self.combined_embeddings.values())))\n",
    "        X_train_embeddings = np.zeros((len(X_train), embedding_size))\n",
    "        X_val_embeddings = np.zeros((len(X_val), embedding_size))\n",
    "\n",
    "        for i, text in enumerate(X_train):\n",
    "            words = text.split()\n",
    "            valid_vectors = [self.combined_embeddings[word] for word in words if word in self.combined_embeddings]\n",
    "            if valid_vectors:\n",
    "                X_train_embeddings[i] = np.mean(valid_vectors, axis=0)\n",
    "            else:\n",
    "                X_train_embeddings[i] = np.zeros(embedding_size)\n",
    "\n",
    "        for i, text in enumerate(X_val):\n",
    "            words = text.split()\n",
    "            valid_vectors = [self.combined_embeddings[word] for word in words if word in self.combined_embeddings]\n",
    "            if valid_vectors:\n",
    "                X_val_embeddings[i] = np.mean(valid_vectors, axis=0)\n",
    "            else:\n",
    "                X_val_embeddings[i] = np.zeros(embedding_size)\n",
    "\n",
    "        return X_train_embeddings, X_val_embeddings, y_train, y_val\n",
    "\n",
    "    def train_model(self, X_train_embeddings, y_train):\n",
    "        \"\"\"Train a logistic regression model.\n",
    "\n",
    "        Args:\n",
    "            X_train_embeddings (ndarray): Training data embeddings.\n",
    "            y_train (array): Training labels.\n",
    "\n",
    "        Returns:\n",
    "            LogisticRegression: Trained logistic regression model.\n",
    "        \"\"\"\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train_embeddings, y_train)\n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self, model, X_val_embeddings, y_val):\n",
    "        \"\"\"Predict and evaluate the model.\n",
    "\n",
    "        Args:\n",
    "            model (LogisticRegression): The trained model.\n",
    "            X_val_embeddings (ndarray): Validation data embeddings.\n",
    "            y_val (array): Validation labels.\n",
    "        \"\"\"\n",
    "        y_pred = model.predict(X_val_embeddings)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        display(Markdown(f\"<strong style='color: Chartreuse; font-size: 20'>Validation Accuracy: {accuracy * 100:.2f}%</strong>\"))\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Load embeddings, prepare data, train and evaluate the model for each stage.\"\"\"\n",
    "        for stage in self.stages:\n",
    "            print(f\"\\nProcessing {stage}...\")\n",
    "            X_train_embeddings, X_val_embeddings, y_train, y_val = self.prepare_data(stage)\n",
    "            if X_train_embeddings is not None and X_val_embeddings is not None and y_train is not None and y_val is not None:\n",
    "                model = self.train_model(X_train_embeddings, y_train)  # Train the model\n",
    "                self.evaluate_model(model, X_val_embeddings, y_val)  # Evaluate the model\n",
    "\n",
    "\n",
    "# Paths to your embeddings\n",
    "custom_embeddings_path = 'custom_embeddings.bin'  # Path to your custom binary embeddings\n",
    "additional_embeddings_path = 'GoogleNews-vectors-negative300.bin'  # Path to additional embeddings (optional)\n",
    "\n",
    "# Assuming 'pipeline' is already defined and available\n",
    "stage_indices = [3]  # Specify the stages as integers\n",
    "\n",
    "# Instantiate the EmbeddingModel\n",
    "embedding_model = EmbeddingModel(pipeline, stage_indices, custom_embeddings_path, additional_embeddings_path)\n",
    "\n",
    "# Run the entire process for all specified stages\n",
    "embedding_model.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394455b3-8d02-45bf-8a82-feb1bad47bae",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: Red; font-size: 20px; font-weight: italics;\">\n",
    "Displayed results above are not true representative of the test. Small portion of files used for testing due to size and memory requirements. Same with question 5 results\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87af3b5-6cc6-4b49-bd9d-ad0c5bef4cd2",
   "metadata": {},
   "source": [
    "#### 5. Take NN trained in previous task and replace last linear layer with linear layer for classification (just different number of outputs (you can do it as nn.last_layer = nn.Linear(hiiden, 2)). Train and test it as in the 2nd task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c7d14-6eef-4f7a-a5b4-b44769f14bda",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Replacing NN Layer and Classification With Trained Embedding\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5907266-8ff5-47a9-8f08-79eb88ef5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "class ClassifierManager:\n",
    "    def __init__(self, classifier, embedding_file_paths):\n",
    "        \"\"\"\n",
    "        Initialize the ClassifierManager with the classifier instance and embedding file paths.\n",
    "\n",
    "        Parameters:\n",
    "        - classifier: An instance of the MLPClassifier.\n",
    "        - embedding_file_paths: A list of paths to embedding files for the classifier.\n",
    "        \"\"\"\n",
    "        self.classifier = classifier\n",
    "        # Set up the classifier for embedding-based data\n",
    "        self.classifier.vectorizer.data_type = 'embedding'\n",
    "        self.classifier.embedding_file_paths = embedding_file_paths\n",
    "        self.replaced_layer_index = -1  # Track the index of the replaced layer\n",
    "\n",
    "    def replace_last_layer(self):\n",
    "        \"\"\"Replace the last layer of the classifier with a new linear layer for binary classification.\"\"\"\n",
    "        # Create a new linear layer adjusted for binary classification\n",
    "        new_layer = nn.Linear(self.classifier.hidden_size[-1], 2)\n",
    "        \n",
    "        if hasattr(self.classifier, 'update_layer'):\n",
    "            # Call the update_layer method from the original classifier to replace the last layer\n",
    "            self.classifier.update_layer(\n",
    "                index=self.replaced_layer_index,  # Update the last layer\n",
    "                new_layer=new_layer,\n",
    "                new_learning_rate=0.0001,\n",
    "                new_batch_size=None,\n",
    "                new_num_epochs=None\n",
    "            ) \n",
    "        else:\n",
    "            print(\"Error: update_layer method not found in the classifier.\")\n",
    "\n",
    "    def train_and_test(self):\n",
    "        \"\"\"Train the classifier and then get test predictions.\"\"\"\n",
    "        # Call the train method from the original classifier to train the model\n",
    "        self.classifier.train()\n",
    "        # Call the test method from the original classifier to get predictions\n",
    "        test_predictions = self.classifier.test()\n",
    "        return test_predictions\n",
    "\n",
    "    def display_layers(self):\n",
    "        \"\"\"Display the classifier network layers after modification, indicating the replaced layer.\"\"\"\n",
    "        # Create a DataFrame with layer indices and types by accessing the original classifier's network\n",
    "        layers = list(enumerate(self.classifier.network.children()))\n",
    "        df_after = pd.DataFrame(layers, columns=['Index', 'Layer'])\n",
    "        \n",
    "        # Mark the replaced layer with an indicator\n",
    "        replaced_layer_info = f\" (Replaced)\" if self.replaced_layer_index == -1 else \"\"\n",
    "        df_after.loc[df_after['Index'] == self.replaced_layer_index, 'Layer'] = (\n",
    "            df_after.loc[df_after['Index'] == self.replaced_layer_index, 'Layer']\n",
    "            .astype(str) + replaced_layer_info\n",
    "        )\n",
    "        \n",
    "        print(\"Layers after replacement:\\n\")\n",
    "        display(df_after)\n",
    "        print('\\n')\n",
    "\n",
    "# Usage\n",
    "custom_embeddings_path = 'custom_embeddings.bin'   \n",
    "google_embeddings_path = 'GoogleNews-vectors-negative300.bin'   \n",
    "embedding_file_paths = [custom_embeddings_path, google_embeddings_path]\n",
    "\n",
    "# Create an instance of Vectorizer for handling embeddings\n",
    "vectorizer = Vectorizer(embedding_file_paths=embedding_file_paths, data_type='embedding')\n",
    "# Initialize the classifier using MLPClassifier from the original class\n",
    "classifier = MLPClassifier(stages=[3], vectorizer=vectorizer, hidden_size = [512, 256, 128], num_layers = 3) #increased deepness for large corpus\n",
    "\n",
    "# Initialize the ClassifierManager with the classifier and embedding file paths\n",
    "manager = ClassifierManager(classifier, embedding_file_paths)\n",
    "\n",
    "# Replace the last layer in the classifier and display updated layers\n",
    "manager.replace_last_layer()\n",
    "manager.display_layers()\n",
    "\n",
    "# Train the classifier and obtain test predictions, storing results in a DataFrame\n",
    "test_predictions = pd.DataFrame({'prediction': manager.train_and_test(), 'true_label': None})\n",
    "test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6510a-4dbf-406c-8156-309ca8436b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ccea7-e864-4737-9627-6967d3aefab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
