{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "1b2e3915-4ea5-4393-9844-41209d5e5444",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 'Tesla' exists in Google search: False\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def verify_entity_google_search(entity_name, api_key, cx):\n",
    "    \"\"\"\n",
    "    Verifies if a named entity exists using Google's Custom Search API.\n",
    "    \n",
    "    Args:\n",
    "    - entity_name (str): The name of the entity to check.\n",
    "    - api_key (str): Your Google API Key.\n",
    "    - cx (str): Your Custom Search Engine ID (CX).\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the entity exists, False otherwise.\n",
    "    \"\"\"\n",
    "    # URL for Google Custom Search API\n",
    "    url = f\"https://www.googleapis.com/customsearch/v1?q={entity_name}&key={api_key}&cx={cx}\"\n",
    "    \n",
    "    # Send request to the API\n",
    "    response = requests.get(url).json()\n",
    "    \n",
    "    # Check if the 'items' key exists in the response (meaning search results were found)\n",
    "    if 'items' in response:\n",
    "        # If we get search results, the entity likely exists\n",
    "        return True\n",
    "    else:\n",
    "        # No results, the entity might not exist or isn't prominent enough\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "api_key = \"YOUR_GOOGLE_API_KEY\"  # Replace with your actual API key\n",
    "cx = \"YOUR_CUSTOM_SEARCH_ENGINE_ID\"  # Replace with your actual CSE ID\n",
    "\n",
    "entity_name = \"Tesla\"\n",
    "exists = verify_entity_google_search(entity_name, api_key, cx)\n",
    "print(f\"Entity '{entity_name}' exists in Google search: {exists}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8039e912-ada2-4f32-afc2-e833d5d1877d",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Getting dataset from <a href = https://www.kaggle.com/competitions/nlp-getting-started/data style=\"text-decoration: underline; color : Default\">kaggle</a>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "70f50199-ca35-492b-be1d-d7a42e514498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.12/site-packages/kaggle/__init__.py\n",
      "nlp-getting-started.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Extraction completed\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules for file handling and Kaggle operations\n",
    "import os       \n",
    "import shutil    \n",
    "import zipfile  # Handle ZIP archive files\n",
    "import kaggle   # Kaggle API client for downloading datasets\n",
    "\n",
    "# Print the location of the installed kaggle package\n",
    "print(kaggle.__file__)\n",
    "# Download dataset using the Kaggle API\n",
    "!kaggle competitions download -c nlp-getting-started\n",
    "\n",
    "file_name = os.path.join(os.getcwd(), 'nlp-getting-started.zip')  # Get full path to the ZIP file\n",
    "if os.path.exists(file_name):  # Check if the file exists\n",
    "    with zipfile.ZipFile(file_name, 'r') as zip_f:  # Open ZIP file in read mode\n",
    "        zip_f.extractall(os.getcwd())  # Extract all files to current directory\n",
    "    print('Extraction completed')  # Confirm extraction\n",
    "else:\n",
    "    print(f'\"{file_name}\" does not exist. Download dataset from https://www.kaggle.com/competitions/nlp-getting-started/data')  # File not found message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ee83d",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Data Preprocessing\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "72f098be-94a4-4ae2-8cb1-ad6b219a86d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyxDamerauLevenshtein in /usr/local/anaconda3/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: fuzzywuzzy in /usr/local/anaconda3/lib/python3.12/site-packages (0.18.0)\n"
     ]
    }
   ],
   "source": [
    "# Install the pyxDamerauLevenshtein package for calculating string distance using the Damerau-Levenshtein algorithm\n",
    "!pip install pyxDamerauLevenshtein\n",
    "# Install the fuzzywuzzy package for fuzzy string matching\n",
    "!pip install fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "58449ea6-629f-43b3-91ba-e1c1b9d5456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob  # For sentiment analysis\n",
    "from collections import Counter  # For counting frequencies\n",
    "import re  # For text cleaning using regular expressions \n",
    "import spacy  # For natural language processing\n",
    "from unidecode import unidecode  # For Unicode normalization  \n",
    "import pandas as pd \n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "ed8ed60d-2462-463c-a269-e585d7ab2b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm \n",
    "#!python -m spacy download en_core_web_md \n",
    "#!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "6bb66c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy's language model for advanced NLP features\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "\n",
    "# Define custom stop words that are specific to the context of tweets\n",
    "custom_stop_words = { \n",
    "   'tweets', 'retweet', 'retweets', 'follow', 'following',\n",
    "    'follower', 'followers', 'dm', 'directmessage', 'hashtag', 'amp', 'via', 'twitter'\n",
    "}   \n",
    "\n",
    "# Combine NLTK stopwords, custom stopwords, and spaCy's stopwords into a comprehensive set\n",
    "combined_stop_words = custom_stop_words.union(nlp.Defaults.stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "23995e56-cd7b-467a-aaf0-133dd3f04213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy vocabulary length is: 84780\n"
     ]
    }
   ],
   "source": [
    "print(f\"Spacy vocabulary length is: {len(nlp.vocab.strings ) }\")#check spacy vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "0d71f911-91c3-4052-a635-5eaf3d7121f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataHandler:\n",
    "    def __init__(self, train_files, test_file=None, text_column='text'):\n",
    "        \"\"\"\n",
    "        Initialize the DataHandler class.\n",
    "\n",
    "        :param train_files: List of file paths for training data.\n",
    "        :param test_file: File path for test data (optional).\n",
    "        :param text_column: Name of the text column in the dataset.\n",
    "        \"\"\"\n",
    "        self.train_files = train_files  # List of file paths for training data.\n",
    "        self.test_file = test_file  # File path for test data.\n",
    "        self.text_column = text_column  # Text column name.\n",
    "        self.train_data = None  # Placeholder for the combined training data.\n",
    "        self.test_data = None  # Placeholder for the test data.\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load and concatenate training data from multiple files.\n",
    "        Load the test data separately if provided.\n",
    "        \"\"\"\n",
    "        # Load and concatenate training data\n",
    "        train_dataframes = []\n",
    "        for file in self.train_files:\n",
    "            try:\n",
    "                df = pd.read_csv(file)\n",
    "                train_dataframes.append(df)\n",
    "                print(f\"Loaded training file: {file}, Shape: {df.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading training file {file}: {e}\")\n",
    "        \n",
    "        try:\n",
    "            self.train_data = pd.concat(train_dataframes, ignore_index=True)\n",
    "            print(f\"All training data concatenated successfully. Shape: {self.train_data.shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error concatenating training data: {e}\")\n",
    "        \n",
    "        # Load test data if provided\n",
    "        if self.test_file:\n",
    "            try:\n",
    "                self.test_data = pd.read_csv(self.test_file)\n",
    "                print(f\"Test data loaded successfully. Shape: {self.test_data.shape}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading test data: {e}\")\n",
    "\n",
    "    def validate_train_data(self):\n",
    "        \"\"\"\n",
    "        Validate the loaded training data and output metadata summaries.\n",
    "        \"\"\"\n",
    "        if self.train_data is None:\n",
    "            print(\"No training data loaded. Please load the training data first.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Training Data Shape: {self.train_data.shape}\")\n",
    "        print(f\"Columns: {self.train_data.columns.tolist()}\")\n",
    "        print(\"Data Types:\")\n",
    "        print(self.train_data.dtypes)\n",
    "        print(\"Missing Values:\")\n",
    "        print(self.train_data.isnull().sum())\n",
    "        print(\"Sample Training Data:\")\n",
    "        print(self.train_data.head())\n",
    "\n",
    "    def validate_test_data(self):\n",
    "        \"\"\"\n",
    "        Validate the loaded test data and output metadata summaries.\n",
    "        \"\"\"\n",
    "        if self.test_data is None:\n",
    "            print(\"No test data loaded. Please load the test data first.\")\n",
    "            return\n",
    "\n",
    "        print(f\"Test Data Shape: {self.test_data.shape}\")\n",
    "        print(f\"Columns: {self.test_data.columns.tolist()}\")\n",
    "        print(\"Data Type\")\n",
    "        print(self.test_data.dtypes)\n",
    "        print(\"Missing Values:\")\n",
    "        print(self.test_data.isnull().sum())\n",
    "        print(\"Sample Test Data:\")\n",
    "        print(self.test_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "8336082f-9d97-43cf-a16b-146220e82026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "from spacy.language import Language\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self, stages=[]):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.stop_words = self.nlp.Defaults.stop_words\n",
    "        self.valid_words = set(self.nlp.vocab.strings)\n",
    "        self.stages = stages\n",
    "        \n",
    "        self.patterns = {\n",
    "            'HASHTAG': re.compile(r'#\\w+'),\n",
    "            'USERNAME': re.compile(r'@\\w+'),\n",
    "            'TWITTER_URL': re.compile(r'https?://t.co/\\S+'),\n",
    "            'TELEGRAM_URL': re.compile(r'https?://t.me/\\S+'),\n",
    "            'WEB_URL': re.compile(r'https?://www\\.\\S+')\n",
    "        }\n",
    "        self.date_patterns = [\n",
    "            r'\\b(?:\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4})\\b',\n",
    "            r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2},? \\d{4}\\b',\n",
    "            r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) \\d{1,2},? \\d{4}\\b',\n",
    "            r'\\b\\d{4}-(?:0[1-9]|1[0-2])-(?:0[1-9]|[12][0-9]|3[01])\\b',\n",
    "            r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December) \\d{4}\\b',\n",
    "            r'\\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) \\d{4}\\b',\n",
    "            r'\\b(?:0?[1-9]|[12][0-9]|3[01])-(?:January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)-\\d{4}\\b',\n",
    "            r'\\b\\d{4} (?:January|February|March|April|May|June|July|August|September|October|November|December) \\d{1,2}\\b',\n",
    "            r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\b',\n",
    "            r'\\b(?:19[0-9]{2}|20[0-9]{2})\\b',\n",
    "            r'\\b(?:\\d{1,2}(?:st|nd|rd|th)? (?:January|February|March|April|May|June|July|August|September|October|November|December),? \\d{4})\\b',\n",
    "            r'\\b(?:\\d{1,2}(?:st|nd|rd|th)? of (?:January|February|March|April|May|June|July|August|September|October|November|December),? \\d{4})\\b'\n",
    "        ]\n",
    "\n",
    "        \n",
    "        self.regex_filter_patterns = [ \n",
    "            r'https?://\\S+|www\\.\\S+',           # Drop all links (URLs).\n",
    "            r'#(?!\\S)',                         # Drop standalone hashtags (e.g., #).\n",
    "            r'\\b\\d+\\b',                         # Drop standalone numbers.\n",
    "            r'\\b(\\w)\\1{1,}\\b',                  # Drop standalone two-letter words with repeating letters (e.g., \"aa\").\n",
    "            r'[^\\w\\s#@]',                       # Replace non-alphanumeric characters with space (already done).\n",
    "            r'(?<!\\S)\\b\\w{1,2}\\b(?!\\S)',        # Drop single-letter or two-letter words.\n",
    "            r'[^\\x00-\\x7F]',                    # Drop non-ASCII characters.\n",
    "            r'[^\\w#@]+$',                       # Remove trailing non-word characters after words, hashtags, mentions, or URLs.\n",
    "        ]\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        self._add_custom_components()\n",
    "\n",
    "    def _ensure_text(self, input_data):\n",
    "        \"\"\"Ensure input is in string format for uniform processing.\"\"\"\n",
    "        if isinstance(input_data, list):\n",
    "            return ' '.join(input_data)  # Join list into a single string\n",
    "        return input_data\n",
    "\n",
    "    def apply_regex_filter(self, text):\n",
    "        \"\"\"Apply all regex patterns to input text.\"\"\"\n",
    "        text = self._ensure_text(text)\n",
    "        cleaned_text = text\n",
    "        for pattern in self.regex_filter_patterns:\n",
    "            cleaned_text = re.sub(pattern, ' ', cleaned_text)  # Replace matches with space\n",
    "        return cleaned_text.split()  # Return a list of words\n",
    "\n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove stopwords from the list of tokens and return only unique tokens.\"\"\"\n",
    "        if isinstance(tokens, str):\n",
    "            tokens = tokens.split()  # If input is string, split it into tokens\n",
    "        return [word.lower() for word in tokens if word not in self.stop_words]\n",
    "\n",
    "    def drop_mentions(self, text):\n",
    "        \"\"\"Remove username mentions from the input text.\"\"\"\n",
    "        text = self._ensure_text(text)\n",
    "        pattern = r'@\\w+'\n",
    "        cleaned_text = re.sub(pattern, '', text)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "        return cleaned_text.lower().split()  # Return as list of tokens\n",
    "\n",
    "    def drop_hashtags(self, text):\n",
    "        \"\"\"Remove hashtags from the input text.\"\"\"\n",
    "        text = self._ensure_text(text)\n",
    "        pattern = r'#\\S+'\n",
    "        cleaned_text = re.sub(pattern, '', text)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "        return cleaned_text.lower().split()  # Return as list of tokens\n",
    "\n",
    "    def filter_valid_tokens(self, tokens):\n",
    "        \"\"\"Filter out invalid tokens based on word rules, keeping valid usernames starting with @.\"\"\"\n",
    "        if isinstance(tokens, str):\n",
    "            tokens = tokens.split()  # If input is string, split it into tokens\n",
    "    \n",
    "        # Regex pattern for a valid username: starts with '@', followed by letters, numbers, or underscores\n",
    "        username_pattern = r\"^@[a-zA-Z0-9_]+$\"\n",
    "        \n",
    "        return [\n",
    "            token.lower() for token in tokens\n",
    "            if (\n",
    "                # Keep tokens that match the valid username pattern (starting with '@')\n",
    "                re.match(username_pattern, token)\n",
    "                or\n",
    "                # Keep valid words with letters and numbers (e.g., 'word123')\n",
    "                (re.match(r\"^[a-zA-Z]+[0-9]*$\", token) and re.findall(r'[a-zA-Z]+', token)[0] in self.valid_words)\n",
    "                or\n",
    "                # Keep tokens starting with '#' followed by letters and numbers (e.g., '#hashtag')\n",
    "                (token.startswith(\"#\") and re.match(r\"^[#][a-zA-Z]+[0-9]*$\", token))\n",
    "            )\n",
    "        ]\n",
    " \n",
    "    def process_text(self, text):\n",
    "        \"\"\"Process text using spaCy pipeline and active filters.\"\"\"\n",
    "        text = self._ensure_text(text)  # Ensure the input is a string\n",
    "        doc = self.nlp(text)\n",
    "        cleaned_tokens = [token.text for token in doc]\n",
    "        \n",
    "        if \"apply_regex_filter\" in self.stages:\n",
    "            cleaned_tokens = self.apply_regex_filter(\" \".join(cleaned_tokens))\n",
    "        if \"remove_stopwords\" in self.stages:\n",
    "            cleaned_tokens = self.remove_stopwords(cleaned_tokens)\n",
    "        if \"filter_valid_tokens\" in self.stages:\n",
    "            cleaned_tokens = self.filter_valid_tokens(cleaned_tokens)\n",
    "        if \"drop_mentions\" in self.stages:\n",
    "            cleaned_tokens = self.drop_mentions(\" \".join(cleaned_tokens))\n",
    "        if \"drop_hashtags\" in self.stages:\n",
    "            cleaned_tokens = self.drop_hashtags(\" \".join(cleaned_tokens))\n",
    "\n",
    "        return cleaned_tokens\n",
    "\n",
    "\n",
    "    def _add_custom_components(self):\n",
    "        @Language.component(\"regex_matcher\")\n",
    "        def regex_matcher(doc):\n",
    "            matches = []\n",
    "            for label, pattern in self.patterns.items():\n",
    "                for match in pattern.finditer(doc.text):\n",
    "                    span = doc.char_span(match.start(), match.end(), label=label)\n",
    "                    if span:\n",
    "                        matches.append((label, span.text))\n",
    "            doc._.custom_matches = matches\n",
    "            return doc\n",
    "        \n",
    "        @Language.component(\"date_extractor\")\n",
    "        def date_extractor(doc):\n",
    "            raw_dates = []\n",
    "            combined_pattern = '|'.join(self.date_patterns)\n",
    "            for match in re.findall(combined_pattern, doc.text):\n",
    "                raw_dates.append(match)\n",
    "            doc._.raw_dates = raw_dates\n",
    "            doc._.raw_date = random.choice(raw_dates) if raw_dates else None\n",
    "\n",
    "            doc._.date_info = self.convert_to_datetime(doc._.raw_date)\n",
    "            return doc\n",
    "\n",
    "        \n",
    "        @Language.component(\"token_cleaner\")\n",
    "        def token_cleaner(doc):\n",
    "            tokens = [token.text for token in doc]\n",
    "\n",
    "                        \n",
    "            # Run remove_stopwords only if it's in active_components\n",
    "            if \"apply_regex_filter\" in self.stages:\n",
    "                tokens = self.apply_regex_filter(tokens)\n",
    "                        \n",
    "            # Run remove_stopwords only if it's in active_components\n",
    "            if \"remove_stopwords\" in self.stages:\n",
    "                tokens = self.remove_stopwords(tokens)\n",
    "            \n",
    "            # Run filter_valid_tokens only if it's in active_components\n",
    "            if \"filter_valid_tokens\" in self.stages:\n",
    "                tokens = self.filter_valid_tokens(tokens)\n",
    "                \n",
    "            # Run drop_mentions only if it's in active_components\n",
    "            if \"drop_mentions\" in self.stages:\n",
    "                tokens = self.drop_mentions(tokens)\n",
    "            \n",
    "            # Run drop_hashtags only if it's in active_components\n",
    "            if \"drop_hashtags\" in self.stages:\n",
    "                tokens = self.drop_hashtags(tokens)\n",
    "            \n",
    "            doc._.cleaned_tokens = tokens\n",
    "            return doc\n",
    "\n",
    "        # Add custom extensions to store cleaned tokens\n",
    "        if not spacy.tokens.Doc.has_extension(\"cleaned_tokens\"):\n",
    "            spacy.tokens.Doc.set_extension(\"cleaned_tokens\", default=[])\n",
    "        # Add custom extensions\n",
    "        if not spacy.tokens.Doc.has_extension(\"custom_matches\"):\n",
    "            spacy.tokens.Doc.set_extension(\"custom_matches\", default=[])\n",
    "        if not spacy.tokens.Doc.has_extension(\"raw_dates\"):\n",
    "            spacy.tokens.Doc.set_extension(\"raw_dates\", default=[])\n",
    "        if not spacy.tokens.Doc.has_extension(\"raw_date\"):\n",
    "            spacy.tokens.Doc.set_extension(\"raw_date\", default=None)\n",
    "        if not spacy.tokens.Doc.has_extension(\"date_info\"):\n",
    "            spacy.tokens.Doc.set_extension(\"date_info\", default=(None, None)) \n",
    "        if not spacy.tokens.Doc.has_extension(\"cleaned_tokens\"):\n",
    "            spacy.tokens.Doc.set_extension(\"cleaned_tokens\", default=[])\n",
    "\n",
    "        # Add components to pipeline \n",
    "        self.nlp.add_pipe(\"token_cleaner\", first=True)\n",
    "        self.nlp.add_pipe(\"regex_matcher\", last=True)\n",
    "        self.nlp.add_pipe(\"date_extractor\", last=True)\n",
    "\n",
    "\n",
    "    def convert_to_datetime(self, raw_date):\n",
    "        if not raw_date:\n",
    "            return float('nan'), float('nan')\n",
    "        \n",
    "        try:\n",
    "            date = pd.to_datetime(raw_date, format='%d/%m/%Y')\n",
    "            return date, 'full'\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            date = pd.to_datetime(raw_date, format='%d-%m-%Y')\n",
    "            return date, 'full'\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            date = pd.to_datetime(raw_date, format='%B, %d %Y')\n",
    "            return date, 'full'\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            date = pd.to_datetime(raw_date, format='%B %d %Y')\n",
    "            return date, 'full'\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            date = pd.to_datetime(raw_date, format='%B %Y')\n",
    "            date = date.replace(day=1)  # impute day\n",
    "            return date, 'month and year'\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            date = pd.to_datetime(raw_date, format='%B')\n",
    "            return date, 'month'\n",
    "        except ValueError:\n",
    "            pass\n",
    "        try:\n",
    "            date = pd.to_datetime(raw_date, format='%Y')\n",
    "            date = date.replace(day=1, month=1)\n",
    "            return date, 'year'\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        return float('nan'), float('nan')\n",
    "\n",
    "    def process_text(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        cleaned_tokens = doc._.cleaned_tokens\n",
    "        regex_matches = [(label, match) for label, match in doc._.custom_matches]\n",
    "        named_entities = [(ent.label_, ent.text) for ent in doc.ents]\n",
    "        raw_dates = doc._.raw_dates\n",
    "        date_info = doc._.date_info\n",
    "    \n",
    "        # Combine regex matches and named entities\n",
    "        all_entities = regex_matches + named_entities\n",
    "    \n",
    "        # Group entities by their labels into a dictionary\n",
    "        entity_dict = {}\n",
    "        for label, value in all_entities:\n",
    "            if label not in entity_dict:\n",
    "                entity_dict[label] = []\n",
    "            entity_dict[label].append(value)\n",
    "    \n",
    "        # Return the processed results\n",
    "        return cleaned_tokens, entity_dict, raw_dates, date_info\n",
    "\n",
    "\n",
    "\n",
    "    def count_entity_label(self, row, entity_label):\n",
    "        # Retrieve the dictionary of entities for the row\n",
    "        entities_dict = row['entities']\n",
    "    \n",
    "        # Count the occurrences of the specified entity label\n",
    "        return len(entities_dict.get(entity_label, []))\n",
    "\n",
    "    def add_entity_counts(self, df):\n",
    "        # First, create a new column containing all entities for each row\n",
    "        df['entities'] = df['text'].apply(lambda x: self.process_text(x)[1])  # Assuming process_text returns entities as 2nd element\n",
    "    \n",
    "        # Extract unique entity labels across the entire DataFrame\n",
    "        entity_labels = set()\n",
    "        for entities_dict in df['entities']:\n",
    "            entity_labels.update(entities_dict.keys())\n",
    "    \n",
    "        # Sort entity labels to ensure consistent column ordering\n",
    "        entity_labels = sorted(entity_labels)\n",
    "    \n",
    "        # Now create a column for each entity label to count occurrences\n",
    "        for entity_label in entity_labels:\n",
    "            column_name = f\"{entity_label.lower()}_count\"\n",
    "            df[column_name] = df['entities'].apply(\n",
    "                lambda entities_dict: len(entities_dict.get(entity_label, []))\n",
    "            )\n",
    "    \n",
    "        return df\n",
    "\n",
    "        \n",
    "    def add_date_counts(self, df):\n",
    "        df['year_or_month_count'] = df['raw_dates'].apply(lambda x: sum([1 for date in x if len(date.split()) <= 2]))\n",
    "        df['full_date_count'] = df['raw_dates'].apply(lambda x: sum([1 for date in x if len(date.split()) == 3]))\n",
    "        return df\n",
    " \n",
    "    def process_dataframe(self, df):\n",
    "        df['cleaned_tokens'], df['entities'], df['raw_dates'], df['date_info'] = zip(\n",
    "            *df['text'].map(self.process_text)\n",
    "        )\n",
    "        df[\"date_month\"], df[\"date_year\"] = zip(*df[\"date_info\"])\n",
    "        df = self.add_entity_counts(df)\n",
    "        df = self.add_date_counts(df)        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def apply_stages_incrementally(self, df, text_column):\n",
    "        \"\"\"\n",
    "        Apply the stages incrementally and add a column for each stage with the name f'stage{i}'.\n",
    "        Generate a bar plot for the count of unique tokens after each filtering stage,\n",
    "        with the value of each bar displayed on top.\n",
    "    \n",
    "        :param df: Input DataFrame containing the text data.\n",
    "        :param text_column: The name of the column containing the input text.\n",
    "        :return: DataFrame with additional columns for each stage.\n",
    "        \"\"\"\n",
    "        # Lists to hold data for plotting\n",
    "        filter_steps = []\n",
    "        unique_token_counts = []\n",
    "        \n",
    "        for i, stage in enumerate(self.stages, start=1):\n",
    "            # Check if the stage is a valid method in the class\n",
    "            if hasattr(self, stage):\n",
    "                method = getattr(self, stage)\n",
    "                # Apply the method for the current stage and create a new column with name f'stage{i}'\n",
    "                new_column_name = f'stage{i}'\n",
    "                df[new_column_name] = df[text_column].apply(method)\n",
    "                \n",
    "                # Collect data for plotting\n",
    "                filter_steps.append(new_column_name)\n",
    "                tokens = df[new_column_name].explode().dropna().tolist()  # Flatten tokens for counting\n",
    "                unique_token_counts.append(len(set(tokens)))  # Count unique tokens\n",
    "                \n",
    "                # Update the text_column to reflect the latest processing stage\n",
    "                text_column = new_column_name\n",
    "            else:\n",
    "                raise ValueError(f\"Stage '{stage}' is not a valid method.\")\n",
    "        \n",
    "        # Create a figure for the bar chart\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        # Adjust bar width to reduce space between bars\n",
    "        bar_width = 0.4  # Reduce bar width to make bars thinner\n",
    "        x_positions = range(len(filter_steps))  # Set the x positions for the bars directly adjacent\n",
    "        \n",
    "        # Make sure bars are placed directly on top of each other by setting width\n",
    "        bars = ax.bar(x_positions, unique_token_counts, color='skyblue', width=bar_width)\n",
    "        \n",
    "        # Annotate each bar with its value on top\n",
    "        for bar in bars:\n",
    "            yval = bar.get_height()  # Get the height (value) of the bar\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2,  # x position (center of the bar)\n",
    "                yval,  # y position (top of the bar)\n",
    "                f'{yval}',  # Text to display (value of the bar)\n",
    "                ha='center',  # Horizontal alignment\n",
    "                va='bottom',  # Vertical alignment\n",
    "                fontsize=10  # Font size for the annotation\n",
    "            )\n",
    "        \n",
    "        # Customize the plot\n",
    "        ax.set_xlabel('Filtering Steps')\n",
    "        ax.set_ylabel('Count of Unique Tokens')\n",
    "        ax.set_title('Count of Unique Tokens After Each Filtering Step')\n",
    "        \n",
    "        # Set x-tick labels to the names of the filtering steps\n",
    "        ax.set_xticks(x_positions)\n",
    "        ax.set_xticklabels(filter_steps)\n",
    "        \n",
    "        # Rotate x-axis labels for better readability\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "        # Adjust the layout to avoid overlap and make the plot more compact\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "d9e1e1e0-b050-47ae-bc1f-cfbae4207648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emoji\n",
    "#!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "55c788dd-ec88-40a1-bafb-4c2661f0d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "#nltk.download('punkt') \n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "import re\n",
    "import spacy\n",
    "from emoji import is_emoji\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "9958f307-ac5a-454d-89bb-e29b84f03c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "class TokenPipeline:\n",
    "    def __init__(self, train_files, test_file=None, stages=None, text_column=\"text\"):\n",
    "        self.loader = DataHandler(train_files=train_files, test_file=test_file)\n",
    "        self.loader.load_data()  # Load the data using DataHandler\n",
    "        \n",
    "        self.train_data = self.loader.train_data\n",
    "        self.test_data = self.loader.test_data\n",
    "        self.stages = stages if stages else [\n",
    "            \"apply_regex_filter\",\n",
    "            \"remove_stopwords\",\n",
    "            \"filter_valid_tokens\",\n",
    "            \"drop_mentions\",\n",
    "            \"drop_hashtags\"\n",
    "        ]\n",
    "        self.text_column = text_column\n",
    "        self.feature_columns = []\n",
    "        \n",
    "        self.processor = TextProcessor(stages=self.stages)\n",
    "\n",
    "    def prepare_and_process_data(self):\n",
    "        processed_train_data = self.processor.apply_stages_incrementally(self.train_data, self.text_column)\n",
    "        self.processor.process_dataframe(processed_train_data) \n",
    "\n",
    "        processed_test_data = None\n",
    "        if self.test_data is not None:\n",
    "            processed_test_data = self.processor.apply_stages_incrementally(self.test_data, self.text_column)\n",
    "            self.processor.process_dataframe(processed_test_data) \n",
    "            \n",
    "        return processed_train_data, processed_test_data\n",
    "    \n",
    "    def extract_features(self):\n",
    "        # Initialize feature columns\n",
    "        self.feature_columns = [\n",
    "            'text_length', 'word_count', 'exclamation_count', 'capitalized_words_pct', \n",
    "            'sentiment_polarity', 'sentiment_subjectivity', 'url_count', \n",
    "            'emoji_count', 'unique_word_count', 'noun_count', 'verb_count', \n",
    "            'adj_count', 'adverb_count', 'time_of_day', 'readability_score'\n",
    "        ]\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.train_data[self.text_column].apply(self._extract_features_from_text)\n",
    "        \n",
    "        # Create DataFrame from the features list\n",
    "        feature_df = pd.DataFrame(features.tolist(), columns=self.feature_columns)\n",
    "        \n",
    "        return feature_df \n",
    "    \n",
    "    def _extract_features_from_text(self, text):\n",
    "        text = str(text)  # Ensure it's a string\n",
    "        doc = nlp(text)  # Process text using SpaCy\n",
    "\n",
    "        text_length = len(text)\n",
    "        word_count = len([token for token in doc if token.is_alpha])\n",
    "        exclamation_count = text.count('!')\n",
    "\n",
    "        capitalized_words = [token.text for token in doc if token.is_upper]\n",
    "        capitalized_words_pct = len(capitalized_words) / word_count if word_count else 0\n",
    "\n",
    "        sentiment_polarity = None  # Placeholder (SpaCy doesn't have built-in sentiment)\n",
    "        sentiment_subjectivity = None\n",
    "\n",
    "        url_count = len([token for token in doc if token.like_url])\n",
    "        emoji_count = len([token.text for token in doc if is_emoji(token.text)])\n",
    "        \n",
    "        unique_word_count = len(set([token.text.lower() for token in doc if token.is_alpha]))\n",
    "        \n",
    "        noun_count = sum(1 for token in doc if token.pos_ == \"NOUN\")\n",
    "        verb_count = sum(1 for token in doc if token.pos_ == \"VERB\")\n",
    "        adj_count = sum(1 for token in doc if token.pos_ == \"ADJ\")\n",
    "        adverb_count = sum(1 for token in doc if token.pos_ == \"ADV\")\n",
    "\n",
    "        time_of_day = None  # Placeholder (needs additional timestamp logic)\n",
    "\n",
    "        syllables = sum(self._syllables_in_word(token.text) for token in doc if token.is_alpha)\n",
    "        sentences = len(list(doc.sents))\n",
    "        readability_score = 0\n",
    "        if sentences > 0 and word_count > 0:\n",
    "            readability_score = 0.39 * (word_count / sentences) + 11.8 * (syllables / word_count) - 15.59\n",
    "\n",
    "        return [\n",
    "            text_length, word_count, exclamation_count, capitalized_words_pct, \n",
    "            sentiment_polarity, sentiment_subjectivity, url_count, \n",
    "            emoji_count, unique_word_count, noun_count, verb_count, \n",
    "            adj_count, adverb_count, time_of_day, readability_score\n",
    "        ]\n",
    "\n",
    "    def _syllables_in_word(self, word):\n",
    "        word = word.lower()\n",
    "        vowels = \"aeiouy\"\n",
    "        syllables = 0\n",
    "        prev_char = \"\"\n",
    "\n",
    "        for char in word:\n",
    "            if char in vowels and prev_char not in vowels:\n",
    "                syllables += 1\n",
    "            prev_char = char\n",
    "\n",
    "        if word.endswith('e'):\n",
    "            syllables -= 1\n",
    "\n",
    "        return max(syllables, 1)\n",
    "\n",
    "    def move_count_columns(self, df):\n",
    "        count_columns = [col for col in df.columns if col.endswith('count') or col in self.feature_columns]\n",
    "        feature_df = df[count_columns].copy()\n",
    "        df.drop(columns=count_columns, inplace=True)\n",
    "        return df, feature_df\n",
    "\n",
    "    def process_and_move_columns(self):\n",
    "        processed_train_data, processed_test_data = self.prepare_and_process_data()\n",
    "        train_tokens_df, train_entity_count_df = self.move_count_columns(processed_train_data)\n",
    "\n",
    "        test_tokens_df, test_entity_count_df = None, None\n",
    "        if processed_test_data is not None:\n",
    "            test_tokens_df, test_entity_count_df = self.move_count_columns(processed_test_data)\n",
    "\n",
    "        return (train_tokens_df, train_entity_count_df), (test_tokens_df, test_entity_count_df)\n",
    "\n",
    "    @staticmethod\n",
    "    def lemmatize_df(df, column_name, method='nltk'):\n",
    "        \"\"\"\n",
    "        Applies lemmatization on the given DataFrame column based on the chosen method.\n",
    "    \n",
    "        Parameters:\n",
    "        df (pd.DataFrame): The DataFrame with the text data.\n",
    "        column_name (str): The name of the column containing text data.\n",
    "        method (str): The lemmatization method to use ('nltk', 'spacy', 'simple').\n",
    "    \n",
    "        Returns:\n",
    "        pd.DataFrame: The DataFrame with lemmatized tokens in the specified column.\n",
    "        \"\"\"\n",
    "        # Ensure the column contains strings, and handle missing values\n",
    "        df[column_name] = df[column_name].fillna('').astype(str)\n",
    "    \n",
    "        # Function to lemmatize text based on the selected method\n",
    "        def lemmatize_text(text, method):\n",
    "            # Tokenize text (if it's not already tokenized)\n",
    "            tokens = text.split()  # split string into tokens by whitespace\n",
    "            \n",
    "            # Apply lemmatization to the tokens\n",
    "            if method == 'nltk':\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "                return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "            \n",
    "            elif method == 'spacy':\n",
    "                nlp = spacy.load('en_core_web_sm')\n",
    "                doc = nlp(' '.join(tokens))\n",
    "                return [token.lemma_ for token in doc]\n",
    "            \n",
    "            elif method == 'simple':\n",
    "                return [re.sub(r'(ing|ed|es|ly|s)$', '', word) for word in tokens]\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(\"Invalid method. Choose either 'nltk', 'spacy', or 'simple'.\")\n",
    "    \n",
    "        # Apply the lemmatization to each entry in the column\n",
    "        df[column_name] = df[column_name].apply(lambda x: lemmatize_text(x, method))\n",
    "        \n",
    "        return df\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16864379-bd1f-4776-ac31-752afeccd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_df = pipeline.extract_features() \n",
    "#train_tokens_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "e9d93d70-80a7-4f6c-af20-6cd2a7796aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureCorrelation:\n",
    "    def __init__(self, train_tokens_df, feature_df, target_column, correlation_threshold=0.8):\n",
    "        self.train_tokens_df = train_tokens_df\n",
    "        self.feature_df = feature_df\n",
    "        self.target_column = target_column\n",
    "        self.correlation_threshold = correlation_threshold\n",
    "    \n",
    "    def encode_non_numeric(self, df):\n",
    "        \"\"\"Encodes non-numeric columns to numeric using LabelEncoder, but keeps the original values intact.\"\"\"\n",
    "        le = LabelEncoder()\n",
    "        encoded_df = df.copy()  # Create a copy to preserve the original data\n",
    "\n",
    "        for col in df.select_dtypes(exclude=[float, int]).columns:\n",
    "            if df[col].dtype == 'object':  # Only encode text columns\n",
    "                encoded_df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "        return encoded_df\n",
    "\n",
    "    def calculate_correlation(self, df):\n",
    "        \"\"\"Calculates the correlation matrix for the given DataFrame.\"\"\"\n",
    "        return df.corr()\n",
    "\n",
    "    def remove_multicollinearity(self, df, corr_matrix):\n",
    "        \"\"\"Removes multicollinearity by dropping columns with correlation greater than the threshold.\"\"\"\n",
    "        columns_to_drop = set()  # Use a set to avoid duplicates\n",
    "    \n",
    "        # Check if the target column exists in the correlation matrix\n",
    "        if self.target_column not in corr_matrix.columns:\n",
    "            print(f\"Warning: {self.target_column} not found in the correlation matrix for the provided DataFrame.\")\n",
    "            self_target_corr_exists = False\n",
    "        else:\n",
    "            self_target_corr_exists = True\n",
    "    \n",
    "        # Iterate over the upper triangle of the correlation matrix\n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i + 1, len(corr_matrix.columns)):\n",
    "                col1 = corr_matrix.columns[i]\n",
    "                col2 = corr_matrix.columns[j]\n",
    "    \n",
    "                if col1 in columns_to_drop or col2 in columns_to_drop:\n",
    "                    continue  # Skip if already marked for removal\n",
    "    \n",
    "                # Check if the correlation exceeds the threshold\n",
    "                if abs(corr_matrix.iloc[i, j]) > self.correlation_threshold:\n",
    "                    if self_target_corr_exists:\n",
    "                        # Compare their correlation with the target column\n",
    "                        target_corr_col1 = corr_matrix[self.target_column].get(col1, 0)\n",
    "                        target_corr_col2 = corr_matrix[self.target_column].get(col2, 0)\n",
    "    \n",
    "                        if abs(target_corr_col1) < abs(target_corr_col2):\n",
    "                            columns_to_drop.add(col1)\n",
    "                        else:\n",
    "                            columns_to_drop.add(col2)\n",
    "                    else:\n",
    "                        # No target correlation available; arbitrarily drop one of the highly correlated columns\n",
    "                        columns_to_drop.add(col2)  # Or use a different logic if needed\n",
    "    \n",
    "        return list(columns_to_drop)\n",
    "\n",
    "    def plot_heatmap(self, corr_matrix, title):\n",
    "        \"\"\"Plots the heatmap for the given correlation matrix.\"\"\"\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "        plt.title(title)\n",
    "        plt.show()\n",
    "\n",
    "    def process_and_plot(self):\n",
    "        # Encode non-numeric columns in both train_tokens_df and feature_df (keeping original columns intact)\n",
    "        train_tokens_df_encoded = self.encode_non_numeric(self.train_tokens_df)\n",
    "        feature_df_encoded = self.encode_non_numeric(self.feature_df)\n",
    "\n",
    "        # Calculate the correlation matrices for the encoded versions\n",
    "        train_tokens_corr = self.calculate_correlation(train_tokens_df_encoded)\n",
    "        feature_corr = self.calculate_correlation(feature_df_encoded)\n",
    "\n",
    "        # Plot the heatmaps for both DataFrames\n",
    "        self.plot_heatmap(train_tokens_corr, \"Feature Correlation Matrix for train_tokens_df (Including Encoded Non-Numeric Features)\")\n",
    "        self.plot_heatmap(feature_corr, \"Feature Correlation Matrix for feature_df\")\n",
    "\n",
    "        # Remove multicollinearity from both DataFrames and get the columns to drop\n",
    "        dropped_train_tokens = self.remove_multicollinearity(self.train_tokens_df, train_tokens_corr)\n",
    "        dropped_feature = self.remove_multicollinearity(self.feature_df, feature_corr)\n",
    "\n",
    "        # Filter out columns from train_tokens_df that are dropped due to multicollinearity\n",
    "        final_train_tokens_df = self.train_tokens_df.drop(columns=dropped_train_tokens, errors='ignore')\n",
    "\n",
    "        # Filter out columns from feature_df that are dropped due to multicollinearity\n",
    "        final_feature_df = self.feature_df.drop(columns=dropped_feature, errors='ignore')\n",
    "\n",
    "        # Combine the two DataFrames into the final DataFrame with all valid columns\n",
    "        combined_df = pd.concat([final_train_tokens_df, final_feature_df], axis=1)\n",
    "\n",
    "        # Drop columns that contain only NaN values (i.e., columns with all None/NaN values)\n",
    "        combined_df = combined_df.dropna(axis=1, how='all')\n",
    "\n",
    "        # Display the results\n",
    "        print(f\"Dropped columns from train_tokens_df due to multicollinearity: {dropped_train_tokens}\")\n",
    "        print(f\"Dropped columns from feature_df due to multicollinearity: {dropped_feature}\")\n",
    "        print(\"Combined DataFrame after removing multicollinearity and NaN-only columns:\")\n",
    "\n",
    "        return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "b33350ba-e9a6-4f44-9e48-fceba5516916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessingPipeline:\n",
    "    def __init__(self, train_files, test_file=None, text_column='text', target_column='target'):\n",
    "        \"\"\"\n",
    "        Initialize the data processing pipeline with necessary components.\n",
    "\n",
    "        Parameters:\n",
    "        - train_files (list): List of training file paths (CSV).\n",
    "        - test_file (str or None): Path to the test file (CSV); defaults to None if not provided.\n",
    "        - text_column (str): The name of the column containing text data; defaults to 'text'.\n",
    "        - target_column (str): The name of the column containing target values; defaults to 'target'.\n",
    "        \"\"\"\n",
    "        self.text_column = text_column\n",
    "        self.target_column = target_column\n",
    "        self.data_handler = DataHandler(train_files, test_file, target_column)\n",
    "        \n",
    "        # Initialize TokenPipeline with both train_files and test_file (if provided)\n",
    "        self.token_pipeline = TokenPipeline(train_files=train_files, test_file=test_file, text_column=text_column)\n",
    "        \n",
    "        # Class-level variables for outputs\n",
    "        self.combined_data = None\n",
    "        self.X_train = None\n",
    "        self.X_val = None\n",
    "        self.y_train = None\n",
    "        self.y_val = None\n",
    "\n",
    "    def execute(self):\n",
    "        \"\"\"Execute the full data processing pipeline.\"\"\"\n",
    "        self.data_handler.load_data()  # Load data\n",
    "\n",
    "        # Validate loaded data\n",
    "        if self.data_handler.train_data is None:\n",
    "            raise ValueError(\"Training data is missing.\")\n",
    "\n",
    "        # Combine the training and test data if test data exists\n",
    "        if self.data_handler.test_data is not None:\n",
    "            self.combined_data = pd.concat(\n",
    "                [self.data_handler.train_data, self.data_handler.test_data], ignore_index=True\n",
    "            )\n",
    "        else:\n",
    "            self.combined_data = self.data_handler.train_data\n",
    "\n",
    "        # Handle NaN values in combined_data\n",
    "        self.handle_nan_values()\n",
    "\n",
    "        # Process tokens directly into combined_data\n",
    "        self.token_pipeline.process_and_move_columns()  # Updates combined_data in-place\n",
    "\n",
    "        # Perform train-test split on the combined data\n",
    "        self.split_combined_data()\n",
    "\n",
    "    def handle_nan_values(self):\n",
    "        \"\"\"Fill NaN values in the combined data.\"\"\"\n",
    "        if self.combined_data is not None:\n",
    "            for col in self.combined_data.columns:\n",
    "                if self.combined_data[col].dtype == 'object':\n",
    "                    self.combined_data[col] = self.combined_data[col].fillna('')\n",
    "                else:\n",
    "                    self.combined_data[col] = self.combined_data[col].fillna(0)\n",
    "\n",
    "    def split_combined_data(self, test_size=0.2, random_state=42):\n",
    "        \"\"\"Split the combined data into train and validation sets.\"\"\"\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        if self.combined_data is None:\n",
    "            raise ValueError(\"Combined data is not initialized.\")\n",
    "\n",
    "        text_data = self.combined_data[self.text_column]\n",
    "        target_data = self.combined_data[self.target_column]\n",
    "\n",
    "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
    "            text_data, target_data, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "    def get_tokenized_data(self, column: str = 'stage3') -> tuple:\n",
    "        \"\"\"\n",
    "        Extract tokenized text and target columns.\n",
    "    \n",
    "        Parameters:\n",
    "        - column (str): The name of the column containing tokens (default is 'stage3').\n",
    "    \n",
    "        Returns:\n",
    "        - tuple: (X_train_tokens, X_val_tokens, y_train, y_val)\n",
    "        \"\"\"\n",
    "        if self.combined_data is None:\n",
    "            raise ValueError(\"Combined data is not initialized.\")\n",
    "    \n",
    "        # Ensure the token column exists\n",
    "        if column not in self.combined_data.columns:\n",
    "            raise ValueError(f\"Token column '{column}' is missing in combined_data.\")\n",
    "    \n",
    "        # Extract tokenized text for train and validation splits\n",
    "        X_train_tokens = self.combined_data.loc[self.X_train.index, column]\n",
    "        X_val_tokens = self.combined_data.loc[self.X_val.index, column]\n",
    "    \n",
    "        # Convert tokenized lists to strings if necessary\n",
    "        X_train_tokens = X_train_tokens.apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "        X_val_tokens = X_val_tokens.apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n",
    "    \n",
    "        return X_train_tokens, X_val_tokens, self.y_train, self.y_val\n",
    "\n",
    "# Define the paths for training and test data\n",
    "train_files = ['train.csv']  # Replace with actual training data file paths\n",
    "test_file = 'test.csv'  # Replace with the actual test file path\n",
    "\n",
    "# Instantiate the DataProcessingPipeline class\n",
    "pipeline = DataProcessingPipeline(\n",
    "    train_files=train_files,\n",
    "    test_file=test_file,\n",
    "    text_column='text',      # Replace with the actual column name for text\n",
    "    target_column='target'   # Replace with the actual column name for target\n",
    ")\n",
    "\n",
    "# Execute the pipeline\n",
    "pipeline.execute()\n",
    "\n",
    "# Access processed data\n",
    "combined_data = pipeline.combined_data\n",
    "X_train, X_val, y_train, y_val = pipeline.get_tokenized_data(column='stage3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "c2794d27-b5a8-49e6-bb8a-e889b850a25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[branch f3181fb] Tokenization and data splitting stage completed\n",
      " Committer: Albert Aina <AlbertLeo1@Alberts-MacBook-Pro.local>\n",
      "Your name and email address were configured automatically based\n",
      "on your username and hostname. Please check that they are accurate.\n",
      "You can suppress this message by setting them explicitly. Run the\n",
      "following command and follow the instructions in your editor to edit\n",
      "your configuration file:\n",
      "\n",
      "    git config --global --edit\n",
      "\n",
      "After doing this, you may fix the identity used for this commit with:\n",
      "\n",
      "    git commit --amend --reset-author\n",
      "\n",
      " 2 files changed, 443 insertions(+), 617 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git commit -m \"Tokenization and data splitting stage completed\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0070d9-f4cf-4a48-907e-4f380f5bd0d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "The key fingerprint is:\n",
    "SHA256:WQRydCAw9QwmLLbROrbKLifaSSbufRFRTwzzvbbn39o albertohyna@gmail.com\n",
    "\n",
    "Agent pid 6970\n",
    "\n",
    "Identity added: /Users/AlbertLeo1/.ssh/id_ed25519 (albertohyna@gmail.com)\n",
    "\n",
    "AlbertLeo1@Alberts-MacBook-Pro HSEPY % cat ~/.ssh/id_ed25519.pub\n",
    "\n",
    "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJtInuOgnOVLeGdbkGO7ZiLMNK25OOaHhjKsE25CMRBx albertohyna@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea961b4d-c0f5-44a5-ac0c-d2cff1f1f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results for verification\n",
    "print(\"\\nTokenized Training Data:\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04f24b-b5a4-40eb-9431-68f1cb5d0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results for verification\n",
    "print(\"\\nTokenized Validation Data:\")\n",
    "X_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b49394-d7eb-474d-96fe-5cb9713df49e",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Topic Modelling</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f43e6d-5360-46cb-81ae-5827af31f5ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "class TopicModeling:\n",
    "    def __init__(self, data, stages=None):\n",
    "        \"\"\"\n",
    "        Initialize the TopicModeling class with a DataFrame and optional stages.\n",
    "\n",
    "        Parameters:\n",
    "        - data: The DataFrame containing tokenized text data for various stages.\n",
    "        - stages: A list of stages to process; defaults to an empty list if None.\n",
    "        \"\"\"\n",
    "        self.data = data  # Input DataFrame\n",
    "        self.stages = [f'stage{i}' for i in stages] if stages is not None else []  # Updated format\n",
    "        self.stage_data = {}  # Dictionary to hold tokenized data for each stage\n",
    "        self.coherence_scores = {}  # Dictionary to hold coherence scores for each stage\n",
    "        self.best_topics = {}  # Dictionary to hold best topics for each stage\n",
    "        self.best_stage = None\n",
    "        self.log_df = pd.DataFrame(columns=[\"Coherence Score\", \"Num Topics\", \"Topics\"])  # DataFrame to log results\n",
    "\n",
    "    def prepare_stage_data(self):\n",
    "        \"\"\"\n",
    "        Prepare tokenized data for each specified stage using the stage column.\n",
    "        \"\"\"\n",
    "        for stage in self.stages:\n",
    "            if stage not in self.data.columns:\n",
    "                print(f\"Column {stage} not found in data. Skipping stage {stage}.\")\n",
    "                continue\n",
    "            \n",
    "            # Extract tokenized data (assume already tokenized in list format)\n",
    "            tokenized_data = self.data[stage].dropna().tolist()\n",
    "            \n",
    "            # Ensure all entries are lists (tokenized format)\n",
    "            if all(isinstance(entry, list) for entry in tokenized_data):\n",
    "                self.stage_data[stage] = tokenized_data\n",
    "            else:\n",
    "                # Raise an error or handle improper tokenization\n",
    "                raise ValueError(f\"Data in column {stage} is not properly tokenized.\")\n",
    " \n",
    "    def prepare_corpus(self, tokenized_data):\n",
    "        \"\"\"\n",
    "        Prepare the corpus by vectorizing the tokenized data.\n",
    "\n",
    "        Parameters:\n",
    "        - tokenized_data: List of tokenized documents.\n",
    "\n",
    "        Returns:\n",
    "        - corpus: The vectorized document-term matrix.\n",
    "        - vectorizer: The CountVectorizer used for vectorization.\n",
    "        - gensim_dictionary: A Gensim dictionary mapping words to their IDs.\n",
    "        - gensim_corpus: A Gensim corpus of documents in bag-of-words format.\n",
    "        \"\"\"\n",
    "        vectorizer = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, lowercase=False, token_pattern=None)\n",
    "        corpus = vectorizer.fit_transform(tokenized_data)  # Vectorize using CountVectorizer\n",
    "        gensim_dictionary = corpora.Dictionary(tokenized_data)  # Create Gensim dictionary\n",
    "        gensim_corpus = [gensim_dictionary.doc2bow(text) for text in tokenized_data]  # Create bag-of-words corpus\n",
    "        return corpus, vectorizer, gensim_dictionary, gensim_corpus\n",
    "\n",
    "    def evaluate_coherence(self, gensim_corpus, gensim_dictionary, tokenized_data, stage, start=2, end=40):\n",
    "        \"\"\"\n",
    "        Evaluate coherence for a range of topic numbers using Gensim's LDA model.\n",
    "\n",
    "        Parameters:\n",
    "        - gensim_corpus: The Gensim corpus for the LDA model.\n",
    "        - gensim_dictionary: The Gensim dictionary for the corpus.\n",
    "        - tokenized_data: The original tokenized data.\n",
    "        - stage: The current stage being evaluated.\n",
    "        - start: The starting number of topics to evaluate (default is 2).\n",
    "        - end: The ending number of topics to evaluate (default is 20).\n",
    "\n",
    "        Returns:\n",
    "        - coherence_scores: A dictionary of coherence scores for each topic number.\n",
    "        - topics_by_num: A dictionary of extracted topics organized by number of topics.\n",
    "        \"\"\"\n",
    "        coherence_scores = {}\n",
    "        max_coherence_score = None\n",
    "        topics_by_num = {}\n",
    "    \n",
    "        for num_topics in range(start, end + 1):\n",
    "            # Create and train the LDA model\n",
    "            lda_model = LdaModel(corpus=gensim_corpus, id2word=gensim_dictionary, num_topics=num_topics, random_state=229)\n",
    "            coherence_model = CoherenceModel(model=lda_model, texts=tokenized_data, dictionary=gensim_dictionary, coherence='c_v')\n",
    "            coherence_score = coherence_model.get_coherence()  # Calculate coherence score\n",
    "            coherence_scores[num_topics] = coherence_score\n",
    "    \n",
    "            # Extract topics and clean the output using regex\n",
    "            topics = lda_model.print_topics(num_topics=num_topics, num_words=10)\n",
    "            cleaned_topics = [\n",
    "                re.sub(r'\\d+\\.\\d+\\*\"?(\\w+)\"?', r'\\1', topic[1]).replace(' + ', ' ')  # Remove probabilities and replace '+' with space\n",
    "                for topic in topics\n",
    "            ]\n",
    "            \n",
    "            # Add cleaned topics to the DataFrame\n",
    "            new_row = pd.DataFrame({\n",
    "                \"Coherence Score\": [coherence_score], \n",
    "                \"Num Topics\": [num_topics],\n",
    "                \"Topics\": [\" | \".join(cleaned_topics)]  # Join topics into a single string for readability\n",
    "            }, index=[f'{stage}_{num_topics}'])\n",
    "            \n",
    "            # Drop columns with all NaN values from new_row\n",
    "            new_row_cleaned = new_row.dropna(axis=1, how='all')  # Drop columns with all NaN values\n",
    "            self.log_df = pd.concat([self.log_df, new_row_cleaned], axis=0)\n",
    "\n",
    "            topics_by_num[num_topics] = cleaned_topics  # Store topics by number of topics\n",
    "    \n",
    "        return coherence_scores, topics_by_num\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run the process for each specified stage: prepare data, evaluate coherence,\n",
    "        and store the best topics.\n",
    "        \"\"\"\n",
    "        self.prepare_stage_data()  # Prepare tokenized data for all stages\n",
    "        \n",
    "        best_coherence_score = -float('inf')  # Initialize with a very low value\n",
    "        best_stage = None  # Variable to store the best stage\n",
    "        \n",
    "        for stage, tokenized_data in self.stage_data.items():\n",
    "            # Prepare Gensim corpus and dictionary from tokenized data\n",
    "            _, _, gensim_dictionary, gensim_corpus = self.prepare_corpus(tokenized_data)\n",
    "            # Evaluate coherence scores and extract topics\n",
    "            stage_coherence_scores, topics_by_num = self.evaluate_coherence(gensim_corpus, gensim_dictionary, tokenized_data, stage)\n",
    "            \n",
    "            # Store coherence scores for the stage\n",
    "            self.coherence_scores[stage] = stage_coherence_scores\n",
    "            \n",
    "            # Find the best number of topics based on the coherence scores\n",
    "            best_num_topics = max(stage_coherence_scores, key=stage_coherence_scores.get)\n",
    "            \n",
    "            # Store only the best topics associated with the best number of topics\n",
    "            self.best_topics[stage] = topics_by_num[best_num_topics]\n",
    "            \n",
    "            # Check if the current stage has the best coherence score\n",
    "            if stage_coherence_scores[best_num_topics] > best_coherence_score:\n",
    "                best_coherence_score = stage_coherence_scores[best_num_topics]\n",
    "                best_stage = stage  # Update best stage\n",
    "        \n",
    "            print(f\"Best coherence score for {stage}: {stage_coherence_scores[best_num_topics]} with {best_num_topics} topics.\")\n",
    "        \n",
    "        # Store the stage with the best coherence score\n",
    "        self.best_stage = best_stage\n",
    "        self.best_coherence_score = best_coherence_score\n",
    "    \n",
    "        print(f\"Best stage overall: {self.best_stage} with a coherence score of {self.best_coherence_score}\")\n",
    "\n",
    "\n",
    "    def plot_coherence(self):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for stage, scores in self.coherence_scores.items():\n",
    "            # Plot the line\n",
    "            plt.plot(scores.keys(), scores.values(), label=f\"Stage: {stage}\")\n",
    "            # Mark each point with \"o\"\n",
    "            plt.scatter(scores.keys(), scores.values(), marker='o')\n",
    "        \n",
    "        plt.xlabel(\"Number of Topics\")\n",
    "        plt.ylabel(\"Coherence Score\")\n",
    "        plt.title(\"Coherence Scores by Number of Topics\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_word_cloud(self, stage, num_topics=None, num_words=10):\n",
    "        \"\"\"\n",
    "        Generate and display a word cloud for topics in a specified stage.\n",
    "        \n",
    "        Args:\n",
    "            stage (int): The stage number to fetch topics for (e.g., 3).\n",
    "            num_topics (int, optional): The specific topic to display. If None, all topics are displayed.\n",
    "            num_words (int): The number of top words to include in the word cloud.\n",
    "        \"\"\"\n",
    "        # Convert the stage to the correct format, e.g., 'stage_3'\n",
    "        stage_key = f\"stage_{stage}\"\n",
    "        \n",
    "        # Fetch topics for the stage from self.best_topics\n",
    "        if stage_key not in self.best_topics:\n",
    "            print(f\"No topics found for {stage_key}.\")\n",
    "            return\n",
    "        \n",
    "        topics_by_num = self.best_topics[stage_key]\n",
    "        \n",
    "        # Handle topics_by_num as a list or dictionary\n",
    "        if isinstance(topics_by_num, list):\n",
    "            # Assuming it's a list of topics (list of tuples with words)\n",
    "            if num_topics is not None:\n",
    "                topics_by_num = [t for t in topics_by_num if t[0] == num_topics]\n",
    "            topics_by_num = dict(topics_by_num)  # Convert to dictionary for consistent processing\n",
    "        \n",
    "        # Ensure topics_by_num is now a dictionary\n",
    "        if not isinstance(topics_by_num, dict):\n",
    "            raise TypeError(f\"Expected topics_by_num to be a dictionary, got {type(topics_by_num)}\")\n",
    "        \n",
    "        # Generate and plot word clouds\n",
    "        for topic_num, words in topics_by_num.items():\n",
    "            # Limit the words to num_words\n",
    "            top_words = words[:num_words]\n",
    "            word_freq = {word: freq for word, freq in top_words}\n",
    "            \n",
    "            # Generate the word cloud\n",
    "            wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(word_freq)\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"Word Cloud for Stage {stage}, Topic {topic_num}\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "# Example usage:\n",
    "topic_modeling = TopicModeling(train_tokens_df, stages=[2, 3, 4, 5])\n",
    "\n",
    "# Run the topic modeling process\n",
    "topic_modeling.run()\n",
    "\n",
    "# Display the log DataFrame with coherence scores and topics\n",
    "display(topic_modeling.log_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ec556f-c0cc-41e2-970d-9be5af375258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf HSEPY/\n",
    "#!git clone https://github.com/AlbertLeo1/HSEPY.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621b737-00ca-4d2a-9e6c-e8fbed9c3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stage_best_topics = topic_modeling.best_topics\n",
    "best_stage = topic_modeling.best_stage \n",
    "best_topic = all_stage_best_topics[best_stage]\n",
    "all_stage_best_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c361a41-0d60-4033-a33d-e58df76e5acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b22939-a683-4a48-ac4c-1cef9db98b64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topic_modeling.plot_coherence()\n",
    "topic_modeling.plot_word_cloud(stage = 5, num_topics=18, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a8737-acce-42d6-9009-9bbf06217370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1fb24b-5301-48e6-bdda-ccce64be5444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d2a38-1dba-48f4-9449-3bf7a77b2d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05b68b6f",
   "metadata": {},
   "source": [
    "#### 1. Create topic models using LDA from sklearn varying number of topics from 2 to 20 and choose the best one with coherence score. Plot words with highest probabilities with word cloud for the best model and for couple other models which number of topics differs from optimal sunstantially (e.g. if you find out that 8 topics is the best option you need to plot wordcloud for 2,8,16,20 topics just to see if coherence really allows you to find best the split (it might not)). !Remember that topic modelling works the best on cleaned dataset (delete stopwords, lemmatize, try to delete too freequent/too rare words etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1143ecc-ebe3-4017-b6c0-00b68599e57a",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Topic Modelling\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c624fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel, CoherenceModel\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "class TopicModeling:\n",
    "    def __init__(self, pipeline, stages=None):\n",
    "        \"\"\"\n",
    "        Initialize the TopicModeling class with a pipeline and optional stages.\n",
    "\n",
    "        Parameters:\n",
    "        - pipeline: The data processing pipeline used to obtain tokenized data.\n",
    "        - stages: A list of stages to process; defaults to an empty list if None.\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.stages = [f'stage_{i}' for i in stages] if stages is not None else []\n",
    "        self.stage_data = {}  # Dictionary to hold tokenized data for each stage\n",
    "        self.coherence_scores = {}  # Dictionary to hold coherence scores for each stage\n",
    "        self.best_topics = {}  # Dictionary to hold best topics for each stage\n",
    "        self.log_df = pd.DataFrame(columns=[\"Coherence Score\", \"Num Topics\", \"Topics\"])  # DataFrame to log results\n",
    "\n",
    "    def prepare_stage_data(self):\n",
    "        \"\"\"\n",
    "        Prepare tokenized data for each specified stage by combining training and validation datasets.\n",
    "        \"\"\"\n",
    "        for stage in self.stages:\n",
    "            # Get tokenized data for the stage\n",
    "            X_train, X_val, _, _, _ = self.pipeline.get_tokenized_data(stage)\n",
    "            combined_data = pd.concat([X_train, X_val]).reset_index(drop=True)\n",
    "            # Tokenize the combined data and convert to a list of lists\n",
    "            self.stage_data[stage] = combined_data.apply(lambda x: x.split()).tolist()\n",
    "\n",
    "    def prepare_corpus(self, tokenized_data):\n",
    "        \"\"\"\n",
    "        Prepare the corpus by vectorizing the tokenized data.\n",
    "\n",
    "        Parameters:\n",
    "        - tokenized_data: List of tokenized documents.\n",
    "\n",
    "        Returns:\n",
    "        - corpus: The vectorized document-term matrix.\n",
    "        - vectorizer: The CountVectorizer used for vectorization.\n",
    "        - gensim_dictionary: A Gensim dictionary mapping words to their IDs.\n",
    "        - gensim_corpus: A Gensim corpus of documents in bag-of-words format.\n",
    "        \"\"\"\n",
    "        vectorizer = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, lowercase=False, token_pattern=None)\n",
    "        corpus = vectorizer.fit_transform(tokenized_data)  # Vectorize using CountVectorizer\n",
    "        gensim_dictionary = corpora.Dictionary(tokenized_data)  # Create Gensim dictionary\n",
    "        gensim_corpus = [gensim_dictionary.doc2bow(text) for text in tokenized_data]  # Create bag-of-words corpus\n",
    "        return corpus, vectorizer, gensim_dictionary, gensim_corpus\n",
    "\n",
    "    def evaluate_coherence(self, gensim_corpus, gensim_dictionary, tokenized_data, stage, start=2, end=25):\n",
    "        \"\"\"\n",
    "        Evaluate coherence for a range of topic numbers using Gensim's LDA model.\n",
    "\n",
    "        Parameters:\n",
    "        - gensim_corpus: The Gensim corpus for the LDA model.\n",
    "        - gensim_dictionary: The Gensim dictionary for the corpus.\n",
    "        - tokenized_data: The original tokenized data.\n",
    "        - stage: The current stage being evaluated.\n",
    "        - start: The starting number of topics to evaluate (default is 2).\n",
    "        - end: The ending number of topics to evaluate (default is 25).\n",
    "\n",
    "        Returns:\n",
    "        - coherence_scores: A dictionary of coherence scores for each topic number.\n",
    "        - topics_by_num: A dictionary of extracted topics organized by number of topics.\n",
    "        \"\"\"\n",
    "        coherence_scores = {}\n",
    "        topics_by_num = {}\n",
    "    \n",
    "        for num_topics in range(start, end + 1):\n",
    "            # Create and train the LDA model\n",
    "            lda_model = LdaModel(corpus=gensim_corpus, id2word=gensim_dictionary, num_topics=num_topics, random_state=229)\n",
    "            coherence_model = CoherenceModel(model=lda_model, texts=tokenized_data, dictionary=gensim_dictionary, coherence='c_v')\n",
    "            coherence_score = coherence_model.get_coherence()  # Calculate coherence score\n",
    "            coherence_scores[num_topics] = coherence_score\n",
    "    \n",
    "            # Extract topics and clean the output using regex\n",
    "            topics = lda_model.print_topics(num_topics=num_topics, num_words=10)\n",
    "            cleaned_topics = [\n",
    "                re.sub(r'\\d+\\.\\d+\\*\"?(\\w+)\"?', r'\\1', topic[1]).replace(' + ', ' ')  # Remove probabilities and replace '+' with space\n",
    "                for topic in topics\n",
    "            ]\n",
    "            \n",
    "            # Add cleaned topics to the DataFrame\n",
    "            new_row = pd.DataFrame({\n",
    "                \"Coherence Score\": [coherence_score], \n",
    "                \"Num Topics\": [num_topics],\n",
    "                \"Topics\": [\" | \".join(cleaned_topics)]  # Join topics into a single string for readability\n",
    "            }, index=[f'{stage}_{num_topics}'])\n",
    "            \n",
    "            self.log_df = pd.concat([self.log_df, new_row], axis=0)  # Append new row to the log DataFrame\n",
    "    \n",
    "            topics_by_num[num_topics] = cleaned_topics  # Store topics by number of topics\n",
    "    \n",
    "        return coherence_scores, topics_by_num\n",
    "\n",
    "    def plot_coherence_scores(self, coherence_scores, stage):\n",
    "        \"\"\"\n",
    "        Plot the coherence scores for the given stage.\n",
    "\n",
    "        Parameters:\n",
    "        - coherence_scores: A dictionary of coherence scores for each topic number.\n",
    "        - stage: The current stage being evaluated.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(coherence_scores.keys(), coherence_scores.values(), marker='o')\n",
    "        plt.title(f'Coherence Scores for {stage}')  # Set plot title\n",
    "        plt.xlabel('Number of Topics')  # X-axis label\n",
    "        plt.ylabel('Coherence Score')  # Y-axis label\n",
    "        plt.xticks(list(coherence_scores.keys()))  # Set x-ticks\n",
    "        plt.grid()  # Add grid to the plot\n",
    "        plt.show()  # Display the plot\n",
    "\n",
    "    def plot_wordcloud(self, topic_words, title):\n",
    "        \"\"\"\n",
    "        Plot a word cloud for the given topic words.\n",
    "\n",
    "        Parameters:\n",
    "        - topic_words: A list of words to visualize in the word cloud.\n",
    "        - title: The title for the word cloud plot.\n",
    "        \"\"\"\n",
    "        print('\\n')  # Print a newline for formatting\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(topic_words))  # Generate word cloud\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')  # Display word cloud\n",
    "        plt.title(title)  # Set plot title\n",
    "        plt.axis('off')  # Turn off axis\n",
    "        plt.show()  # Display the plot\n",
    "        print('\\n')  # Print a newline for formatting\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Run the process for each specified stage: prepare data, evaluate coherence,\n",
    "        and plot results.\n",
    "        \"\"\"\n",
    "        self.prepare_stage_data()  # Prepare tokenized data for all stages\n",
    "\n",
    "        for stage, tokenized_data in self.stage_data.items():\n",
    "            # Prepare Gensim corpus and dictionary from tokenized data\n",
    "            _, _, gensim_dictionary, gensim_corpus = self.prepare_corpus(tokenized_data)\n",
    "            # Evaluate coherence scores and extract topics\n",
    "            stage_coherence_scores, topics_by_num = self.evaluate_coherence(gensim_corpus, gensim_dictionary, tokenized_data, stage)\n",
    "\n",
    "            self.coherence_scores[stage] = stage_coherence_scores  # Store coherence scores for the stage\n",
    "\n",
    "            # Find the best number of topics based on coherence scores\n",
    "            best_num_topics = max(stage_coherence_scores, key=stage_coherence_scores.get)\n",
    "            self.best_topics[stage] = topics_by_num  # Store best topics for the stage\n",
    "            print(f\"Best coherence score for {stage}: {stage_coherence_scores[best_num_topics]} with {best_num_topics} topics.\")\n",
    "            \n",
    "            # Call the plotting method to visualize coherence scores\n",
    "            self.plot_coherence_scores(stage_coherence_scores, stage)  # Plot the coherence scores\n",
    "\n",
    "    def plot_cloud(self, stage, topics):\n",
    "        \"\"\"\n",
    "        Plot word clouds for specific topic combinations from the best topics in a stage.\n",
    "\n",
    "        Parameters:\n",
    "        - stage: The current stage to visualize.\n",
    "        - topics: A list of the number of topics to plot word clouds for.\n",
    "        \"\"\"\n",
    "        if stage not in self.best_topics:\n",
    "            print(f\"Stage {stage} not found in best topics. Available stages: {self.best_topics.keys()}. Skipping...\")\n",
    "            return\n",
    "        \n",
    "        best_stage_topics = self.best_topics[stage]  # Retrieve best topics for the stage\n",
    "\n",
    "        for num_topics in topics:\n",
    "            if num_topics in best_stage_topics:\n",
    "                topic_words = best_stage_topics[num_topics]  # Get words for the specified number of topics\n",
    "                for idx, words in enumerate(topic_words):\n",
    "                    # Plot word cloud for each topic\n",
    "                    self.plot_wordcloud(words.split(), title=f\"Word Cloud for {stage} - {num_topics} Topics (Topic {idx})\")\n",
    "            else:\n",
    "                print(f\"Number of topics {num_topics} not found in best topics for {stage}\")\n",
    "\n",
    "\n",
    "pipeline = pipeline  \n",
    "topic_modeling = TopicModeling(pipeline, stages=[5])  # Testing multiple stages will display the best (stage 5 is the best)\n",
    "topic_modeling.run()  # Run the topic modeling process\n",
    "display(topic_modeling.log_df)  # Display the log DataFrame with coherence scores and topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f00a2a3-fbd2-4eca-ab2b-0afc972d4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeling.plot_cloud('stage_5', list(range(2, 21)))#specify the range of topics to explore from 2 to 21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713be540-fe43-4590-8bc3-51e61ac3944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_modeling.plot_cloud('stage_5', [2, 6, 8, 16, 20])#splot specific topics combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bae32c",
   "metadata": {},
   "source": [
    "#### <em>2. Using torch create and train neural network to predict target class (use train set to calculate loss and update weights. validation set to calculate f1 average score after every epoch and test set for final testing) Experiment with size of layers, number of layers, activation functions If you encounter OOM errors reduxe the number of layers/neurons.</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9516073",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891b78d-e8e7-4a51-889a-100e7bdc27bf",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Train NN\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ab3262-8880-43f7-b9cc-5d08a0177f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "class Vectorizer:\n",
    "    def __init__(self, method='tfidf', ngram_range=(1, 2), max_features=4000, \n",
    "                 data_type='tokenized', embedding_file_paths=None):\n",
    "        \"\"\"\n",
    "        Initializes the Vectorizer with specified parameters.\n",
    "\n",
    "        Args:\n",
    "            method (str): The vectorization method to use ('tfidf' or 'count').\n",
    "            ngram_range (tuple): The range of n-grams to consider.\n",
    "            max_features (int): The maximum number of features to extract.\n",
    "            data_type (str): The type of data being processed ('tokenized' or 'embedding').\n",
    "            embedding_file_paths (list or None): List of file paths for embedding files.\n",
    "        \"\"\"\n",
    "        self.method = method\n",
    "        self.ngram_range = ngram_range\n",
    "        self.max_features = max_features\n",
    "        self.vectorization = None\n",
    "        self.data_type = data_type\n",
    "        self.embedding_file_paths = embedding_file_paths if embedding_file_paths is not None else []\n",
    "\n",
    "    def fit_transform(self, X_train, X_val, y_train=None, y_val=None):\n",
    "        \"\"\"\n",
    "        Fit the vectorizer on the training data and transform both training and validation data.\n",
    "\n",
    "        Args:\n",
    "            X_train (list): Training data.\n",
    "            X_val (list): Validation data.\n",
    "            y_train (list or None): Target labels for training data (optional).\n",
    "            y_val (list or None): Target labels for validation data (optional).\n",
    "\n",
    "        Returns:\n",
    "            tuple: Transformed training and validation data along with their labels if provided.\n",
    "        \"\"\"\n",
    "        # Choose vectorization method\n",
    "        if self.method == 'tfidf':\n",
    "            self.vectorization = TfidfVectorizer(ngram_range=self.ngram_range, max_features=self.max_features)\n",
    "        elif self.method == 'count':\n",
    "            self.vectorization = CountVectorizer(ngram_range=self.ngram_range, max_features=self.max_features)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported method. Use 'tfidf' or 'count'.\")\n",
    "\n",
    "        # Fit on the training data\n",
    "        X_train_vec = self.vectorization.fit_transform(X_train).toarray()\n",
    "        X_val_vec = self.vectorization.transform(X_val).toarray()\n",
    "\n",
    "        # Convert target labels to NumPy arrays if provided\n",
    "        y_train_vec = np.array(y_train) if y_train is not None else None\n",
    "        y_val_vec = np.array(y_val) if y_val is not None else None\n",
    "        \n",
    "        # Return transformed data based on data type\n",
    "        if self.data_type == 'tokenized':\n",
    "            return X_train_vec, X_val_vec, y_train_vec, y_val_vec\n",
    "        else:\n",
    "            print(\"Calling a wrong method for your data?\")\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform the test data using the fitted vectorizer.\n",
    "\n",
    "        Args:\n",
    "            X (list): Test data to be transformed.\n",
    "\n",
    "        Returns:\n",
    "            array: Transformed test data.\n",
    "        \"\"\"\n",
    "        if self.data_type == 'tokenized':\n",
    "            return self.vectorization.transform(X).toarray()\n",
    "        else:\n",
    "            print(\"Calling a wrong method for your data?\")\n",
    "\n",
    "    def load_embeddings(self):\n",
    "        \"\"\"\n",
    "        Loads embeddings from one or more .bin files.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Combined embedding matrix and word indices if data type is 'embedding'.\n",
    "        \"\"\"\n",
    "        embedding_matrices = []\n",
    "        word_indices = {}\n",
    "\n",
    "        # Check if embedding_file_paths is a single string or a list\n",
    "        if isinstance(self.embedding_file_paths, str):\n",
    "            self.embedding_file_paths = [self.embedding_file_paths]\n",
    "\n",
    "        for file_path in self.embedding_file_paths:\n",
    "            # Load word embeddings from the specified file\n",
    "            model = KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "            \n",
    "            # Get the vocabulary size and embedding dimension\n",
    "            vocab_size = len(model.key_to_index)\n",
    "            embedding_dimension = model.vector_size\n",
    "            embedding_matrix = np.zeros((vocab_size, embedding_dimension))\n",
    "\n",
    "            # Create a word index mapping\n",
    "            word_index = {word: i for i, word in enumerate(model.key_to_index)}\n",
    "            for word, i in word_index.items():\n",
    "                embedding_matrix[i] = model[word]\n",
    "\n",
    "            # Append the current embedding matrix to the list\n",
    "            embedding_matrices.append(embedding_matrix)\n",
    "\n",
    "            # Update the global word index if necessary\n",
    "            word_indices.update(word_index)\n",
    "\n",
    "        # Concatenate all embedding matrices into one\n",
    "        combined_embedding_matrix = np.vstack(embedding_matrices)\n",
    "\n",
    "        # Return combined embeddings and word indices based on data type\n",
    "        if self.data_type == 'embedding':\n",
    "            return combined_embedding_matrix, word_indices\n",
    "        else:\n",
    "            print(\"Calling a wrong method for your data?\")\n",
    "\n",
    "    def texts_to_indices(self, texts, word_index):\n",
    "        \"\"\"\n",
    "        Converts texts into lists of indices based on the provided word index.\n",
    "\n",
    "        Args:\n",
    "            texts (list): List of texts to convert.\n",
    "            word_index (dict): Dictionary mapping words to indices.\n",
    "\n",
    "        Returns:\n",
    "            list: List of lists containing indices for each text.\n",
    "        \"\"\"\n",
    "        indices_list = []\n",
    "        for text in texts:\n",
    "            # Convert each word in the text to its corresponding index\n",
    "            indices = [word_index[word] for word in text.split() if word in word_index]\n",
    "            indices_list.append(indices)\n",
    "\n",
    "        # Return indices based on data type\n",
    "        if self.data_type == 'embedding':\n",
    "            return indices_list\n",
    "        else:\n",
    "            print(\"Calling a wrong method for your data?\")\n",
    "\n",
    "    def get_embedding_data(self, embedding_matrix, texts, word_index):\n",
    "        \"\"\"\n",
    "        Returns embeddings for the given texts based on the provided embedding matrix.\n",
    "\n",
    "        Args:\n",
    "            embedding_matrix (array): Matrix containing word embeddings.\n",
    "            texts (list): List of texts to get embeddings for.\n",
    "            word_index (dict): Dictionary mapping words to indices.\n",
    "\n",
    "        Returns:\n",
    "            array: Array of embeddings for the input texts.\n",
    "        \"\"\"\n",
    "        # Convert texts to indices\n",
    "        indices_list = self.texts_to_indices(texts, word_index)\n",
    "        \n",
    "        # Initialize embeddings array\n",
    "        embeddings = np.zeros((len(texts), embedding_matrix.shape[1]))\n",
    "\n",
    "        for i, index_list in enumerate(indices_list):\n",
    "            if index_list:\n",
    "                # Calculate the mean embedding for the indices in the list\n",
    "                embeddings[i] = np.mean(embedding_matrix[index_list], axis=0)\n",
    "\n",
    "        # Return embeddings based on data type\n",
    "        if self.data_type == 'embedding':\n",
    "            return embeddings\n",
    "        else:\n",
    "            print(\"Calling a wrong method for your data?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d2a8e-9360-40a3-ae4d-91dbb70ed6c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    \"\"\"Custom dataset for handling text data.\"\"\"\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long) if y is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx], self.y[idx]) if self.y is not None else self.X[idx]\n",
    "\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    \"\"\"Multilayer Perceptron model for text classification.\n",
    "\n",
    "    Args:\n",
    "        stages (list): List of stages for processing.\n",
    "        vectorizer: Vectorizer instance for text data.\n",
    "        num_classes (int, optional): Number of output classes. Defaults to None.\n",
    "        hidden_size (int or list, optional): Size of hidden layers. Defaults to 128.\n",
    "        num_layers (int, optional): Number of hidden layers. Defaults to 2.\n",
    "        activation_fn (str, optional): Activation function name. Defaults to 'relu'.\n",
    "        learning_rate (float, optional): Learning rate for optimizer. Defaults to 0.00005.\n",
    "        batch_size (int, optional): Batch size for training. Defaults to 32.\n",
    "        patience (int, optional): Early stopping patience. Defaults to 10.\n",
    "        num_epochs (int, optional): Number of epochs for training. Defaults to 50.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, stages, vectorizer, num_classes=None, hidden_size=[128, 64], num_layers=2, \n",
    "                 activation_fn='relu', learning_rate=0.00007, batch_size=64, \n",
    "                 patience=10, num_epochs=50):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.stages = stages  # Store the stages for processing\n",
    "        self.vectorizer = vectorizer  # Assign the vectorizer for data transformation\n",
    "        self.embedding_file_paths = vectorizer.embedding_file_paths  # Retrieve embedding file paths from vectorizer\n",
    "        self.hidden_size = hidden_size if isinstance(hidden_size, list) else [hidden_size]  # Set hidden size as a list\n",
    "        self.layer = []  # Initialize list to store layers\n",
    "        self.best_f1_score = 0  # Initialize the best F1 score\n",
    "        \n",
    "        # Initialize a DataFrame to collect all epoch data across stages\n",
    "        self.all_epoch_data = pd.DataFrame(columns=[\"Stage\", \"Epoch\", \"Loss\", \"F1_macro\", \"Prediction\"])\n",
    "    \n",
    "        for stage in self.stages: \n",
    "            print(f\"Processing Stage: {stage}\")  # Print the current stage being processed\n",
    "            tokenized_data = pipeline.get_tokenized_data(f'stage_{stage}')  # Retrieve tokenized data for the current stage\n",
    "            self.X_train, self.X_val, self.y_train, self.y_val, self.test_data = tokenized_data  # Unpack tokenized data\n",
    "            self.num_classes = num_classes  # Set number of classes\n",
    "            \n",
    "            # If num_classes is not provided, infer from training data\n",
    "            if num_classes is None:\n",
    "                self.num_classes = len(set(self.y_train))  # Calculate the number of unique classes from training labels\n",
    "                \n",
    "            self.batch_size = batch_size  # Set batch size for training\n",
    "            \n",
    "            # Calculate input size based on data type\n",
    "            if self.vectorizer.data_type == 'embedding':\n",
    "                self.input_size = self.prepare_embedding_data()  # Prepare input size for embedding data\n",
    "                    \n",
    "            elif self.vectorizer.data_type == 'tokenized':\n",
    "                self.input_size = self.prepare_tokenized_data()  # Prepare input size for tokenized data\n",
    "                \n",
    "            else:\n",
    "                print(\"Error: Unsupported data type. Please use 'embedding' or 'tokenized'.\")  # Error message for unsupported data types\n",
    "                raise ValueError(\"Unsupported data type: {}\".format(self.vectorizer.data_type))\n",
    "    \n",
    "            self.class_weights = self.calculate_class_weights(self.y_train)  # Calculate class weights for handling class imbalance\n",
    "            \n",
    "            # Define the activation function based on user input\n",
    "            self.activation_function = self.get_activation_function(activation_fn)\n",
    "            \n",
    "            # Define the MLP structure with dynamic input size and output layer based on num_classes\n",
    "            self.network = self.build_network()\n",
    "            self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set device for computation\n",
    "            self.network.to(self.device)  # Move the model to the specified device\n",
    "            \n",
    "            # Set the appropriate loss function based on number of classes\n",
    "            if self.num_classes == 1:\n",
    "                self.criterion = nn.BCEWithLogitsLoss()  # Use binary cross-entropy loss for binary classification\n",
    "            else:\n",
    "                self.criterion = nn.CrossEntropyLoss(weight=self.class_weights)  # Use cross-entropy loss for multi-class classification\n",
    "            \n",
    "            # Layers and training components\n",
    "            self.layers = list(self.network.children())  # Extract the layers from the network\n",
    "            self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)  # Set up the Adam optimizer\n",
    "            self.scheduler = lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.1, patience=2)  # Learning rate scheduler\n",
    "            \n",
    "            # Set up training parameters\n",
    "            self.patience = patience  # Set the patience for early stopping\n",
    "            self.num_epochs = num_epochs  # Set total epochs for training\n",
    "            self.epoch_data = pd.DataFrame(columns=[\"Epoch\", \"Loss\", \"F1_macro\", \"Prediction\"])  # DataFrame to collect epoch data\n",
    "            self.all_epoch_data = pd.concat([self.all_epoch_data, self.epoch_data], ignore_index=True)  # Concatenate epoch data across stages\n",
    "\n",
    "    \n",
    "    def prepare_tokenized_data(self):\n",
    "        \"\"\"Prepare data for both binary and multiclass classification using the vectorizer.\n",
    "    \n",
    "        This method transforms the training and validation datasets into tokenized formats \n",
    "        suitable for classification tasks. It also prepares data loaders for training, \n",
    "        validation, and optionally testing.\n",
    "    \n",
    "        Returns:\n",
    "            int: The input size for building the network based on the vectorized dimension.\n",
    "                  Returns None if an error occurs during preparation.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If vectorization results in None or if the data does not conform\n",
    "                        to expected dimensions.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize y_train_vec and y_val_vec to None\n",
    "            y_train_vec, y_val_vec = None, None\n",
    "    \n",
    "            # Assuming the vectorizer is handling tokenized data\n",
    "            # Transform the data using the vectorizer\n",
    "            X_train_vec, X_val_vec, y_train_vec, y_val_vec = self.vectorizer.fit_transform(self.X_train, self.X_val, self.y_train, self.y_val)\n",
    "    \n",
    "            # Check that vectorization results are valid\n",
    "            if X_train_vec is None or X_val_vec is None:\n",
    "                raise ValueError(\"Vectorization resulted in None for training or validation data.\")\n",
    "    \n",
    "            # Ensure the resulting arrays are 2D\n",
    "            if len(X_train_vec.shape) != 2 or len(X_val_vec.shape) != 2:\n",
    "                raise ValueError(\"Vectorized data must be 2D arrays.\")\n",
    "    \n",
    "            # Handle labels\n",
    "            y_train_vec = np.ravel(y_train_vec) if y_train_vec is not None else np.ravel(self.y_train)\n",
    "            y_val_vec = np.ravel(y_val_vec) if y_val_vec is not None else np.ravel(self.y_val)\n",
    "    \n",
    "            # Check unique classes for binary vs multiclass classification\n",
    "            unique_classes = np.unique(self.y_train)\n",
    "            if len(unique_classes) > 2:\n",
    "                # Multiclass: one-hot encode labels\n",
    "                lb = LabelBinarizer()\n",
    "                y_train_vec = lb.fit_transform(y_train_vec)\n",
    "                y_val_vec = lb.transform(y_val_vec)\n",
    "    \n",
    "            # Automatically set input size\n",
    "            input_size = X_train_vec.shape[1]\n",
    "            assert X_train_vec.shape[0] == len(y_train_vec), \"Mismatch in X_train and y_train lengths\"\n",
    "            assert X_val_vec.shape[0] == len(y_val_vec), \"Mismatch in X_val and y_val lengths\"\n",
    "    \n",
    "            # Prepare data loaders for training and validation\n",
    "            self.train_loader = DataLoader(CustomTextDataset(X_train_vec, y_train_vec), batch_size=self.batch_size, shuffle=True)\n",
    "            self.val_loader = DataLoader(CustomTextDataset(X_val_vec, y_val_vec), batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "            # Prepare test data if applicable\n",
    "            if hasattr(self, 'test_data') and self.test_data is not None:\n",
    "                self.test_data_vec = self.vectorizer.transform(self.test_data)\n",
    "                self.test_loader = DataLoader(CustomTextDataset(self.test_data_vec), batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "            return input_size  # Return the input size for building the network\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred in prepare_tokenized_data: {str(e)}\")\n",
    "            return None  # Ensure a return value even on failure\n",
    "\n",
    "\n",
    "    def prepare_embedding_data(self):\n",
    "        \"\"\"Prepare data for classification using only embeddings.\n",
    "    \n",
    "        This method loads embeddings from a specified file, processes training and validation\n",
    "        datasets to obtain embedding vectors, and prepares data loaders for training,\n",
    "        validation, and optionally testing.\n",
    "    \n",
    "        Returns:\n",
    "            int: The input size for building the network based on the embedding dimension.\n",
    "                  Returns None if an error occurs during preparation.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If the embedding file path is not specified or if the data does not\n",
    "                        conform to expected dimensions.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load embeddings from the specified file\n",
    "            if self.embedding_file_paths is not None:\n",
    "                embedding_matrix, word_index = self.vectorizer.load_embeddings()\n",
    "            else:\n",
    "                raise ValueError(\"Embedding file path must be specified for embedding data type.\")\n",
    "    \n",
    "            # Use the vectorizer to get the embeddings for X_train and X_val\n",
    "            X_train_vec = self.vectorizer.get_embedding_data(embedding_matrix, self.X_train, word_index)\n",
    "            X_val_vec = self.vectorizer.get_embedding_data(embedding_matrix, self.X_val, word_index)\n",
    "    \n",
    "            # Ensure embeddings are in 2D array format\n",
    "            if len(X_train_vec.shape) != 2 or len(X_val_vec.shape) != 2:\n",
    "                raise ValueError(\"Embedding data must be 2D arrays.\")\n",
    "    \n",
    "            # Handle labels (assumed to be present as y_train, y_val)\n",
    "            y_train_vec = np.ravel(self.y_train) if self.y_train is not None else None\n",
    "            y_val_vec = np.ravel(self.y_val) if self.y_val is not None else None\n",
    "    \n",
    "            # Check unique classes for binary vs multiclass classification\n",
    "            unique_classes = np.unique(y_train_vec)\n",
    "            if len(unique_classes) > 2:\n",
    "                # Multiclass: one-hot encode labels\n",
    "                lb = LabelBinarizer()\n",
    "                y_train_vec = lb.fit_transform(y_train_vec)\n",
    "                y_val_vec = lb.transform(y_val_vec)\n",
    "    \n",
    "            # Automatically set input size\n",
    "            input_size = X_train_vec.shape[1]  \n",
    "            assert X_train_vec.shape[0] == len(y_train_vec), \"Mismatch in X_train and y_train lengths\"\n",
    "            assert X_val_vec.shape[0] == len(y_val_vec), \"Mismatch in X_val and y_val lengths\"\n",
    "    \n",
    "            # Prepare data loaders for training and validation\n",
    "            self.train_loader = DataLoader(CustomTextDataset(X_train_vec, y_train_vec), batch_size=self.batch_size, shuffle=True)\n",
    "            self.val_loader = DataLoader(CustomTextDataset(X_val_vec, y_val_vec), batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "            # Prepare test data if applicable\n",
    "            if hasattr(self, 'test_data') and self.test_data is not None:\n",
    "                self.test_data_vec = self.vectorizer.get_embedding_data(embedding_matrix, self.test_data, word_index)\n",
    "                self.test_loader = DataLoader(CustomTextDataset(self.test_data_vec), batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "            return input_size  # Return the input size for building the network\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred in prepare_data: {str(e)}\")\n",
    "            return None  # Ensure a return value even on failure\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Define the forward pass.\"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "        \n",
    "    def update_layer(self, index, new_layer, new_learning_rate=None, new_batch_size=None, new_num_epochs=None):\n",
    "        \"\"\"Update a layer in the network at the specified index and modify learning parameters.\n",
    "    \n",
    "        Args:\n",
    "            index (int): The index of the layer to update.\n",
    "            new_layer (nn.Module): The new layer to replace the existing layer at the specified index.\n",
    "            new_learning_rate (float, optional): New learning rate for the optimizer. If not provided, the current learning rate is retained.\n",
    "            new_batch_size (int, optional): New batch size for training. If not provided, the current batch size is retained.\n",
    "            new_num_epochs (int, optional): New number of epochs for training. If not provided, the current number of epochs is retained.\n",
    "    \n",
    "        Raises:\n",
    "            IndexError: If the specified index is out of range for the current layers.\n",
    "        \"\"\"\n",
    "        layers = list(self.network.children())  # Get the current layers\n",
    "        # Check if the index is valid\n",
    "        if index < -len(layers) or index >= len(layers):\n",
    "            raise IndexError(\"Layer index out of range.\")  \n",
    "        \n",
    "        layers[index] = new_layer  # Replace the specified layer\n",
    "        self.network = nn.Sequential(*layers)  # Rebuild the network with the modified layers\n",
    "        \n",
    "        # Update learning parameters if provided\n",
    "        if new_learning_rate is not None:\n",
    "            self.learning_rate = new_learning_rate  # Update learning rate\n",
    "        if new_batch_size is not None:\n",
    "            self.batch_size = new_batch_size  # Update batch size\n",
    "        if new_num_epochs is not None:\n",
    "            self.num_epochs = new_num_epochs  # Update number of epochs\n",
    "    \n",
    "        # Update the optimizer with the new learning rate\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.learning_rate)\n",
    "    \n",
    "        # Print confirmation of the updates\n",
    "        print(f\"Layer at index {index} updated. Learning rate: {self.learning_rate}, Batch size: {self.batch_size}, Epochs: {self.num_epochs}\")\n",
    "\n",
    "    \n",
    "    def calculate_class_weights(self, y_train):\n",
    "        \"\"\"Calculate class weights based on the training targets.\"\"\"\n",
    "        classes = np.unique(y_train)\n",
    "        class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "        return torch.FloatTensor(class_weights)  # Ensure class weights are on the correct device\n",
    "\n",
    "    def get_activation_function(self, activation_fn):\n",
    "        \"\"\"Return the activation function based on the name provided.\"\"\"\n",
    "        if activation_fn == 'relu':\n",
    "            return nn.ReLU()\n",
    "        elif activation_fn == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        elif activation_fn == 'tanh':\n",
    "            return nn.Tanh()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "            \n",
    "    def build_network(self):\n",
    "        \"\"\"Build the neural network architecture based on the specified hidden layers and output configuration.\n",
    "    \n",
    "        This method constructs the network by sequentially adding layers. \n",
    "        It creates hidden layers as defined in self.hidden_size and adds an output layer \n",
    "        based on the number of classes specified for the task (binary or multi-class classification).\n",
    "    \n",
    "        Returns:\n",
    "            nn.Sequential: A PyTorch sequential model containing the configured layers.\n",
    "        \"\"\"\n",
    "        layers = []  # List to hold all the layers of the network\n",
    "        input_size = self.input_size  # Start with the input size defined for the model\n",
    "    \n",
    "        # Build hidden layers based on specified hidden sizes\n",
    "        for h in self.hidden_size:\n",
    "            layers.append(nn.Linear(input_size, h))  # Add a linear layer\n",
    "            layers.append(self.activation_function)  # Add the activation function\n",
    "            input_size = h  # Update input size for the next layer\n",
    "    \n",
    "        # Build the output layer\n",
    "        if self.num_classes == 1:  # For binary classification\n",
    "            layers.append(nn.Linear(input_size, 1))  # Single output neuron\n",
    "            layers.append(nn.Sigmoid())  # Sigmoid activation function for binary output\n",
    "        else:  # For multi-class classification\n",
    "            layers.append(nn.Linear(input_size, self.num_classes))  # Linear layer for multi-class outputs\n",
    "            # Softmax is typically used for multi-class output, but it can also be incorporated in the loss function.\n",
    "    \n",
    "        return nn.Sequential(*layers)  # Return the sequential model containing all layers\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the model with Out Of Memory (OOM) error handling.\n",
    "    \n",
    "        This method iteratively trains the neural network for a specified number of epochs, \n",
    "        monitors training and validation losses, and evaluates the F1 score. It also implements \n",
    "        early stopping based on validation F1 score and handles potential OOM errors by reducing \n",
    "        model complexity.\n",
    "    \n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame containing the epoch data including loss, F1 score, and predictions.\n",
    "        \"\"\"\n",
    "        best_f1 = 0.0  # Initialize best F1 score\n",
    "        epochs_without_improvement = 0  # Counter for early stopping\n",
    "        train_losses = []  # List to store training losses\n",
    "        val_losses = []  # List to store validation losses\n",
    "        \n",
    "        # Loop over the specified number of epochs\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{self.num_epochs}\")\n",
    "            self.network.train()  # Set the model to training mode\n",
    "            all_train_preds = []  # List to store predictions from training\n",
    "            running_loss = 0.0  # Variable to accumulate training loss\n",
    "    \n",
    "            try:\n",
    "                # Iterate over batches from the training loader\n",
    "                for texts, labels in self.train_loader:\n",
    "                    self.optimizer.zero_grad()  # Clear previous gradients\n",
    "                    outputs = self.network(texts)  # Forward pass through the network\n",
    "    \n",
    "                    # Handle loss calculation based on the number of classes\n",
    "                    if self.num_classes == 1:\n",
    "                        labels = labels.view(-1, 1)  # Reshape labels for binary classification\n",
    "                        loss = self.criterion(outputs, labels)  # Compute binary loss\n",
    "                    else:\n",
    "                        loss = self.criterion(outputs, labels)  # Compute multi-class loss\n",
    "                    \n",
    "                    loss.backward()  # Backward pass to compute gradients\n",
    "                    self.optimizer.step()  # Update model parameters\n",
    "    \n",
    "                    running_loss += loss.item()  # Accumulate loss for the current batch\n",
    "    \n",
    "                    # Generate predictions based on model output\n",
    "                    if self.num_classes == 1:\n",
    "                        predicted = (torch.sigmoid(outputs) > 0.5).float()  # Convert outputs to binary predictions\n",
    "                    else:\n",
    "                        _, predicted = torch.max(outputs.data, 1)  # Get predicted classes for multi-class\n",
    "    \n",
    "                    all_train_preds.extend(predicted.cpu().numpy())  # Store predictions\n",
    "    \n",
    "                # Calculate average training loss for the epoch\n",
    "                train_loss = running_loss / len(self.train_loader)\n",
    "                train_losses.append(train_loss)  # Append to training losses\n",
    "    \n",
    "                # Validate the model and retrieve validation outputs and loss\n",
    "                val_outputs, val_labels, average_loss = self.validate()\n",
    "                val_losses.append(average_loss)  # Append validation loss\n",
    "    \n",
    "                # Evaluate F1 score on the validation set\n",
    "                f1 = self.evaluate(val_labels, val_outputs)\n",
    "    \n",
    "                # Create a DataFrame for the current epoch's data\n",
    "                new_epoch_data = pd.DataFrame({\n",
    "                    \"Epoch\": [epoch + 1], \n",
    "                    \"Loss\": train_loss,\n",
    "                    \"F1_macro\": [f1],\n",
    "                    \"Prediction\": [val_outputs],\n",
    "                })\n",
    "    \n",
    "                self.epoch_data = pd.concat([self.epoch_data, new_epoch_data], ignore_index=True)\n",
    "    \n",
    "                # Print the results for the current epoch\n",
    "                print(f'Epoch [{epoch + 1}], F1 Score: {f1:.4f}, Validation Loss: {average_loss:.4f}')\n",
    "    \n",
    "                # Plot the confusion matrix after each epoch\n",
    "                self.plot_confusion_matrix(val_labels, val_outputs)\n",
    "    \n",
    "                # Call the learning rate scheduler to adjust learning rate\n",
    "                self.scheduler.step(average_loss)\n",
    "                print(f'Current learning rate: {self.scheduler.optimizer.param_groups[0][\"lr\"]}')\n",
    "    \n",
    "                # Early stopping logic based on F1 score improvement\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1  # Update best F1 score\n",
    "                    epochs_without_improvement = 0  # Reset counter\n",
    "                else:\n",
    "                    epochs_without_improvement += 1  # Increment counter for no improvement\n",
    "                    if epochs_without_improvement >= self.patience: \n",
    "                        display(Markdown(f\"<pre style='color: Red; font-size: 20'>Early stopping triggered after {epoch + 1} epochs.</pre>\"))\n",
    "                        break  # Stop training if patience is exceeded\n",
    "    \n",
    "            except RuntimeError as e:\n",
    "                # Handle out of memory (OOM) error by reducing model complexity\n",
    "                if 'out of memory' in str(e):\n",
    "                    print(\"Encountered OOM error. Reducing the number of layers or neurons.\")\n",
    "                    self.hidden_size = [max(32, layer // 2) for layer in self.hidden_size]  # Reduce the size of hidden layers\n",
    "                    self.network = self.build_network()  # Rebuild the model with new size\n",
    "                    self.network.to(self.device)  # Ensure the model is on the correct device\n",
    "                    self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.learning_rate)  # Reinitialize optimizer\n",
    "                    print(\"Retrying training with a smaller model.\")\n",
    "                    break  # Exit the loop and retry training\n",
    "    \n",
    "        # After training, plot the losses\n",
    "        self.plot_loss(train_losses, val_losses)\n",
    "    \n",
    "        return self.epoch_data  # Return accumulated data for all epochs\n",
    "\n",
    "\n",
    "\n",
    "    def evaluate(self, true_labels, predicted_labels):\n",
    "        \"\"\"Evaluate the model using F1 score.\n",
    "    \n",
    "        This method calculates the F1 score based on the true labels and predicted labels. \n",
    "        It also updates the best F1 score if the current score exceeds the previously recorded best.\n",
    "    \n",
    "        Args:\n",
    "            true_labels (list or numpy array): The true labels for the data being evaluated.\n",
    "            predicted_labels (list or numpy array): The predicted labels from the model.\n",
    "    \n",
    "        Returns:\n",
    "            float: The calculated F1 score.\n",
    "        \"\"\"\n",
    "        # If using single output for binary classification, convert predicted_labels to binary\n",
    "        if self.num_classes == 1:\n",
    "            predicted_labels = (predicted_labels > 0.5).astype(int)  # Convert probabilities to binary labels\n",
    "    \n",
    "        # Calculate the F1 score with macro averaging\n",
    "        f1 = f1_score(true_labels, predicted_labels, average='macro')\n",
    "        \n",
    "        # Update best F1 score if the current F1 score is higher\n",
    "        if f1 > self.best_f1_score:\n",
    "            self.best_f1_score = f1  # Store the best F1 score\n",
    "    \n",
    "        return f1  # Return the calculated F1 score\n",
    "\n",
    "\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"Validate the model and return predictions and true labels.\n",
    "    \n",
    "        This method evaluates the model's performance on the validation set and computes \n",
    "        the average loss. It returns the predictions made by the model and the true labels \n",
    "        for comparison.\n",
    "    \n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - list: Predicted labels for the validation set.\n",
    "                - list: True labels for the validation set.\n",
    "                - float: Average loss over the validation set.\n",
    "        \"\"\"\n",
    "        self.network.eval()  # Set the model to evaluation mode\n",
    "        val_outputs = []  # Initialize a list to store validation predictions\n",
    "        val_labels = []   # Initialize a list to store true labels\n",
    "        total_loss = 0.0  # Initialize total loss accumulator\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient calculations for efficiency\n",
    "            for val_texts, val_labels_batch in self.val_loader:\n",
    "                outputs = self.network(val_texts)  # Perform forward pass on validation data\n",
    "                loss = self.criterion(outputs, val_labels_batch)  # Calculate the loss\n",
    "                total_loss += loss.item()  # Accumulate total loss\n",
    "                \n",
    "                # Process outputs based on the number of classes\n",
    "                if self.num_classes == 1:\n",
    "                    # Convert outputs to binary predictions for binary classification\n",
    "                    predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                else:\n",
    "                    # For multi-class classification, get the index of the max output\n",
    "                    predicted = torch.argmax(outputs, axis=1)\n",
    "        \n",
    "                # Store predictions and true labels\n",
    "                val_outputs.extend(predicted.cpu().numpy())\n",
    "                val_labels.extend(val_labels_batch.cpu().numpy())\n",
    "        \n",
    "        # Calculate the average loss over all validation batches\n",
    "        average_loss = total_loss / len(self.val_loader)\n",
    "        \n",
    "        return val_outputs, val_labels, average_loss  # Return predictions, true labels, and average loss\n",
    "\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        \"\"\"Test the model on the test set and display the final F1 score if labels are available.\n",
    "    \n",
    "        This method evaluates the model's performance on unseen data and computes the F1 score.\n",
    "        If true labels for the test set are not available, it will notify the user.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of predicted labels for the test set.\n",
    "        \"\"\"\n",
    "        self.network.eval()  # Set the model to evaluation mode\n",
    "        test_preds = []  # Initialize a list to store test predictions\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient calculations for efficiency\n",
    "            for texts in self.test_loader:\n",
    "                outputs = self.network(texts)  # Perform forward pass on test data\n",
    "                \n",
    "                # Process outputs based on the number of classes\n",
    "                if self.num_classes == 1:\n",
    "                    # Convert outputs to binary predictions for binary classification\n",
    "                    predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                    test_preds.extend(predicted.cpu().numpy())  # Store predictions\n",
    "                else:\n",
    "                    # For multi-class classification, get the index of the max output\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    test_preds.extend(predicted.cpu().numpy())  # Store predictions\n",
    "        \n",
    "        # Check if test labels are available\n",
    "        if hasattr(self, 'y_test'):\n",
    "            test_labels = self.y_test.tolist()  # Use actual test labels if available\n",
    "            \n",
    "            # Calculate the F1 score using the true labels and predictions\n",
    "            final_f1 = f1_score(test_labels, test_preds, average='macro')\n",
    "            # Display the final F1 score in Markdown format\n",
    "            display(Markdown(f\"<pre style='color: Chartreuse; font-size: 20'>Final F1 Score: {final_f1:.4f}</pre>\"))\n",
    "        else:\n",
    "            # If no true labels are available, notify the user and skip F1 score calculation\n",
    "            display(Markdown(f\"<pre style='color: Chartreuse; font-size: 20'>Final F1 Score: No true label</pre>\"))\n",
    "        \n",
    "        return test_preds  # Return the list of predicted labels\n",
    "\n",
    "\n",
    "\n",
    "    def plot_loss(self, train_losses, val_losses):\n",
    "        \"\"\"Plot the training and validation loss.\"\"\"\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.title('Loss Over Epochs')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_confusion_matrix(self, true_labels, predicted_labels):\n",
    "        \"\"\"Plot the confusion matrix.\"\"\"\n",
    "        cm = confusion_matrix(true_labels, predicted_labels)\n",
    "        plt.figure(figsize=(1.5, 0.6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "\n",
    "    def wrong_prediction_at_best_epoch(self, X_val):\n",
    "        \"\"\"Get all wrong predictions from the best F1 score epoch and drop duplicates based on val_text.\"\"\"\n",
    "        # Find the index of the epoch with the best F1 score\n",
    "        best_epoch_index = self.epoch_data['F1_macro'].idxmax()\n",
    "        best_epoch_predictions = self.epoch_data.loc[best_epoch_index, 'Prediction']\n",
    "        \n",
    "        # Create a DataFrame to hold wrong predictions\n",
    "        best_epoch_wrong_df = pd.DataFrame({\n",
    "            \"prediction\": best_epoch_predictions,\n",
    "            \"actual\": self.y_val.tolist(),  # Use validation true labels\n",
    "            \"val_text\": X_val.tolist()       # Use validation texts\n",
    "        })\n",
    "        \n",
    "        # Check for length mismatch\n",
    "        if len(best_epoch_predictions) != len(self.y_val) or len(best_epoch_predictions) != len(X_val):\n",
    "            print(f\"Warning: Length mismatch at best epoch {best_epoch_index + 1}. \"\n",
    "                  f\"Predictions: {len(best_epoch_predictions)}, Actual: {len(self.y_val)}, Text: {len(X_val)}.\")\n",
    "        \n",
    "        # Filter for wrong predictions\n",
    "        wrong_predictions_df = best_epoch_wrong_df[best_epoch_wrong_df['prediction'] != best_epoch_wrong_df['actual']]\n",
    "        \n",
    "        # Drop duplicates based on the 'val_text' column\n",
    "        wrong_predictions_df = wrong_predictions_df.drop_duplicates(subset='val_text')\n",
    "        \n",
    "        return wrong_predictions_df\n",
    "\n",
    "\n",
    "vectorizer=Vectorizer(ngram_range=(1, 2), max_features=4000)\n",
    "classifier = MLPClassifier(stages=[3], vectorizer=vectorizer, hidden_size=[128, 64], learning_rate=0.00008)\n",
    "classifier.train()\n",
    "display(Markdown(f\"<pre style='color: Chartreuse; font-size: 20'>Best F1 Score: {classifier.best_f1_score:.4f}</pre>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c065ee-0890-40c3-8b9b-08ebe4aede9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "test_predictions = pd.DataFrame({'prediction': classifier.test(), 'true_label': None})\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ec72e9-7138-4361-8f74-4969dffbcc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(test_predictions['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294058b-e445-44be-b7ab-4aa2591264e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_val_raw, _, _, _ = pipeline.get_tokenized_data('stage_0')\n",
    "# Get predictions from the classifier\n",
    "wrong_predictions = classifier.wrong_prediction_at_best_epoch(X_val=X_val_raw)\n",
    "wrong_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d58abb-5467-4324-9d8d-6e30f506bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_preds_class = wrong_predictions.groupby('prediction').size()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(5, 3))\n",
    "wrong_preds_class.plot(kind='bar')\n",
    "plt.title('Count of Wrong Predictions per Class')\n",
    "plt.xlabel('Prediction Class (0 or 1)')\n",
    "plt.ylabel('Count of Wrong Predictions')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a2028c",
   "metadata": {},
   "source": [
    "#### 3. Create embeddings using this dataset: At the end of every epoch, print dot product similarity between embeddings for different pairs of words to see if they change in direction we would like (for opposite words it should gradually become negative, for similar - positive, for not related words - close to 0):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053c2801-920d-4485-a2e2-84933a2a949b",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Word Embedding\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce5a0cb-835f-4d30-a4f4-3bc24dfbeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import spacy\n",
    "\n",
    "class WordRelationships:\n",
    "    def __init__(self, vocab, nlp):\n",
    "        \"\"\"Initialize WordRelationships with a vocabulary and a SpaCy NLP model.\n",
    "        \n",
    "        Args:\n",
    "            vocab (list): List of words in the vocabulary.\n",
    "            nlp: SpaCy NLP model instance for processing text.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.nlp = nlp  # Store the nlp instance\n",
    "\n",
    "        # Precompute SpaCy tokens for efficiency\n",
    "        self.vocab_tokens = {word: self.nlp(word) for word in self.vocab}\n",
    "        \n",
    "        # Build relationships if vocabulary is not empty\n",
    "        self.antonyms, self.synonyms, self.unrelated = (self.build_word_relationships() \n",
    "                                                        if self.vocab else ({}, {}, {}))\n",
    "\n",
    "    def build_word_relationships(self):\n",
    "        \"\"\"Build dictionaries of antonyms, synonyms, and unrelated words.\"\"\"\n",
    "        antonym_dict = {}\n",
    "        synonym_dict = {}\n",
    "        unrelated_dict = {}\n",
    "\n",
    "        for word in self.vocab:\n",
    "            antonyms, synonyms = self.get_wordnet_relationships(word)\n",
    "            if antonyms:\n",
    "                antonym_dict[word] = antonyms\n",
    "            if synonyms:\n",
    "                synonym_dict[word] = synonyms\n",
    "            \n",
    "            unrelated = self.get_unrelated_words(word)\n",
    "            if unrelated:\n",
    "                unrelated_dict[word] = unrelated\n",
    "\n",
    "        return antonym_dict, synonym_dict, unrelated_dict\n",
    "\n",
    "    def get_wordnet_relationships(self, word):\n",
    "        \"\"\"Get synonyms and antonyms from WordNet for a given word.\n",
    "\n",
    "        Args:\n",
    "            word (str): The word to find relationships for.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing lists of antonyms and synonyms.\n",
    "        \"\"\"\n",
    "        antonyms, synonyms = set(), set()\n",
    "        for syn in wn.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                if lemma.antonyms():\n",
    "                    antonyms.add(lemma.antonyms()[0].name())\n",
    "                synonyms.add(lemma.name())\n",
    "        return list(antonyms), list(synonyms)  # Return as lists\n",
    "\n",
    "    def get_unrelated_words(self, word, topn=10, threshold=0.1):\n",
    "        \"\"\"Get words that are unrelated based on low similarity.\n",
    "\n",
    "        Args:\n",
    "            word (str): The word to find unrelated words for.\n",
    "            topn (int): Maximum number of unrelated words to return.\n",
    "            threshold (float): Similarity threshold to determine unrelatedness.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of unrelated words.\n",
    "        \"\"\"\n",
    "        word_token = self.vocab_tokens[word]\n",
    "        unrelated_words = []\n",
    "\n",
    "        # Check similarity for all words and filter based on threshold\n",
    "        for vocab_word, vocab_token in self.vocab_tokens.items():\n",
    "            similarity = word_token.similarity(vocab_token)\n",
    "            if 0 < similarity < threshold:\n",
    "                unrelated_words.append(vocab_word)\n",
    "                if len(unrelated_words) == topn:\n",
    "                    break\n",
    "        return unrelated_words\n",
    "\n",
    "    def get_opposite_words(self, word):\n",
    "        \"\"\"Return a list of antonym words for the given word.\"\"\"\n",
    "        return self.antonyms.get(word, [])\n",
    "\n",
    "    def get_synonyms(self, word):\n",
    "        \"\"\"Return a list of synonym words for the given word.\"\"\"\n",
    "        return self.synonyms.get(word, [])\n",
    "\n",
    "    def get_relationship(self, word1, word2):\n",
    "        \"\"\"Identify the relationship between two words: synonym, opposite, unrelated, or related.\n",
    "\n",
    "        Args:\n",
    "            word1 (str): The first word.\n",
    "            word2 (str): The second word.\n",
    "\n",
    "        Returns:\n",
    "            str: The relationship type between the two words.\n",
    "        \"\"\"\n",
    "        synonyms_of_word1 = self.get_synonyms(word1)\n",
    "        antonyms_of_word1 = self.get_opposite_words(word1)\n",
    "        \n",
    "        if word2 in synonyms_of_word1:\n",
    "            return \"synonym\"\n",
    "        elif word2 in antonyms_of_word1:\n",
    "            return \"opposite\"\n",
    "        elif word2 in self.unrelated.get(word1, []):\n",
    "            return \"unrelated\"\n",
    "        else:\n",
    "            return \"related\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab67267-6206-4669-bcbc-55268a87e4f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, word_pairs):\n",
    "        self.word_pairs = word_pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word1, word2, similarity = self.word_pairs[idx]\n",
    "        return (torch.tensor(word1, dtype=torch.long), \n",
    "                torch.tensor(word2, dtype=torch.long), \n",
    "                torch.tensor(similarity, dtype=torch.float))\n",
    "\n",
    "\n",
    "class WordEmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(WordEmbeddingModel, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.embeddings(input)\n",
    "\n",
    "\n",
    "class EmbeddingTrainer:\n",
    "    \"\"\"\n",
    "    A class to train word embeddings based on tokenized data.\n",
    "\n",
    "    Attributes:\n",
    "        tokenized_data (pd.Series): The input tokenized data containing words.\n",
    "        vocab (dict): A dictionary mapping words to unique indices.\n",
    "        inv_vocab (dict): A dictionary mapping indices to words.\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "        epochs (int): The number of training epochs.\n",
    "        nlp (spacy.Language): The spaCy model for word relationships.\n",
    "        word_relationships (WordRelationships): An instance for managing word relationships.\n",
    "        word_pairs (list): A list of word pairs generated from the tokenized data.\n",
    "        dataset (EmbeddingDataset): An instance of the dataset containing word pairs.\n",
    "        data_loader (DataLoader): A PyTorch DataLoader for batching the dataset.\n",
    "        model (WordEmbeddingModel): The neural network model for word embeddings.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer for updating model weights.\n",
    "        criterion (nn.Module): The loss function used for training.\n",
    "        word_vectors (KeyedVectors): A Gensim KeyedVectors instance for storing embeddings.\n",
    "        symmetric (str): Defines behavior for handling symmetric word pairs ('keep' or 'drop').\n",
    "        relationships_df (pd.DataFrame): A DataFrame to store similarity relationships for word pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenized_data: pd.Series, embedding_dim=100, batch_size=32, epochs=20, learning_rate=0.01, window_size=2, topn=10, symmetric='keep'):\n",
    "        self.tokenized_data = tokenized_data\n",
    "        self.vocab, self.inv_vocab = self.build_vocabulary(tokenized_data)  # Build vocabulary from tokenized data\n",
    "        self.vocab_size = len(self.vocab)  # Calculate vocabulary size here\n",
    "        self.epochs = epochs\n",
    "        self.nlp = spacy.load(\"en_core_web_md\")  # Load the spaCy model for word relationships\n",
    "\n",
    "        # Pass the nlp instance to WordRelationships\n",
    "        self.word_relationships = WordRelationships(self.vocab, self.nlp)  # Create an instance to handle word relationships\n",
    "        self.word_pairs = self.generate_word_pairs(window_size, topn)  # Generate word pairs based on the context\n",
    "        self.dataset = EmbeddingDataset(self.word_pairs)  # Create a dataset instance for word pairs\n",
    "        self.data_loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)  # Create a DataLoader for batching\n",
    "        self.model = WordEmbeddingModel(self.vocab_size, embedding_dim)  # Initialize the word embedding model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)  # Define the optimizer\n",
    "        self.criterion = nn.CosineEmbeddingLoss()  # Define the loss function\n",
    "        self.word_vectors = KeyedVectors(vector_size=embedding_dim)  # Initialize KeyedVectors for storing embeddings\n",
    "        self.symmetric = symmetric  # Store the symmetry behavior choice\n",
    "        \n",
    "        # Create a DataFrame to store relationships between word pairs\n",
    "        self.relationships_df = pd.DataFrame(index=[f\"{self.inv_vocab[word1_idx]}<-->{self.inv_vocab[word2_idx]}\" \n",
    "                                                    for word1_idx, word2_idx, _ in self.word_pairs])\n",
    "\n",
    "    def generate_word_pairs(self, window_size=2, topn=10):\n",
    "        \"\"\"\n",
    "        Generates word pairs from the tokenized data within a specified context window.\n",
    "    \n",
    "        Each word in the tokenized data is paired with its surrounding context words,\n",
    "        with the option to filter out pairs based on the similarity relationships.\n",
    "    \n",
    "        Parameters:\n",
    "            window_size (int): The size of the context window to look around each word.\n",
    "            topn (int): The number of top context words to consider for each target word (not used in this implementation).\n",
    "    \n",
    "        Returns:\n",
    "            list: A list of tuples, where each tuple contains the index of a target word,\n",
    "                  the index of a context word, and a similarity flag (1 for opposite words, 0 otherwise).\n",
    "        \"\"\"\n",
    "        word_pairs = []  # Initialize a list to store word pairs\n",
    "        for document in self.tokenized_data:  # Iterate over each document in the tokenized data\n",
    "            words = document.split()  # Split the document into individual words\n",
    "            for i, word in enumerate(words):  # Enumerate through the words to get their indices\n",
    "                # Define the start and end indices for the context window\n",
    "                start = max(0, i - window_size)  # Ensure start index is not negative\n",
    "                end = min(len(words), i + window_size + 1)  # Ensure end index does not exceed the word list\n",
    "                # Extract context words from the current window, excluding the target word itself\n",
    "                context_words = [words[j] for j in range(start, end) if j != i]\n",
    "                for context_word in context_words:  # Iterate through the context words\n",
    "                    # Check if both the target and context words are in the vocabulary\n",
    "                    if word in self.vocab and context_word in self.vocab:\n",
    "                        # Set similarity flag: 1 if the word has no opposite words, else 0\n",
    "                        similarity = 1 if self.word_relationships.get_opposite_words(word) == [] else 0\n",
    "                        # Append the target word index, context word index, and similarity to the word_pairs list\n",
    "                        word_pairs.append((self.vocab[word], self.vocab[context_word], similarity))\n",
    "        return word_pairs  # Return the list of generated word pairs\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the word embedding model for a specified number of epochs.\n",
    "    \n",
    "        This method performs the forward and backward passes through the model, \n",
    "        computes the loss using cosine embedding loss, and updates the model weights \n",
    "        using the Adam optimizer. It also tracks the average loss for each epoch \n",
    "        and updates the relationships DataFrame after training.\n",
    "    \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.model.train()  # Set the model to training mode\n",
    "        \n",
    "        for epoch in range(self.epochs):  # Iterate over each epoch\n",
    "            total_loss = 0  # Reset total loss at the start of each epoch\n",
    "            \n",
    "            for word1, word2, similarity in self.data_loader:  # Iterate through batches from the data loader\n",
    "                # Check if any index is out of range\n",
    "                if word1.max() >= self.vocab_size or word2.max() >= self.vocab_size:  # Ensure indices are within vocabulary size\n",
    "                    print(\"Index out of range!\")  # Print warning if index is out of range\n",
    "                    continue  # Skip this batch if indices are out of range\n",
    "    \n",
    "                self.optimizer.zero_grad()  # Clear previous gradients\n",
    "                \n",
    "                # Forward pass for word1 and word2 to get their embeddings\n",
    "                emb1 = self.model(word1)  # Get embeddings for the first word\n",
    "                emb2 = self.model(word2)  # Get embeddings for the second word\n",
    "    \n",
    "                # Compute loss using cosine embedding loss\n",
    "                loss = self.criterion(emb1, emb2, similarity)  # Calculate the loss\n",
    "                loss.backward()  # Perform the backward pass\n",
    "                \n",
    "                self.optimizer.step()  # Update model weights\n",
    "                \n",
    "                total_loss += loss.item()  # Accumulate the total loss\n",
    "    \n",
    "            # Calculate and print average loss for the epoch\n",
    "            average_loss = total_loss / len(self.data_loader)  # Compute average loss\n",
    "            display(Markdown(f\"<strong style='color: Chartreuse; font-size: 20'>\\nEpoch [{epoch + 1}/{self.epochs}], Loss: {average_loss:.4f}</strong>\")) \n",
    "    \n",
    "            # Update relationships DataFrame for each pair after training\n",
    "            self.update_relationships(epoch)  # Update relationships based on training results\n",
    "    \n",
    "            self.evaluate_embeddings()  # Call the evaluation function to assess embeddings\n",
    "\n",
    "\n",
    "    def update_relationships(self, epoch):\n",
    "        \"\"\"\n",
    "        Updates the relationships DataFrame with similarity values for word pairs \n",
    "        after training.\n",
    "    \n",
    "        This method computes the cosine similarity between embeddings of word pairs \n",
    "        generated during training. It adds the similarity values to a new column \n",
    "        corresponding to the current epoch in the relationships DataFrame.\n",
    "    \n",
    "        Args:\n",
    "            epoch (int): The current epoch number, used to create a unique column \n",
    "                         for the similarity values in the DataFrame.\n",
    "    \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        epoch_column = f\"Epoch {epoch + 1}\"  # Create a unique column name for the current epoch\n",
    "    \n",
    "        # Create a new column for the current epoch in the relationships DataFrame\n",
    "        self.relationships_df[epoch_column] = None\n",
    "    \n",
    "        # Iterate through the generated word pairs\n",
    "        for word1_idx, word2_idx, _ in self.word_pairs:\n",
    "            # Check if both word indices exist in the inverse vocabulary\n",
    "            if word1_idx in self.inv_vocab and word2_idx in self.inv_vocab:\n",
    "                word1 = self.inv_vocab[word1_idx]  # Get the first word from the inverse vocabulary\n",
    "                word2 = self.inv_vocab[word2_idx]  # Get the second word from the inverse vocabulary\n",
    "                pair_label = f\"{word1}<-->{word2}\"  # Create a label for the word pair\n",
    "                \n",
    "                # Get embeddings for the word pair\n",
    "                emb1 = self.model(torch.tensor(word1_idx, dtype=torch.long).unsqueeze(0))  # Get embedding for word1\n",
    "                emb2 = self.model(torch.tensor(word2_idx, dtype=torch.long).unsqueeze(0))  # Get embedding for word2\n",
    "    \n",
    "                # Check if embeddings are non-zero\n",
    "                if torch.count_nonzero(emb1) > 0 and torch.count_nonzero(emb2) > 0:\n",
    "                    # Calculate the cosine similarity between the two embeddings\n",
    "                    similarity = torch.dot(emb1.view(-1), emb2.view(-1)).item()\n",
    "                    \n",
    "                    # Store the similarity value in the corresponding epoch column\n",
    "                    self.relationships_df.loc[pair_label, epoch_column] = similarity\n",
    "                    self.relationships_df.index.name = \"WordPair\"  # Set the index name for the DataFrame\n",
    "\n",
    "\n",
    "    def evaluate_embeddings(self):\n",
    "        \"\"\"\n",
    "        Evaluates the learned embeddings by checking the similarity for word pairs.\n",
    "    \n",
    "        This method calculates the cosine similarity between the embeddings of word \n",
    "        pairs. It tracks evaluated pairs to avoid duplicate calculations, and it can \n",
    "        handle symmetry based on the class's `symmetric` attribute.\n",
    "    \n",
    "        If `symmetric` is set to 'drop', only unique pairs (word1, word2) are evaluated.\n",
    "        If set to 'keep', both (word1, word2) and (word2, word1) pairs are evaluated.\n",
    "    \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        seen_pairs = set()  # To track already evaluated pairs\n",
    "    \n",
    "        # Iterate through the generated word pairs\n",
    "        for word1_idx, word2_idx, _ in self.word_pairs:\n",
    "            if self.symmetric == 'drop':\n",
    "                # Only calculate similarity for unique pairs (word1, word2)\n",
    "                if (word2_idx, word1_idx) in seen_pairs:\n",
    "                    continue  # Skip already evaluated pairs\n",
    "                seen_pairs.add((word1_idx, word2_idx))  # Track this pair as seen\n",
    "    \n",
    "            # Check if both word indices exist in the inverse vocabulary\n",
    "            if word1_idx in self.inv_vocab and word2_idx in self.inv_vocab:\n",
    "                word1 = self.inv_vocab[word1_idx]  # Get the first word from the inverse vocabulary\n",
    "                word2 = self.inv_vocab[word2_idx]  # Get the second word from the inverse vocabulary\n",
    "                \n",
    "                # Get embeddings for the word pair\n",
    "                emb1 = self.model(torch.tensor(word1_idx, dtype=torch.long).unsqueeze(0))  # Embedding for word1\n",
    "                emb2 = self.model(torch.tensor(word2_idx, dtype=torch.long).unsqueeze(0))  # Embedding for word2\n",
    "    \n",
    "                # Check if embeddings are non-zero\n",
    "                if torch.count_nonzero(emb1) > 0 and torch.count_nonzero(emb2) > 0:\n",
    "                    # Calculate the cosine similarity between the two embeddings\n",
    "                    similarity = torch.dot(emb1.view(-1), emb2.view(-1)).item()\n",
    "    \n",
    "                    # Get the relationship category (synonym, opposite, unrelated, related)\n",
    "                    category = self.word_relationships.get_relationship(word1, word2)\n",
    "    \n",
    "                    # Print similarity for the pair\n",
    "                    print(f'{word1} and {word2}: {similarity:.4f}')  # Uncomment to show the category: ({category})\n",
    "    \n",
    "                if self.symmetric == 'keep':\n",
    "                    # If keeping symmetry, calculate the reverse as well\n",
    "                    emb1, emb2 = emb2, emb1  # Swap embeddings\n",
    "                    similarity = torch.dot(emb1.view(-1), emb2.view(-1)).item()  # Recalculate similarity\n",
    "                    print(f'{self.inv_vocab[word2_idx]}<-->{self.inv_vocab[word1_idx]}: {similarity:.4f}')  # Print reversed pair similarity\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def build_vocabulary(tokenized_data):\n",
    "        \"\"\"\n",
    "        Builds a vocabulary from the tokenized data.\n",
    "    \n",
    "        This method creates two dictionaries: \n",
    "        - `vocab`: mapping of words to unique indices.\n",
    "        - `inv_vocab`: mapping of indices back to words.\n",
    "    \n",
    "        Args:\n",
    "            tokenized_data (pd.Series): A series containing tokenized documents.\n",
    "    \n",
    "        Returns:\n",
    "            tuple: A tuple containing:\n",
    "                - vocab (dict): A dictionary mapping words to indices.\n",
    "                - inv_vocab (dict): A dictionary mapping indices back to words.\n",
    "        \"\"\"\n",
    "        vocab = {}  # Dictionary to hold the vocabulary mapping\n",
    "        index = 0   # Initialize index for unique word assignment\n",
    "    \n",
    "        # Iterate through each document in the tokenized data\n",
    "        for document in tokenized_data:\n",
    "            # Split document into words\n",
    "            for word in document.split():\n",
    "                # If the word is not already in the vocabulary, add it\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = index  # Map word to the current index\n",
    "                    index += 1  # Increment the index for the next unique word\n",
    "                \n",
    "        # Create an inverse vocabulary mapping\n",
    "        inv_vocab = {index: word for word, index in vocab.items()}\n",
    "        return vocab, inv_vocab  # Return the vocabulary and its inverse\n",
    "    \n",
    "\n",
    "    def save_embeddings(self, filename='custom_embeddings.bin'):\n",
    "        \"\"\"\n",
    "        Save the learned embeddings to a .bin file using Gensim's KeyedVectors format.\n",
    "    \n",
    "        This method retrieves the learned embeddings from the model and saves them\n",
    "        in a binary format compatible with Gensim. It ensures that the vocabulary size\n",
    "        matches the number of embedding vectors before saving.\n",
    "    \n",
    "        Args:\n",
    "            filename (str): The name of the file to save embeddings to. Defaults to 'custom_embeddings.bin'.\n",
    "    \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        # Retrieve the learned embeddings as a NumPy array\n",
    "        embeddings = self.model.embeddings.weight.data.numpy()\n",
    "        \n",
    "        # Ensure the vocab size matches the number of embedding vectors\n",
    "        if len(self.inv_vocab) != embeddings.shape[0]:\n",
    "            print(\"Error: Vocab size and embedding size do not match.\")\n",
    "            return  # Exit if there is a mismatch\n",
    "        \n",
    "        # Initialize KeyedVectors with preallocated vocab size and embedding dimensions\n",
    "        vector_size = embeddings.shape[1]\n",
    "        keyed_vectors = KeyedVectors(vector_size=vector_size)\n",
    "        \n",
    "        # Collect all words and their vectors in a list\n",
    "        words = list(self.inv_vocab.values())\n",
    "        \n",
    "        # Use add_vectors to add all vectors in one batch\n",
    "        keyed_vectors.add_vectors(words, embeddings)\n",
    "        \n",
    "        # Save in a binary Word2Vec format\n",
    "        keyed_vectors.save_word2vec_format(filename, binary=True)\n",
    "        print(f\"Embeddings successfully saved to {filename}\")  # Confirmation message\n",
    "    \n",
    "\n",
    "# Retrieve tokenized data for training and validation from stage_5\n",
    "X_train, X_val, _, _, _ = pipeline.get_tokenized_data('stage_5')\n",
    "\n",
    "# Combine training and validation data, and select the first 10 entries for demonstration\n",
    "tokenized_data = pd.concat([X_train, X_val], ignore_index=True).head(10)\n",
    "\n",
    "# Initialize the EmbeddingTrainer with the tokenized data\n",
    "# 'symmetric' parameter is set to 'drop' to avoid calculating symmetry for word pairs\n",
    "trainer = EmbeddingTrainer(tokenized_data, symmetric='drop')\n",
    "\n",
    "# Train the embedding model using the initialized trainer\n",
    "trainer.train()\n",
    "\n",
    "# Save the learned embeddings to a binary file\n",
    "trainer.save_embeddings('custom_embeddings.bin')\n",
    "\n",
    "# Generate word pairs from the tokenized data\n",
    "word_pairs = trainer.generate_word_pairs()\n",
    "\n",
    "# Convert the word pairs (list of tuples) into a Pandas DataFrame\n",
    "word_pairs_df = pd.DataFrame(word_pairs, columns=['Word1', 'Word2', 'Similarity'])\n",
    "\n",
    "# Save the DataFrame containing word pairs to a CSV file\n",
    "word_pairs_df.to_csv('word_pairs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e949701a-1636-4f43-aba2-265854d22f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display the DataFrame of relationships\n",
    "display(trainer.relationships_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd599a",
   "metadata": {},
   "source": [
    "#### 4. Use embeddings learned during previous task for classification with any sklearn model. Try to use google pretrained embeddings as well: https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25554645-9e03-417d-832c-5686dff553cf",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Evaluation with Trained Embedding\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a5bee-896b-43f2-a099-f1f25086ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self, pipeline, stage_indices, custom_embeddings_path, additional_embeddings_path=None):\n",
    "        \"\"\"\n",
    "        Initializes the EmbeddingModel.\n",
    "\n",
    "        Args:\n",
    "            pipeline: The data processing pipeline used to retrieve tokenized data.\n",
    "            stage_indices (list): List of integers representing the stages to process.\n",
    "            custom_embeddings_path (str): Path to the custom embeddings file.\n",
    "            additional_embeddings_path (str, optional): Path to additional embeddings file. Default is None.\n",
    "        \"\"\"\n",
    "        self.pipeline = pipeline\n",
    "        self.stages = [f'stage_{i}' for i in stage_indices]  # Format stages from integers\n",
    "        self.custom_embeddings_path = custom_embeddings_path\n",
    "        self.additional_embeddings_path = additional_embeddings_path\n",
    "        self.custom_embeddings = None\n",
    "        self.additional_embeddings = None\n",
    "        self.combined_embeddings = None\n",
    "\n",
    "    def load_embeddings(self, file_path):\n",
    "        \"\"\"Load embeddings using Gensim from a binary file.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the embeddings file.\n",
    "\n",
    "        Returns:\n",
    "            KeyedVectors: Loaded embeddings if successful, None otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return KeyedVectors.load_word2vec_format(file_path, binary=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def load_all_embeddings(self):\n",
    "        \"\"\"Load custom and optional additional embeddings.\"\"\"\n",
    "        print(\"Loading custom embeddings...\")\n",
    "        self.custom_embeddings = self.load_embeddings(self.custom_embeddings_path)\n",
    "\n",
    "        if self.additional_embeddings_path is not None:  # Only load additional embeddings if a path is provided\n",
    "            print(\"Loading additional embeddings...\")\n",
    "            self.additional_embeddings = self.load_embeddings(self.additional_embeddings_path)\n",
    "\n",
    "        # Check if loading was successful\n",
    "        if self.custom_embeddings is not None:\n",
    "            print(f\"Custom embeddings loaded. Number of words: {len(self.custom_embeddings)}\")\n",
    "        else:\n",
    "            print(\"Failed to load custom embeddings.\")\n",
    "\n",
    "        if self.additional_embeddings is not None:\n",
    "            print(f\"Additional embeddings loaded. Number of words: {len(self.additional_embeddings)}\")\n",
    "        else:\n",
    "            print(\"Failed to load additional embeddings.\")\n",
    "\n",
    "    def create_combined_embeddings(self):\n",
    "        \"\"\"Create a combined embedding dictionary by concatenating vectors from both.\"\"\"\n",
    "        if self.custom_embeddings is None:\n",
    "            print(\"Custom embeddings must be loaded before creating combined embeddings.\")\n",
    "            return\n",
    "\n",
    "        combined_embeddings = {}\n",
    "        for word in self.custom_embeddings.index_to_key:  # Access words in custom embeddings\n",
    "            combined_vector = self.custom_embeddings[word]  # Start with custom embeddings\n",
    "            if self.additional_embeddings is not None and word in self.additional_embeddings:  # Check for additional embeddings\n",
    "                # Concatenate vectors from both embeddings\n",
    "                combined_vector = np.concatenate((combined_vector, self.additional_embeddings[word]))\n",
    "            combined_embeddings[word] = combined_vector\n",
    "            \n",
    "        self.combined_embeddings = combined_embeddings\n",
    "\n",
    "    def prepare_data(self, stage):\n",
    "        \"\"\"Load embeddings, create combined embeddings, and prepare data for a specific stage.\n",
    "\n",
    "        Args:\n",
    "            stage (str): The stage to prepare data for.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (X_train_embeddings, X_val_embeddings, y_train, y_val) or (None, None, None, None) on failure.\n",
    "        \"\"\"\n",
    "        self.load_all_embeddings()  # Load all embeddings\n",
    "        self.create_combined_embeddings()  # Create combined embeddings\n",
    "\n",
    "        # Retrieve data using the pipeline for the specified stage\n",
    "        X_train, X_val, y_train, y_val, test_data = self.pipeline.get_tokenized_data(stage)\n",
    "\n",
    "        if self.combined_embeddings is None:\n",
    "            print(\"Combined embeddings must be created before preparing data.\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        # Prepare embedding for the datasets\n",
    "        embedding_size = len(next(iter(self.combined_embeddings.values())))\n",
    "        X_train_embeddings = np.zeros((len(X_train), embedding_size))\n",
    "        X_val_embeddings = np.zeros((len(X_val), embedding_size))\n",
    "\n",
    "        for i, text in enumerate(X_train):\n",
    "            words = text.split()\n",
    "            valid_vectors = [self.combined_embeddings[word] for word in words if word in self.combined_embeddings]\n",
    "            if valid_vectors:\n",
    "                X_train_embeddings[i] = np.mean(valid_vectors, axis=0)\n",
    "            else:\n",
    "                X_train_embeddings[i] = np.zeros(embedding_size)\n",
    "\n",
    "        for i, text in enumerate(X_val):\n",
    "            words = text.split()\n",
    "            valid_vectors = [self.combined_embeddings[word] for word in words if word in self.combined_embeddings]\n",
    "            if valid_vectors:\n",
    "                X_val_embeddings[i] = np.mean(valid_vectors, axis=0)\n",
    "            else:\n",
    "                X_val_embeddings[i] = np.zeros(embedding_size)\n",
    "\n",
    "        return X_train_embeddings, X_val_embeddings, y_train, y_val\n",
    "\n",
    "    def train_model(self, X_train_embeddings, y_train):\n",
    "        \"\"\"Train a logistic regression model.\n",
    "\n",
    "        Args:\n",
    "            X_train_embeddings (ndarray): Training data embeddings.\n",
    "            y_train (array): Training labels.\n",
    "\n",
    "        Returns:\n",
    "            LogisticRegression: Trained logistic regression model.\n",
    "        \"\"\"\n",
    "        model = LogisticRegression(max_iter=1000)\n",
    "        model.fit(X_train_embeddings, y_train)\n",
    "        return model\n",
    "\n",
    "    def evaluate_model(self, model, X_val_embeddings, y_val):\n",
    "        \"\"\"Predict and evaluate the model.\n",
    "\n",
    "        Args:\n",
    "            model (LogisticRegression): The trained model.\n",
    "            X_val_embeddings (ndarray): Validation data embeddings.\n",
    "            y_val (array): Validation labels.\n",
    "        \"\"\"\n",
    "        y_pred = model.predict(X_val_embeddings)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        display(Markdown(f\"<strong style='color: Chartreuse; font-size: 20'>Validation Accuracy: {accuracy * 100:.2f}%</strong>\"))\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Load embeddings, prepare data, train and evaluate the model for each stage.\"\"\"\n",
    "        for stage in self.stages:\n",
    "            print(f\"\\nProcessing {stage}...\")\n",
    "            X_train_embeddings, X_val_embeddings, y_train, y_val = self.prepare_data(stage)\n",
    "            if X_train_embeddings is not None and X_val_embeddings is not None and y_train is not None and y_val is not None:\n",
    "                model = self.train_model(X_train_embeddings, y_train)  # Train the model\n",
    "                self.evaluate_model(model, X_val_embeddings, y_val)  # Evaluate the model\n",
    "\n",
    "\n",
    "# Paths to your embeddings\n",
    "custom_embeddings_path = 'custom_embeddings.bin'  # Path to your custom binary embeddings\n",
    "additional_embeddings_path = 'GoogleNews-vectors-negative300.bin'  # Path to additional embeddings (optional)\n",
    "\n",
    "# Assuming 'pipeline' is already defined and available\n",
    "stage_indices = [3]  # Specify the stages as integers\n",
    "\n",
    "# Instantiate the EmbeddingModel\n",
    "embedding_model = EmbeddingModel(pipeline, stage_indices, custom_embeddings_path, additional_embeddings_path)\n",
    "\n",
    "# Run the entire process for all specified stages\n",
    "embedding_model.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394455b3-8d02-45bf-8a82-feb1bad47bae",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: Red; font-size: 20px; font-weight: italics;\">\n",
    "Displayed results above are not true representative of the test. Small portion of files used for testing due to size and memory requirements. Same with question 5 results\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87af3b5-6cc6-4b49-bd9d-ad0c5bef4cd2",
   "metadata": {},
   "source": [
    "#### 5. Take NN trained in previous task and replace last linear layer with linear layer for classification (just different number of outputs (you can do it as nn.last_layer = nn.Linear(hiiden, 2)). Train and test it as in the 2nd task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0c7d14-6eef-4f7a-a5b4-b44769f14bda",
   "metadata": {},
   "source": [
    "<span style=\"font-family: 'Courier New', Courier, monospace; color: #00FF33; font-size: 30px; font-weight: bold;\">\n",
    "Replacing NN Layer and Classification With Trained Embedding\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5907266-8ff5-47a9-8f08-79eb88ef5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "\n",
    "class ClassifierManager:\n",
    "    def __init__(self, classifier, embedding_file_paths):\n",
    "        \"\"\"\n",
    "        Initialize the ClassifierManager with the classifier instance and embedding file paths.\n",
    "\n",
    "        Parameters:\n",
    "        - classifier: An instance of the MLPClassifier.\n",
    "        - embedding_file_paths: A list of paths to embedding files for the classifier.\n",
    "        \"\"\"\n",
    "        self.classifier = classifier\n",
    "        # Set up the classifier for embedding-based data\n",
    "        self.classifier.vectorizer.data_type = 'embedding'\n",
    "        self.classifier.embedding_file_paths = embedding_file_paths\n",
    "        self.replaced_layer_index = -1  # Track the index of the replaced layer\n",
    "\n",
    "    def replace_last_layer(self):\n",
    "        \"\"\"Replace the last layer of the classifier with a new linear layer for binary classification.\"\"\"\n",
    "        # Create a new linear layer adjusted for binary classification\n",
    "        new_layer = nn.Linear(self.classifier.hidden_size[-1], 2)\n",
    "        \n",
    "        if hasattr(self.classifier, 'update_layer'):\n",
    "            # Call the update_layer method from the original classifier to replace the last layer\n",
    "            self.classifier.update_layer(\n",
    "                index=self.replaced_layer_index,  # Update the last layer\n",
    "                new_layer=new_layer,\n",
    "                new_learning_rate=0.0001,\n",
    "                new_batch_size=None,\n",
    "                new_num_epochs=None\n",
    "            ) \n",
    "        else:\n",
    "            print(\"Error: update_layer method not found in the classifier.\")\n",
    "\n",
    "    def train_and_test(self):\n",
    "        \"\"\"Train the classifier and then get test predictions.\"\"\"\n",
    "        # Call the train method from the original classifier to train the model\n",
    "        self.classifier.train()\n",
    "        # Call the test method from the original classifier to get predictions\n",
    "        test_predictions = self.classifier.test()\n",
    "        return test_predictions\n",
    "\n",
    "    def display_layers(self):\n",
    "        \"\"\"Display the classifier network layers after modification, indicating the replaced layer.\"\"\"\n",
    "        # Create a DataFrame with layer indices and types by accessing the original classifier's network\n",
    "        layers = list(enumerate(self.classifier.network.children()))\n",
    "        df_after = pd.DataFrame(layers, columns=['Index', 'Layer'])\n",
    "        \n",
    "        # Mark the replaced layer with an indicator\n",
    "        replaced_layer_info = f\" (Replaced)\" if self.replaced_layer_index == -1 else \"\"\n",
    "        df_after.loc[df_after['Index'] == self.replaced_layer_index, 'Layer'] = (\n",
    "            df_after.loc[df_after['Index'] == self.replaced_layer_index, 'Layer']\n",
    "            .astype(str) + replaced_layer_info\n",
    "        )\n",
    "        \n",
    "        print(\"Layers after replacement:\\n\")\n",
    "        display(df_after)\n",
    "        print('\\n')\n",
    "\n",
    "# Usage\n",
    "custom_embeddings_path = 'custom_embeddings.bin'   \n",
    "google_embeddings_path = 'GoogleNews-vectors-negative300.bin'   \n",
    "embedding_file_paths = [custom_embeddings_path, google_embeddings_path]\n",
    "\n",
    "# Create an instance of Vectorizer for handling embeddings\n",
    "vectorizer = Vectorizer(embedding_file_paths=embedding_file_paths, data_type='embedding')\n",
    "# Initialize the classifier using MLPClassifier from the original class\n",
    "classifier = MLPClassifier(stages=[3], vectorizer=vectorizer, hidden_size = [512, 256, 128], num_layers = 3) #increased deepness for large corpus\n",
    "\n",
    "# Initialize the ClassifierManager with the classifier and embedding file paths\n",
    "manager = ClassifierManager(classifier, embedding_file_paths)\n",
    "\n",
    "# Replace the last layer in the classifier and display updated layers\n",
    "manager.replace_last_layer()\n",
    "manager.display_layers()\n",
    "\n",
    "# Train the classifier and obtain test predictions, storing results in a DataFrame\n",
    "test_predictions = pd.DataFrame({'prediction': manager.train_and_test(), 'true_label': None})\n",
    "test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6510a-4dbf-406c-8156-309ca8436b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81ccea7-e864-4737-9627-6967d3aefab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
